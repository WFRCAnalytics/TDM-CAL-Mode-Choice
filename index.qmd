---
title: Calculating the Mode Choice Calibration Parameters
subtitle: Using Household Travel Survey and UTA On-Board Survey Data
description: Lorem Ipsum
author:
 - name: Pukar Bhandari
   email: pukar.bhandari@wfrc.utah.gov
   affiliation:
     - name: Wasatch Front Regional Council
       url: "https://wfrc.utah.gov/"
date: "2025-11-03"
---

# Setup Environment

This section prepares the Python environment with all necessary libraries and configurations. We'll import data manipulation libraries (pandas, numpy), and visualization tools (matplotlib, seaborn).

## Install Libraries

```python
!conda install -c conda-forge numpy pandas matplotlib seaborn requests pathlib BigQuery
```

## Import Libraries

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import importlib.util
from pathlib import Path
import sys
import os
import requests
import io

from dotenv import load_dotenv
load_dotenv()
```

## Environment Variables



## Helper Functions

```{python}
def fetch_github(
    url: str,
    mode: str = "private",
    token_env_var: str = "GITHUB_TOKEN"
) -> requests.Response:
    """
    Fetch content from GitHub repositories.

    Args:
        url: GitHub raw URL (e.g., https://raw.githubusercontent.com/...)
        mode: "public" for public repos, "private" for private repos requiring authentication
        token_env_var: Name of environment variable containing GitHub token (default: GITHUB_TOKEN)

    Returns:
        requests.Response object

    Raises:
        ValueError: If token is missing for private mode or invalid mode
        requests.HTTPError: If request fails
    """
    # Validate mode
    if mode not in ["public", "private"]:
        raise ValueError(f"mode must be 'public' or 'private', got '{mode}'")

    if mode == "public":
        response = requests.get(url, timeout=30)
    else:
        token = os.getenv(token_env_var)
        if not token:
            raise ValueError(
                f"GitHub token not found in environment variable '{token_env_var}'. "
                f"Check your .env file has: {token_env_var}=your_token_here"
            )

        headers = {
            'Authorization': f'token {token}',
            'Accept': 'application/vnd.github.v3.raw'
        }
        response = requests.get(url, headers=headers, timeout=30)

    response.raise_for_status()
    return response
```

```{python}
def calculate_categorical_shares(
    df,
    category_col,
    weight_col='LINKED_WEIGHT',
    group_cols=['Veh_Cat3p', 'Purp5_text', 'PK_OK'],
    filter_condition=None,
    category_mapping=None,
    prefix='share',
    exclude_daily_purposes=['HBW', 'HBO', 'NHB']  # Only HBC and HBSch get daily
):
    """
    Calculate weighted shares of categories within groups, ensuring they sum to 1.

    Parameters:
    -----------
    df : DataFrame
        Input dataframe
    category_col : str
        Column containing the categories to calculate shares for
    weight_col : str
        Column containing weights for each observation
    group_cols : list
        Columns to group by (e.g., vehicle ownership, purpose, peak/off-peak)
    filter_condition : str or None
        Optional pandas query string to filter data before calculation
    category_mapping : dict or None
        Optional mapping to rename/recode categories
    prefix : str
        Prefix for output column names
    exclude_daily_purposes : list
        Purposes that should NOT get daily aggregates (only peak/off-peak)

    Returns:
    --------
    DataFrame with shares in wide format
    """

    # Start with a copy
    df_work = df.copy()

    # Apply filter if provided
    if filter_condition:
        df_work = df_work.query(filter_condition)

    # Apply category mapping if provided
    if category_mapping:
        df_work[category_col] = df_work[category_col].map(category_mapping)
        # Remove unmapped values
        df_work = df_work.dropna(subset=[category_col])

    # Remove any NaN values in key columns
    df_work = df_work.dropna(subset=[category_col, weight_col] + group_cols)

    # Initialize results dictionary
    results = {}

    # Calculate weighted shares for each group combination
    for veh in sorted(df_work['Veh_Cat3p'].unique()):
        for purpose in sorted(df_work['Purp5_text'].unique()):
            for peak in sorted(df_work['PK_OK'].unique()):
                subset = df_work.query(
                    f"Veh_Cat3p == '{veh}' & Purp5_text == '{purpose}' & PK_OK == '{peak}'"
                )

                if len(subset) > 0:
                    # Calculate weighted counts by category
                    weighted_counts = subset.groupby(category_col)[weight_col].sum()
                    total_weight = subset[weight_col].sum()

                    for cat, weighted_count in weighted_counts.items():
                        var_name = f"{prefix}_{cat}_{veh}veh"
                        if var_name not in results:
                            results[var_name] = {}
                        results[var_name][(purpose, peak)] = weighted_count / total_weight

    # Calculate "_all" aggregates (across all vehicle categories)
    for purpose in sorted(df_work['Purp5_text'].unique()):
        for peak in sorted(df_work['PK_OK'].unique()):
            subset = df_work.query(f"Purp5_text == '{purpose}' & PK_OK == '{peak}'")

            if len(subset) > 0:
                weighted_counts = subset.groupby(category_col)[weight_col].sum()
                total_weight = subset[weight_col].sum()

                for cat, weighted_count in weighted_counts.items():
                    var_name = f"{prefix}_{cat}_all"
                    if var_name not in results:
                        results[var_name] = {}
                    results[var_name][(purpose, peak)] = weighted_count / total_weight

    # Calculate daily aggregates for HBC and HBSch only
    daily_purposes = [p for p in df_work['Purp5_text'].unique()
                     if p not in exclude_daily_purposes]

    for purpose in daily_purposes:
        subset = df_work.query(f"Purp5_text == '{purpose}'")

        if len(subset) > 0:
            # For each vehicle category
            for veh in sorted(df_work['Veh_Cat3p'].unique()):
                veh_subset = subset.query(f"Veh_Cat3p == '{veh}'")
                if len(veh_subset) > 0:
                    weighted_counts = veh_subset.groupby(category_col)[weight_col].sum()
                    total_weight = veh_subset[weight_col].sum()

                    for cat, weighted_count in weighted_counts.items():
                        var_name = f"{prefix}_{cat}_{veh}veh"
                        if var_name not in results:
                            results[var_name] = {}
                        results[var_name][(purpose, 'Daily')] = weighted_count / total_weight

            # For "all" vehicle categories
            weighted_counts = subset.groupby(category_col)[weight_col].sum()
            total_weight = subset[weight_col].sum()

            for cat, weighted_count in weighted_counts.items():
                var_name = f"{prefix}_{cat}_all"
                if var_name not in results:
                    results[var_name] = {}
                results[var_name][(purpose, 'Daily')] = weighted_count / total_weight

    # Convert to DataFrame
    df_result = pd.DataFrame(results).T
    df_result.columns = pd.MultiIndex.from_tuples(df_result.columns)
    df_result = df_result.fillna(0)

    # Sort index
    df_result = df_result.sort_index()

    return df_result
```

```{python}
# Verify categorical shares sum to 1.0000
def verify_shares(df_shares, tolerance=0.01):
    """
    Verify that shares sum to approximately 1 within each group.
    """
    print("=== Verifying Share Sums ===\n")

    # Check sums for each column
    all_good = True
    issues = []

    for col in df_shares.columns:
        col_sum = df_shares[col].sum()

        if abs(col_sum - 1.0) > tolerance:
            all_good = False
            issues.append((col, col_sum))

    if all_good:
        print("âœ… All shares sum to 1.0 (within tolerance)")
    else:
        print(f"âš ï¸  Warning: {len(issues)} columns don't sum to 1.0:\n")
        for col, sum_val in issues[:10]:  # Show first 10
            print(f"   {col}: {sum_val:.4f}")
        if len(issues) > 10:
            print(f"   ... and {len(issues) - 10} more")

    print(f"\nTotal columns: {len(df_shares.columns)}")
    print(f"Columns with correct sums: {len(df_shares.columns) - len(issues)}")

    return df_shares.sum(axis=0)
```

## Setup BigQuery

```{python}
#| eval: false
#| echo: false

# # Import global TDM functions from local 'Resource' clone

# import sys
# sys.path.insert(0, '../Resources/2-Python/global-functions')
# import BigQuery

# client = BigQuery.getBigQueryClient_Confidential2023UtahHTS()
```

```{python}
# # Import global TDM functions from remote 'Resource' repo

# Fetch and import BigQuery module
response = fetch_github(
    'https://raw.githubusercontent.com/WFRCAnalytics/Resources/refs/heads/master/2-Python/global-functions/BigQuery.py',
    mode="public"
)

BigQuery = importlib.util.module_from_spec(importlib.util.spec_from_loader('BigQuery', loader=None))
exec(response.text, BigQuery.__dict__)

# Initiatlize BigQuery Client
client = BigQuery.getBigQueryClient_Confidential2023UtahHTS()
```

# Load Data

## UTA On-Board Survey

```{python}
# Read Linked UTA On-Board Survey Data directly from GitHub repo
response = fetch_github(
    "https://raw.githubusercontent.com/WFRCAnalytics/DATA-OBS-Prep-For-TDM/refs/heads/main/_output/UTA_OBS_2024_Linked.csv",
    mode = "private"
)

df_obs_linked = pd.read_csv(io.StringIO(response.text))
df_obs_linked
```

## Household Travel Survey

```{python}
# Load Linked Trips data from 2023 Household Travel Survey
df_hts_linked = client.query(
    "SELECT * FROM " + 'wfrc-modeling-data.prd_tdm_hts_2023.trip_linked'
).to_dataframe()

df_hts_linked
```

# Understanding Mode Hierarchy

```{mermaid}
flowchart LR
  T["ðŸš¦ Total Trips"]

  %% Top-level split
  T --> M["Motorized"]
  T --> NM["Non-motorized"]

  %% Motorized branch
  subgraph S1["ðŸš— Motorized Transportation"]
    direction TB
    M --> A["Auto"]
    M --> TR["Transit"]

    %% Auto
    A --> DA["Drive alone"]
    A --> SR["Shared ride"]
    SR --> SR2["SR2<br/><i>2 people</i>"]
    SR --> SR3["SR3<br/><i>3+ people</i>"]

    %% Transit (two complementary views)
    subgraph S1a["ðŸšŒ Transit â€“ By Service Type"]
      direction TB
      TR --> TM["By service type"]
      TM --> LB["Local bus"]
      TM --> CB["Core bus"]
      TM --> EB["Express bus"]
      TM --> LRT["LRT<br/><i>light rail</i>"]
      TM --> CRT["CRT<br/><i>commuter rail</i>"]
      TM --> BRT["BRT<br/><i>bus rapid</i>"]
    end

    subgraph S1b["ðŸš¶ðŸš— Transit â€“ By Access Type"]
      direction TB
      TR --> TA["By access type"]

      %% Walk-to-transit
      TA --> WTT["ðŸš¶ Walk-to-transit"]
      WTT --> WLB["Walk â†’ Local bus"]
      WTT --> WCB["Walk â†’ Core bus"]
      WTT --> WEB["Walk â†’ Express bus"]
      WTT --> WLRT["Walk â†’ LRT"]
      WTT --> WCRT["Walk â†’ CRT"]
      WTT --> WBRT["Walk â†’ BRT"]

      %% Drive-to-transit
      TA --> DTT["ðŸš— Drive-to-transit"]
      DTT --> DLB["Drive â†’ Local bus"]
      DTT --> DCB["Drive â†’ Core bus"]
      DTT --> DEB["Drive â†’ Express bus"]
      DTT --> DLRT["Drive â†’ LRT"]
      DTT --> DCRT["Drive â†’ CRT"]
      DTT --> DBRT["Drive â†’ BRT"]
    end
  end

  %% Non-motorized branch
  subgraph S2["ðŸš¶ Non-motorized Transportation"]
    direction TB
    NM --> WALK["Walk"]
    NM --> BIKE["Bike"]
  end

  %% Styling
  classDef rootNode fill:#2c3e50,stroke:#34495e,stroke-width:3px,color:#fff,font-weight:bold,font-size:16px
  classDef motorizedNode fill:#3498db,stroke:#2980b9,stroke-width:2px,color:#fff,font-weight:bold
  classDef nonMotorizedNode fill:#27ae60,stroke:#229954,stroke-width:2px,color:#fff,font-weight:bold
  classDef autoNode fill:#e67e22,stroke:#d35400,stroke-width:2px,color:#fff
  classDef transitNode fill:#9b59b6,stroke:#8e44ad,stroke-width:2px,color:#fff
  classDef walkTransitNode fill:#1abc9c,stroke:#16a085,stroke-width:2px,color:#fff
  classDef driveTransitNode fill:#e74c3c,stroke:#c0392b,stroke-width:2px,color:#fff
  classDef leafNode fill:#ecf0f1,stroke:#95a5a6,stroke-width:1px,color:#2c3e50
  classDef subgraphStyle fill:#f8f9fa,stroke:#dee2e6,stroke-width:2px

  class T rootNode
  class M motorizedNode
  class NM nonMotorizedNode
  class A,DA,SR,SR2,SR3 autoNode
  class TR,TM,TA transitNode
  class WTT,WLB,WCB,WEB,WLRT,WCRT,WBRT walkTransitNode
  class DTT,DLB,DCB,DEB,DLRT,DCRT,DBRT driveTransitNode
  class LB,CB,EB,LRT,CRT,BRT,WALK,BIKE leafNode
```


# Identify Variable of Interest

## On-Board Survey

```{python}
df_obs_linked[[
    "Linked_Mode_txt", # Linked Mode: MT = MicroTransit (dont need), LCL = local bus, COR = Core Bus, EXP = Express Bus, LRT = Light Rail Transit (TRAX), CRT = Commuter Rail Transit (FrontRunner), BRT = Bus Rapid Transit (OVX, UVX)
    "Purp5_text", # Trip Purpose: HBW = Home-based Work, HBO = Home-based Other, HBC = Home-based College, NHB = Non-home based, HBSch = Home-based School
    "PK_OK", # Peak vs OffPeak
    "Veh_Cat3p", # Vehicle Ownership: 0 = 0 Vehicle Household, 1 = 1 Vehicle Household, 2 = 2 Vehicle Household, 3 = 3+ Vehicle Household
    "Mode_Fin", # Current Mode: 1 = MT = MicroTransit (Dont need),  4 = LCL = local bus, 5 = COR = Core Bus, 6 = EXP = Express Bus, 7 = LRT = Light Rail Transit (TRAX), 8 = CRT = Commuter Rail Transit, 9 = BRT = Bus Rapid Transit (OVX, UVX)
    "Ac_Mode_Model", # Access Mode to Transit: Walk (Walk to Transit), Drive (Drive to Transit)
]]
```

# Trips: Motorized vs Non-motorized



## Non-motorized Trips: Walk vs Bike



## Motorized Trips: Auto vs Transit



### Auto Trips: Drive Alone vs Shared Rides



#### Shared Rides: 2 People vs 3+ People



### Transit Trips: By Service and By Access Mode

#### By Service Type

```{python}
# 1. Transit by Service Type (within linked transit trips)
print("\n" + "=" * 60)
print("1. TRANSIT BY SERVICE TYPE")
print("=" * 60)

service_mapping = {
    'LCL': 'local',
    'COR': 'core',
    'EXP': 'express',
    'LRT': 'lrt',
    'CRT': 'crt',
    'BRT': 'brt'
}

df_transit_by_service = calculate_categorical_shares(
    df_obs_linked,
    category_col='Linked_Mode_txt',
    weight_col='LINKED_WEIGHT',
    group_cols=['Veh_Cat3p', 'Purp5_text', 'PK_OK'],
    filter_condition='Linked_Mode_txt != "MT"',
    category_mapping=service_mapping,
    prefix='calib_share'
)

print("\nShape:", df_transit_by_service.shape)
print("\nFirst few rows:")
print(df_transit_by_service.head(10))
print("\nVerification:")
verify_shares(df_transit_by_service)
```

#### By Access Mode: Walk vs Drive to Transit

```{python}
# 2. Transit by Access Type (Walk vs Drive) (within linked transit trips)
print("\n" + "=" * 60)
print("2. TRANSIT BY ACCESS TYPE")
print("=" * 60)

access_mapping = {
    'Walk': 'walkacc',
    'Drive': 'driveacc'
}

df_transit_by_access = calculate_categorical_shares(
    df_obs_linked,
    category_col='Ac_Mode_Model',
    weight_col='LINKED_WEIGHT',
    group_cols=['Veh_Cat3p', 'Purp5_text', 'PK_OK'],
    filter_condition='Linked_Mode_txt != "MT"',
    category_mapping=access_mapping,
    prefix='calib_share'
)

print("\nShape:", df_transit_by_access.shape)
print("\nFirst few rows:")
print(df_transit_by_access.head(10))
print("\nVerification:")
verify_shares(df_transit_by_access)
```

##### Walk to Transit

```{python}
# 3. Walk-to-Transit by Service Type (within walk-to-transit trips)
print("\n" + "=" * 60)
print("3. WALK-TO-TRANSIT BY SERVICE TYPE")
print("=" * 60)

walk_service_mapping = {
    'LCL': 'walk-local',
    'EXP': 'walk-express',
    'LRT': 'walk-lrt',
    'CRT': 'walk-crt',
    'BRT': 'walk-brt'
}

df_walk_to_transit = calculate_categorical_shares(
    df_obs_linked,
    category_col='Linked_Mode_txt',
    weight_col='LINKED_WEIGHT',
    group_cols=['Veh_Cat3p', 'Purp5_text', 'PK_OK'],
    filter_condition='Ac_Mode_Model == "Walk" & Linked_Mode_txt != "MT"',
    category_mapping=walk_service_mapping,
    prefix='calib_share'
)


print("\nShape:", df_walk_to_transit.shape)
print("\nFirst few rows:")
print(df_walk_to_transit.head(10))
print("\nVerification:")
verify_shares(df_walk_to_transit)
```

##### Drive to Transit


```{python}
# 4. Drive-to-Transit by Service Type (within drive-to-transit trips)
print("\n" + "=" * 60)
print("4. DRIVE-TO-TRANSIT BY SERVICE TYPE")
print("=" * 60)

drive_service_mapping = {
    'LCL': 'drive-local',
    'EXP': 'drive-express',
    'LRT': 'drive-lrt',
    'CRT': 'drive-crt',
    'BRT': 'drive-brt'
}

df_drive_to_transit = calculate_categorical_shares(
    df_obs_linked,
    category_col='Linked_Mode_txt',
    weight_col='LINKED_WEIGHT',
    group_cols=['Veh_Cat3p', 'Purp5_text', 'PK_OK'],
    filter_condition='Ac_Mode_Model == "Drive" & Linked_Mode_txt != "MT"',
    category_mapping=drive_service_mapping,
    prefix='calib_share'
)

print("\nShape:", df_drive_to_transit.shape)
print("\nFirst few rows:")
print(df_drive_to_transit.head(10))
print("\nVerification:")
verify_shares(df_drive_to_transit)
```
