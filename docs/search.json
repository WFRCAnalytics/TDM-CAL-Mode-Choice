[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "",
    "text": "1 Step 1: Distribution of CRT Travel Distances\n\n\n2 Step 2: Calculate Mode Choice Target Parameters"
  },
  {
    "objectID": "1-crt-trip-distances.html",
    "href": "1-crt-trip-distances.html",
    "title": "Explore Distribution of CRT Travel Distances",
    "section": "",
    "text": "This section establishes the Python environment with all required libraries for geospatial analysis and data processing. Beyond standard data manipulation tools, we’ll need geopandas for spatial operations and regular expressions for parsing the non-transit links file. The helper functions defined here will be used throughout the analysis for spatial matching and file parsing.\n\n\n!conda install -c conda-forge numpy pandas geopandas matplotlib seaborn shapely requests pathlib\n\n\n\n\n\nShow the code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom matplotlib.ticker import FuncFormatter\nimport seaborn as sns\nimport geopandas as gpd\nfrom pathlib import Path\nimport glob\nfrom dbfread import DBF\nimport re\nimport os\nimport requests\nimport io\n\nimport openmatrix as omx\nimport subprocess\n\n\n\n\n\n\n\nShow the code\nPROJECT_CRS = \"EPSG:3566\"  # NAD83 / Utah North (ftUS)\n\n\n\n\n\n\n\nShow the code\ndef fetch_github(\n    url: str,\n    mode: str = \"private\",\n    token_env_var: str = \"GITHUB_TOKEN\"\n) -&gt; requests.Response:\n    \"\"\"\n    Fetch content from GitHub repositories.\n\n    Args:\n        url: GitHub raw URL (e.g., https://raw.githubusercontent.com/...)\n        mode: \"public\" for public repos, \"private\" for private repos requiring authentication\n        token_env_var: Name of environment variable containing GitHub token (default: GITHUB_TOKEN)\n\n    Returns:\n        requests.Response object\n\n    Raises:\n        ValueError: If token is missing for private mode or invalid mode\n        requests.HTTPError: If request fails\n    \"\"\"\n    # Validate mode\n    if mode not in [\"public\", \"private\"]:\n        raise ValueError(f\"mode must be 'public' or 'private', got '{mode}'\")\n\n    if mode == \"public\":\n        response = requests.get(url, timeout=30)\n    else:\n        token = os.getenv(token_env_var)\n        if not token:\n            raise ValueError(\n                f\"GitHub token not found in environment variable '{token_env_var}'. \"\n                f\"Check your .env file has: {token_env_var}=your_token_here\"\n            )\n\n        headers = {\n            'Authorization': f'token {token}',\n            'Accept': 'application/vnd.github.v3.raw'\n        }\n        response = requests.get(url, headers=headers, timeout=30)\n\n    response.raise_for_status()\n    return response"
  },
  {
    "objectID": "1-crt-trip-distances.html#install-libraries",
    "href": "1-crt-trip-distances.html#install-libraries",
    "title": "Explore Distribution of CRT Travel Distances",
    "section": "",
    "text": "!conda install -c conda-forge numpy pandas geopandas matplotlib seaborn shapely requests pathlib"
  },
  {
    "objectID": "1-crt-trip-distances.html#import-libraries",
    "href": "1-crt-trip-distances.html#import-libraries",
    "title": "Explore Distribution of CRT Travel Distances",
    "section": "",
    "text": "Show the code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom matplotlib.ticker import FuncFormatter\nimport seaborn as sns\nimport geopandas as gpd\nfrom pathlib import Path\nimport glob\nfrom dbfread import DBF\nimport re\nimport os\nimport requests\nimport io\n\nimport openmatrix as omx\nimport subprocess"
  },
  {
    "objectID": "1-crt-trip-distances.html#environment-variables",
    "href": "1-crt-trip-distances.html#environment-variables",
    "title": "Explore Distribution of CRT Travel Distances",
    "section": "",
    "text": "Show the code\nPROJECT_CRS = \"EPSG:3566\"  # NAD83 / Utah North (ftUS)"
  },
  {
    "objectID": "1-crt-trip-distances.html#helper-functions",
    "href": "1-crt-trip-distances.html#helper-functions",
    "title": "Explore Distribution of CRT Travel Distances",
    "section": "",
    "text": "Show the code\ndef fetch_github(\n    url: str,\n    mode: str = \"private\",\n    token_env_var: str = \"GITHUB_TOKEN\"\n) -&gt; requests.Response:\n    \"\"\"\n    Fetch content from GitHub repositories.\n\n    Args:\n        url: GitHub raw URL (e.g., https://raw.githubusercontent.com/...)\n        mode: \"public\" for public repos, \"private\" for private repos requiring authentication\n        token_env_var: Name of environment variable containing GitHub token (default: GITHUB_TOKEN)\n\n    Returns:\n        requests.Response object\n\n    Raises:\n        ValueError: If token is missing for private mode or invalid mode\n        requests.HTTPError: If request fails\n    \"\"\"\n    # Validate mode\n    if mode not in [\"public\", \"private\"]:\n        raise ValueError(f\"mode must be 'public' or 'private', got '{mode}'\")\n\n    if mode == \"public\":\n        response = requests.get(url, timeout=30)\n    else:\n        token = os.getenv(token_env_var)\n        if not token:\n            raise ValueError(\n                f\"GitHub token not found in environment variable '{token_env_var}'. \"\n                f\"Check your .env file has: {token_env_var}=your_token_here\"\n            )\n\n        headers = {\n            'Authorization': f'token {token}',\n            'Accept': 'application/vnd.github.v3.raw'\n        }\n        response = requests.get(url, headers=headers, timeout=30)\n\n    response.raise_for_status()\n    return response"
  },
  {
    "objectID": "1-crt-trip-distances.html#taz-shapefile",
    "href": "1-crt-trip-distances.html#taz-shapefile",
    "title": "Explore Distribution of CRT Travel Distances",
    "section": "2.1 TAZ Shapefile",
    "text": "2.1 TAZ Shapefile\n\n\nShow the code\ngdf_taz_v10 = gpd.read_file(\n    r\"_data/TAZ/WFv910_TAZ.shp\"\n).to_crs(PROJECT_CRS)\n\n\n\n\nShow the code\ngdf_taz_v10.explore()"
  },
  {
    "objectID": "1-crt-trip-distances.html#processed-on-board-survey-data",
    "href": "1-crt-trip-distances.html#processed-on-board-survey-data",
    "title": "Explore Distribution of CRT Travel Distances",
    "section": "2.2 Processed On-Board Survey Data",
    "text": "2.2 Processed On-Board Survey Data\nThis dataset is the output from the “previous step”, which contains all the calculated access and egress distances and times.\n\n\nShow the code\n# Read Linked UTA On-Board Survey Data directly from GitHub repo\nresponse = fetch_github(\n    \"https://raw.githubusercontent.com/WFRCAnalytics/DATA-OBS-Prep-For-TDM/refs/heads/main/_output/UTA_OBS_2024_Linked_FactorAdjusted.csv\",\n    mode = \"private\"\n)\n\ndf_obs_linked = pd.read_csv(io.StringIO(response.text))\n\n# Filter only the Weekday trips\ndf_obs_linked = df_obs_linked[df_obs_linked[\"DATE_TYPE\"] == \"Weekday\"]\ndf_obs_linked\n\n\nC:\\Users\\Pukar.Bhandari\\AppData\\Local\\Temp\\ipykernel_14160\\3049727246.py:7: DtypeWarning:\n\nColumns (8,24,32,37,54,132,137,174,287) have mixed types. Specify dtype option on import or set low_memory=False.\n\n\n\n\n\n\n\n\n\n\nID\nDATE_COMPLETED\nDATE_TYPE\nROUTE_DIRECTION_Code\nROUTE_DIRECTION\nROUTE_DIRECTION_Other\nHOME_ADDRESS_CITY\nHOME_ADDRESS_STATE\nHOME_ADDRESS_ZIP\nHOME_ADDRESS_LAT\n...\na_Stop_lat\na_Stop_lon\np_Stop_N\na_Stop_N\naccess_dist\naccess_time\negress_dist\negress_time\nAdj_Factor\ntrip_weight\n\n\n\n\n0\n5948\n2024-02-27\nWeekday\nUTA_1_2_00\n2 200 SOUTH - TO U HOSPITAL\nNaN\nClearfield\nUT\n84015\n41.119565\n...\n40.769221\n-111.898663\n27702.0\n25493.0\n0.37\n8.79\n0.68\n16.27\n1.195057\n7.988529\n\n\n1\n6067\n2024-02-27\nWeekday\nUTA_1_47_00\n47 4700 SOUTH - TO W VALLEY CTL\nNaN\nWest Valley City\nUT\n84119\n40.689590\n...\n40.660941\n-111.899443\n23195.0\n23685.0\n0.56\n13.53\n0.24\n5.70\n1.407280\n4.420834\n\n\n3\n6073\n2024-02-28\nWeekday\nUTA_1_750_01\nFRONTRUNNER 750 - SOUTHBOUND\nNaN\nOgden\nUT\n84401\n41.236749\n...\n40.659758\n-111.896432\n10042.0\n10016.0\n4.17\n8.22\n1.37\n3.29\n1.192511\n5.413118\n\n\n4\n6077\n2024-02-28\nWeekday\nUTA_1_47_00\n47 4700 SOUTH - TO W VALLEY CTL\nNaN\nWest Valley City\nUT\n84119\n40.669687\n...\n40.681907\n-112.024041\n22922.0\n22421.0\n0.25\n6.00\n0.49\n11.88\n1.407280\n10.487378\n\n\n5\n6078\n2024-02-28\nWeekday\nUTA_1_220_00\n220 HIGHLAND DRIVE / 1300 EAST - TO SANDY\nNaN\nSalt Lake City\nUT\n84101\n40.765122\n...\n40.687097\n-111.866122\n15115.0\n24517.0\n0.27\n6.51\n0.19\n4.55\n1.407280\n3.287619\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n13790\n30339\n2024-04-26\nWeekday\nUTA_1_704_01\nTRAX GREEN LINE 704 - TO AIRPORT\nNaN\nSalt Lake City\nUT\n84116\n40.772420\n...\n40.771502\n-111.914486\n15101.0\n15122.0\n0.31\n7.53\n0.17\n4.07\n1.405314\n8.528545\n\n\n13791\n30340\n2024-04-26\nWeekday\nUTA_1_704_00\nTRAX GREEN LINE 704 - TO WEST VALLEY\nNaN\nSalt Lake City\nUT\n84116\n40.771361\n...\n40.750058\n-111.896818\n15122.0\n15109.0\n0.46\n11.00\n1.21\n29.10\n1.405314\n16.406476\n\n\n13792\n30341\n2024-04-26\nWeekday\nUTA_1_704_00\nTRAX GREEN LINE 704 - TO WEST VALLEY\nNaN\nSalt Lake City\nUT\n84116\n40.773232\n...\n40.725345\n-111.877499\n15122.0\n24968.0\n0.17\n4.07\n0.86\n20.68\n1.405314\n8.203238\n\n\n13793\n1000002\n2024-02-29\nWeekday\nUTA_1_701_00\nTRAX BLUE LINE 701 - TO DRAPER\nNaN\nSalt Lake City\nUT\n84111\n40.751607\n...\n40.674859\n-111.943134\n15123.0\n23342.0\n0.33\n7.95\n0.29\n7.02\n0.893041\n4.139760\n\n\n13794\n1000004\n2024-02-29\nWeekday\nUTA_1_701_00\nTRAX BLUE LINE 701 - TO DRAPER\nNaN\nSalt Lake City\nUT\n84109\n40.722068\n...\n40.525496\n-111.858805\n25663.0\n15041.0\n0.79\n18.85\n0.19\n4.44\n1.405314\n4.676107\n\n\n\n\n12020 rows × 309 columns"
  },
  {
    "objectID": "1-crt-trip-distances.html#transit-network-nodes",
    "href": "1-crt-trip-distances.html#transit-network-nodes",
    "title": "Explore Distribution of CRT Travel Distances",
    "section": "2.3 Transit Network Nodes",
    "text": "2.3 Transit Network Nodes\nThe transit network comes from the travel demand model’s master network. This network contains all possible locations where transit vehicles can stop. We need to match survey stop locations to these official network nodes to calculate accurate walk distances from the model’s non-transit links.\n\nLoad MasterNetwork Nodes\nThe MasterNetwork contains all nodes in the regional transportation network - roadway intersections, transit stops, park-and-ride locations, etc. We load the complete network first, then filter to just transit-related nodes.\n\n\nShow the code\n# Load all nodes from MasterNetwork\ngdf_masternet_nodes = gpd.read_file(\n    # Source: \\\\ModelAce\\ModelAce-E\\1 - TDM\\1 - Official Release (full run)\\v9x\\v9.1\\v9.1.1\\WF-TDM-v9.1.1 - revised-se\\Scenarios\\OY_2023\\0_InputProcessing\\ScenarioNet\n    r\"_data/ScenarioNet/WFv911-revised-se_OY_2023 - Node.shp\"\n).to_crs(PROJECT_CRS)\n\n\n\n\nShow the code\ngdf_masternet_nodes.explore()\n\n\n\n\nFilter to Transit Stop Nodes\nNot all nodes in the MasterNetwork are transit stops. The c_StopsAllModes.dbf file contains a list of node IDs that represent actual transit stops. We filter the network to only these nodes, giving us the set of locations where we can calculate walk access/egress distances.\n\n\nShow the code\n# Read all mode-specific stop files and combine into one dataframe\ndf_stop_nodes_list = pd.concat(\n    # Source: \"\\\\ModelAce\\ModelAce-E\\1 - TDM\\1 - Official Release (full run)\\v9x\\v9.1\\v9.1.1\\WF-TDM-v9.1.1 - revised-se\\Scenarios\\OY_2023\\0_InputProcessing\\\"\n    [pd.DataFrame(DBF(file)).assign(MODE=int(re.search(r'Mode(\\d+)', file).group(1)))\n     for file in glob.glob(r'_data/Stops/c_StopsMode[4-9].dbf')],\n    ignore_index=True\n)\n\ndf_stop_nodes_list['N'] = df_stop_nodes_list['N'].astype('Int64')\n\n\n\n\nShow the code\ndf_stop_nodes_list\n\n\n\n\n\n\n\n\n\nN\nX\nY\nMODE\nROUTE\n\n\n\n\n0\n28936\n418003.0308\n4565514.975\n4\nNaN\n\n\n1\n28909\n417986.8035\n4565124.024\n4\nNaN\n\n\n2\n28888\n417981.0000\n4564879.000\n4\nNaN\n\n\n3\n28870\n417977.5915\n4564656.512\n4\nNaN\n\n\n4\n28851\n417964.0000\n4564428.000\n4\nNaN\n\n\n...\n...\n...\n...\n...\n...\n\n\n1900\n67371\n442160.3193\n4458319.613\n9\nNaN\n\n\n1901\n67380\n440904.0313\n4458351.939\n9\nNaN\n\n\n1902\n67383\n440091.0000\n4458361.924\n9\nNaN\n\n\n1903\n67421\n439380.5328\n4458629.972\n9\nNaN\n\n\n1904\n50029\n438356.2813\n4459074.432\n9\nNaN\n\n\n\n\n1905 rows × 5 columns\n\n\n\n\n\nShow the code\n# Join to get geometry for each N-MODE combination\ndf_stop_nodes = gdf_masternet_nodes[['N', 'geometry']].merge(\n    df_stop_nodes_list[['N', 'MODE']],\n    on='N',\n    how='inner',\n    validate=\"1:m\" # multiple stops (by mode) can have same stop id and geometry\n)\n\n\n\n\nShow the code\ndf_stop_nodes.explore()"
  },
  {
    "objectID": "1-crt-trip-distances.html#transit-trip-matrix",
    "href": "1-crt-trip-distances.html#transit-trip-matrix",
    "title": "Explore Distribution of CRT Travel Distances",
    "section": "2.4 Transit Trip Matrix",
    "text": "2.4 Transit Trip Matrix\n\nConvert Matrix Format using _ConvertMatToOmx.s.\nThis only needs to run once.\n\n\n\n\n\n\nTip\n\n\n\nSkim matrices were copied from \\\\ModelAce\\ModelAce-E\\1 - TDM\\1 - Official Release (full run)\\v9x\\v9.1\\v9.1.1\\WF-TDM-v9.1.1 - revised-se\\Scenarios\\OY_2023\\4_ModeChoice\\1a_Skims (For this exact run, skim matrices were used from modified NTL runs.)\n\n\n\n\nShow the code\n# Define output files to check\noutput_files = [\n    \"_data/Skims/omx/skm_d8_Ok.omx\",\n    \"_data/Skims/omx/skm_d8_Pk.omx\",\n    \"_data/Skims/omx/skm_w8_flag_Ok.omx\",\n    \"_data/Skims/omx/skm_w8_flag_Pk.omx\",\n    \"_data/Skims/omx/skm_w8_Ok.omx\",\n    \"_data/Skims/omx/skm_w8_Pk.omx\",\n]\n\n# Check if all output files exist\nif not all(Path(f).exists() for f in output_files):\n    print(\"Converting matrices to OMX format...\")\n    subprocess.run(\n        [r\"C:\\Program Files\\Citilabs\\CubeVoyager\\Voyager.exe\",\n         r\"_data\\_ConvertMatToOmx.s\"],\n        check=True\n    )\n    print(\"Conversion complete!\")\nelse:\n    print(\"OMX files already exist, skipping conversion.\")\n\n\nOMX files already exist, skipping conversion."
  },
  {
    "objectID": "1-crt-trip-distances.html#read-matrices",
    "href": "1-crt-trip-distances.html#read-matrices",
    "title": "Explore Distribution of CRT Travel Distances",
    "section": "2.5 Read Matrices",
    "text": "2.5 Read Matrices\n\n\nShow the code\n# Read Matrices as DataFrames\nomx_w8_pk = pd.DataFrame(omx.open_file(r\"_data\\Skims\\omx\\skm_w8_Pk.omx\")['D8'])\nomx_w8_ok = pd.DataFrame(omx.open_file(r\"_data\\Skims\\omx\\skm_w8_Ok.omx\")['D8'])\nomx_d8_pk = pd.DataFrame(omx.open_file(r\"_data\\Skims\\omx\\skm_d8_Pk.omx\")['D8'])\nomx_d8_ok = pd.DataFrame(omx.open_file(r\"_data\\Skims\\omx\\skm_d8_Ok.omx\")['D8'])"
  },
  {
    "objectID": "1-crt-trip-distances.html#assign-which-matrix-to-process-for-each-row",
    "href": "1-crt-trip-distances.html#assign-which-matrix-to-process-for-each-row",
    "title": "Explore Distribution of CRT Travel Distances",
    "section": "4.1 Assign which matrix to process for each row",
    "text": "4.1 Assign which matrix to process for each row\n\n\nShow the code\n# Create uses_crt column once\ndf_obs_linked['uses_crt'] = df_obs_linked[['FirstMode', 'SecndMode', 'ThirdMode', 'FourthMode', 'FifthMode', 'LastMode']].eq('CRT').any(axis=1)"
  },
  {
    "objectID": "1-crt-trip-distances.html#calculate-crt-distances",
    "href": "1-crt-trip-distances.html#calculate-crt-distances",
    "title": "Explore Distribution of CRT Travel Distances",
    "section": "4.2 Calculate CRT Distances",
    "text": "4.2 Calculate CRT Distances\n\n\nShow the code\n# Calculate CRT Distances - simplified with composite matrices\ndf_obs_linked['valid_crt_mask'] = (\n    df_obs_linked['uses_crt'] &\n    df_obs_linked['p_TAZID'].notna() &\n    df_obs_linked['a_TAZID'].notna() &\n    df_obs_linked['trip_weight'].notna()  # Add trip_weight check here\n)\n\n\n\n\nShow the code\n# Initialize column\ndf_obs_linked['dist_CRT'] = np.nan\n\n# Process peak periods\npk_mask = df_obs_linked['valid_crt_mask'] & df_obs_linked['Period'].isin(['AM', 'PM'])\nif pk_mask.any():\n    row_indices = (df_obs_linked.loc[pk_mask, 'p_TAZID'] - 1).astype(int)\n    col_indices = (df_obs_linked.loc[pk_mask, 'a_TAZID'] - 1).astype(int)\n    df_obs_linked.loc[pk_mask, 'dist_CRT'] = skim_pk.values[row_indices, col_indices]\n\n# Process off-peak periods\nok_mask = df_obs_linked['valid_crt_mask'] & df_obs_linked['Period'].isin(['MD', 'EV'])\nif ok_mask.any():\n    row_indices = (df_obs_linked.loc[ok_mask, 'p_TAZID'] - 1).astype(int)\n    col_indices = (df_obs_linked.loc[ok_mask, 'a_TAZID'] - 1).astype(int)\n    df_obs_linked.loc[ok_mask, 'dist_CRT'] = skim_ok.values[row_indices, col_indices]\n\ndf_obs_linked[['dist_CRT']]\n\n\n\n\n\n\n\n\n\ndist_CRT\n\n\n\n\n0\nNaN\n\n\n1\nNaN\n\n\n3\n44.26\n\n\n4\nNaN\n\n\n5\nNaN\n\n\n...\n...\n\n\n13790\nNaN\n\n\n13791\nNaN\n\n\n13792\nNaN\n\n\n13793\nNaN\n\n\n13794\nNaN\n\n\n\n\n12020 rows × 1 columns\n\n\n\n\n\nShow the code\n(df_obs_linked[df_obs_linked['dist_CRT'] == 0]['trip_weight'].sum() / df_obs_linked['trip_weight'].sum()) * 100\n\n\n0.2164957499507889\n\n\n\n\nShow the code\ncrt_dist_count = (\n    df_obs_linked[['ID', 'PK_OK', 'Ac_Mode2_Model', 'Eg_Mode2_Model', 'p_TAZID', 'a_TAZID', 'dist_CRT']]\n    # .replace(0, np.nan)\n    .dropna()\n    .sort_values('dist_CRT')\n)\n\ncrt_dist_count\n\n\n\n\n\n\n\n\n\nID\nPK_OK\nAc_Mode2_Model\nEg_Mode2_Model\np_TAZID\na_TAZID\ndist_CRT\n\n\n\n\n8500\n21613\nOK\nDrive\nWalk\n1981.0\n965.0\n0.00\n\n\n2893\n11857\nOK\nDrive\nWalk\n607.0\n720.0\n0.00\n\n\n12284\n27908\nPK\nWalk\nWalk\n1719.0\n1987.0\n0.00\n\n\n12315\n27973\nPK\nWalk\nWalk\n2434.0\n2725.0\n0.00\n\n\n12364\n28075\nOK\nDrive\nWalk\n883.0\n1036.0\n0.00\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n621\n7534\nPK\nDrive\nDrive\n413.0\n2968.0\n81.08\n\n\n10396\n24688\nPK\nDrive\nWalk\n2990.0\n437.0\n81.08\n\n\n11054\n25871\nPK\nDrive\nDrive\n2961.0\n390.0\n81.08\n\n\n2338\n10844\nPK\nDrive\nDrive\n377.0\n3010.0\n81.08\n\n\n445\n7112\nPK\nDrive\nWalk\n249.0\n2939.0\n81.08\n\n\n\n\n2303 rows × 7 columns\n\n\n\n\n\nShow the code\nimport folium\nfrom folium import Marker, CircleMarker, FeatureGroup, PolyLine\nimport pandas as pd\nimport numpy as np\n\n# ============================================================================\n# COLOR AND STYLE CONFIGURATION\n# ============================================================================\nMODE_STYLES = {\n    1: {'color': '#5b80a3', 'weight': 2},   # MT (Microtransit)\n    4: {'color': '#00508b', 'weight': 3},   # LCL (Local Bus) - UTA Blue\n    5: {'color': '#FF6B35', 'weight': 3},   # COR (Core Bus)\n    6: {'color': '#6fb74d', 'weight': 3},   # EXP (Express Bus)\n    7: {'color': '#ce132d', 'weight': 4},   # LRT (Light Rail) - UTA Red\n    8: {'color': '#9c4392', 'weight': 6},   # CRT (Commuter Rail) - UTA Green\n    9: {'color': '#007dbb', 'weight': 5}    # BRT (Bus Rapid Transit)\n}\n\nACCESS_EGRESS_STYLES = {\n    'Walk': {'color': '#898a8d', 'weight': 2},\n    'KNR': {'color': '#FFA500', 'weight': 2},\n    'PNR': {'color': '#FF6B35', 'weight': 2},\n    'Drive': {'color': '#FF6B35', 'weight': 2}\n}\n\nTRANSFER_WALK_STYLE = {'color': '#898a8d', 'weight': 2}\nSNAP_LINE_STYLE = {'color': '#00CED1', 'weight': 2, 'dash_array': '5, 5'}  # Dashed cyan for snap connections\nSURVEYED_TRIP_WEIGHT_MULTIPLIER = 1.5\n\n# ============================================================================\n# SETUP\n# ============================================================================\nRESPONSE_ID = 21613\n\ndf_plot = df_obs_linked[df_obs_linked[\"ID\"] == RESPONSE_ID].iloc[0]\n\n# Determine center\nif pd.notna(df_plot['p_lat']) and pd.notna(df_plot['p_lon']):\n    center_lat, center_lon = df_plot['p_lat'], df_plot['p_lon']\nelif pd.notna(df_plot['a_lat']) and pd.notna(df_plot['a_lon']):\n    center_lat, center_lon = df_plot['a_lat'], df_plot['a_lon']\nelif pd.notna(df_plot['HOME_ADDRESS_LAT']) and pd.notna(df_plot['HOME_ADDRESS_LONG']):\n    center_lat, center_lon = df_plot['HOME_ADDRESS_LAT'], df_plot['HOME_ADDRESS_LONG']\nelse:\n    center_lat, center_lon = 40.7608, -111.8910\n    print(\"Warning: No valid coordinates found, using default center\")\n\nm = folium.Map(location=[center_lat, center_lon], zoom_start=13, tiles='OpenStreetMap')\n\n# Create feature groups\nfg_taz = FeatureGroup(name='Relevant TAZs', show=True)\nfg_all_nodes = FeatureGroup(name='All Transit Network Nodes', show=False)\nfg_stops = FeatureGroup(name='Transit Stops Used', show=True)\nfg_snapped_stops = FeatureGroup(name='Snapped Stop Nodes (from Network)', show=True)\nfg_snap_connections = FeatureGroup(name='Snap Connections', show=True)\nfg_addresses = FeatureGroup(name='Addresses', show=True)\nfg_trip_lines = FeatureGroup(name='Trip Sequence', show=True)\n\n# ============================================================================\n# SHOW RELEVANT TAZs\n# ============================================================================\nif pd.notna(df_plot['p_TAZID']):\n    p_taz = gdf_taz_v10[gdf_taz_v10['TAZID'] == df_plot['p_TAZID']]\n    if len(p_taz) &gt; 0:\n        p_geojson = p_taz.to_crs('EPSG:4326').__geo_interface__\n        folium.GeoJson(\n            p_geojson,\n            style_function=lambda x: {\n                'fillColor': 'darkgreen',\n                'color': 'darkgreen',\n                'weight': 2,\n                'fillOpacity': 0.15\n            },\n            tooltip=f\"Production TAZ: {df_plot['p_TAZID']}\"\n        ).add_to(fg_taz)\n\nif pd.notna(df_plot['a_TAZID']):\n    a_taz = gdf_taz_v10[gdf_taz_v10['TAZID'] == df_plot['a_TAZID']]\n    if len(a_taz) &gt; 0:\n        a_geojson = a_taz.to_crs('EPSG:4326').__geo_interface__\n        folium.GeoJson(\n            a_geojson,\n            style_function=lambda x: {\n                'fillColor': 'darkblue',\n                'color': 'darkblue',\n                'weight': 2,\n                'fillOpacity': 0.15\n            },\n            tooltip=f\"Attraction TAZ: {df_plot['a_TAZID']}\"\n        ).add_to(fg_taz)\n\n# ============================================================================\n# ADD ALL TRANSIT NETWORK NODES (BACKGROUND LAYER)\n# ============================================================================\n# Get unique node geometries (since same node can serve multiple modes)\nunique_nodes = (\n    df_stop_nodes\n    .groupby('N')\n    .agg(\n        geometry=('geometry', 'first'),\n        MODE=('MODE', lambda s: ', '.join(str(m) for m in sorted(s.dropna().unique())))\n    )\n    .reset_index()\n)\n\nunique_nodes_gdf = gpd.GeoDataFrame(unique_nodes, geometry='geometry', crs=df_stop_nodes.crs)\nunique_nodes_gdf_wgs84 = unique_nodes_gdf.to_crs('EPSG:4326')\n\n# Add all nodes as small gray circles\nfor _, node in unique_nodes_gdf_wgs84.iterrows():\n    folium.CircleMarker(\n        location=[node.geometry.y, node.geometry.x],\n        radius=4,\n        color='#444444',\n        fill=True,\n        fillColor='#444444',\n        fillOpacity=0.75,\n        weight=0.75,\n        popup=f\"Node: {node['N']}\\nMode: {node['MODE']}\",\n        tooltip=f\"Node {node['N']}\"\n    ).add_to(fg_all_nodes)\n\n# ============================================================================\n# HELPER FUNCTIONS\n# ============================================================================\ndef get_mode_name(mode_num):\n    mode_names = {\n        1: 'MT (Microtransit)',\n        4: 'LCL (Local)',\n        5: 'COR (Corridor)',\n        6: 'EXP (Express)',\n        7: 'LRT (Light Rail)',\n        8: 'CRT (FrontRunner)',\n        9: 'BRT (Rapid Transit)'\n    }\n    return mode_names.get(mode_num, f'Mode {mode_num}')\n\ndef get_mode_color(mode_num):\n    return MODE_STYLES.get(mode_num, {}).get('color', '#666666')\n\ndef get_mode_weight(mode_num):\n    return MODE_STYLES.get(mode_num, {}).get('weight', 3)\n\ndef add_line_segment(coords_list, color, weight, dash_array, label, popup_text, feature_group):\n    valid_coords = []\n    for coord in coords_list:\n        if coord is not None and len(coord) == 2:\n            lat, lon = coord\n            if pd.notna(lat) and pd.notna(lon):\n                valid_coords.append([lat, lon])\n\n    if len(valid_coords) &gt;= 2:\n        folium.PolyLine(\n            locations=valid_coords,\n            color=color,\n            weight=weight,\n            opacity=0.8,\n            dash_array=dash_array,\n            popup=popup_text,\n            tooltip=label\n        ).add_to(feature_group)\n        return True\n    return False\n\ndef add_stop_marker(lat, lon, label, mode_num, feature_group):\n    if pd.notna(lat) and pd.notna(lon):\n        folium.CircleMarker(\n            location=[lat, lon],\n            radius=6,\n            color=get_mode_color(mode_num),\n            fill=True,\n            fillColor=get_mode_color(mode_num),\n            fillOpacity=0.9,\n            popup=label,\n            tooltip=label\n        ).add_to(feature_group)\n\n# ============================================================================\n# BUILD TRIP SEQUENCE\n# ============================================================================\nis_pa = df_plot['PA_AP'] == 'PA'\ntrip_sequence = []\n\nif is_pa:\n    segments = [\n        ('PREV_TRAN_1', 'Modefrom1', 'Transfer 1 (from origin)'),\n        ('PREV_TRAN_2', 'Modefrom2', 'Transfer 2 (from origin)'),\n        ('PREV_TRAN_3', 'Modefrom3', 'Transfer 3 (from origin)'),\n        ('STOP', 'Mode_Fin', 'SURVEYED TRIP'),\n        ('NEXT_TRAN_1', 'Modeto1', 'Transfer 1 (to destination)'),\n        ('NEXT_TRAN_2', 'Modeto2', 'Transfer 2 (to destination)'),\n        ('NEXT_TRAN_3', 'Modeto3', 'Transfer 3 (to destination)'),\n    ]\nelse:\n    segments = [\n        ('NEXT_TRAN_3', 'Modeto3', 'Transfer 3'),\n        ('NEXT_TRAN_2', 'Modeto2', 'Transfer 2'),\n        ('NEXT_TRAN_1', 'Modeto1', 'Transfer 1'),\n        ('STOP', 'Mode_Fin', 'SURVEYED TRIP'),\n        ('PREV_TRAN_3', 'Modefrom3', 'Transfer 1'),\n        ('PREV_TRAN_2', 'Modefrom2', 'Transfer 2'),\n        ('PREV_TRAN_1', 'Modefrom1', 'Transfer 3'),\n    ]\n\nfor prefix, mode_col, label in segments:\n    if prefix == 'STOP':\n        if is_pa:\n            on_lat = df_plot.get('STOP_ON_LAT')\n            on_lon = df_plot.get('STOP_ON_LONG')\n            off_lat = df_plot.get('STOP_OFF_LAT')\n            off_lon = df_plot.get('STOP_OFF_LONG')\n        else:\n            on_lat = df_plot.get('STOP_OFF_LAT')\n            on_lon = df_plot.get('STOP_OFF_LONG')\n            off_lat = df_plot.get('STOP_ON_LAT')\n            off_lon = df_plot.get('STOP_ON_LONG')\n    else:\n        if is_pa:\n            on_lat = df_plot.get(f'{prefix}_ON_BUS_LAT')\n            on_lon = df_plot.get(f'{prefix}_ON_BUS_LONG')\n            off_lat = df_plot.get(f'{prefix}_OFF_BUS_LAT')\n            off_lon = df_plot.get(f'{prefix}_OFF_BUS_LONG')\n        else:\n            on_lat = df_plot.get(f'{prefix}_OFF_BUS_LAT')\n            on_lon = df_plot.get(f'{prefix}_OFF_BUS_LONG')\n            off_lat = df_plot.get(f'{prefix}_ON_BUS_LAT')\n            off_lon = df_plot.get(f'{prefix}_ON_BUS_LONG')\n\n    mode_num = df_plot.get(mode_col)\n\n    if pd.notna(on_lat) and pd.notna(off_lat) and pd.notna(mode_num):\n        trip_sequence.append({\n            'on_lat': on_lat,\n            'on_lon': on_lon,\n            'off_lat': off_lat,\n            'off_lon': off_lon,\n            'mode_num': int(mode_num),\n            'mode_name': get_mode_name(mode_num),\n            'label': label,\n            'is_current': prefix == 'STOP'\n        })\n\n# ============================================================================\n# ADD SNAPPED NETWORK STOPS\n# ============================================================================\n# Get snapped stop nodes from df_stop_nodes\nsnapped_nodes = []\n\n# Production Stop (Access - where trip starts from network perspective)\nif pd.notna(df_plot['p_Stop_N']):\n    p_stop_matches = df_stop_nodes[df_stop_nodes['N'] == df_plot['p_Stop_N']]\n    if len(p_stop_matches) &gt; 0:\n        node = p_stop_matches.iloc[0]\n        node_geom = node.geometry\n        if df_stop_nodes.crs != 'EPSG:4326':\n            node_gdf = gpd.GeoDataFrame([node], geometry='geometry', crs=df_stop_nodes.crs)\n            node_geom = node_gdf.to_crs('EPSG:4326').iloc[0].geometry\n\n        modes = p_stop_matches['MODE'].unique()\n        mode_names = [get_mode_name(m) for m in modes]\n\n        folium.Marker(\n            location=[node_geom.y, node_geom.x],\n            icon=folium.Icon(color='green', icon='circle', prefix='fa'),\n            popup=f\"Access Stop Node: {df_plot['p_Stop_N']}&lt;br&gt;Modes: {', '.join(mode_names)}\",\n            tooltip=f\"Network Node {df_plot['p_Stop_N']} (Access)\"\n        ).add_to(fg_snapped_stops)\n\n        snapped_nodes.append({\n            'type': 'access',\n            'lat': node_geom.y,\n            'lon': node_geom.x,\n            'node_id': df_plot['p_Stop_N']\n        })\n\n# Attraction Stop (Egress - where trip ends from network perspective)\nif pd.notna(df_plot['a_Stop_N']):\n    a_stop_matches = df_stop_nodes[df_stop_nodes['N'] == df_plot['a_Stop_N']]\n    if len(a_stop_matches) &gt; 0:\n        node = a_stop_matches.iloc[0]\n        node_geom = node.geometry\n        if df_stop_nodes.crs != 'EPSG:4326':\n            node_gdf = gpd.GeoDataFrame([node], geometry='geometry', crs=df_stop_nodes.crs)\n            node_geom = node_gdf.to_crs('EPSG:4326').iloc[0].geometry\n\n        modes = a_stop_matches['MODE'].unique()\n        mode_names = [get_mode_name(m) for m in modes]\n\n        folium.Marker(\n            location=[node_geom.y, node_geom.x],\n            icon=folium.Icon(color='blue', icon='circle', prefix='fa'),\n            popup=f\"Egress Stop Node: {df_plot['a_Stop_N']}&lt;br&gt;Modes: {', '.join(mode_names)}\",\n            tooltip=f\"Network Node {df_plot['a_Stop_N']} (Egress)\"\n        ).add_to(fg_snapped_stops)\n\n        snapped_nodes.append({\n            'type': 'egress',\n            'lat': node_geom.y,\n            'lon': node_geom.x,\n            'node_id': df_plot['a_Stop_N']\n        })\n\n# ============================================================================\n# DRAW TRIP WITH SNAP CONNECTIONS\n# ============================================================================\n\n# 1. Access segment (Production Address → Snapped Access Stop)\nif len(snapped_nodes) &gt; 0 and snapped_nodes[0]['type'] == 'access':\n    access_node = snapped_nodes[0]\n    access_mode = df_plot['Ac_Mode3_Model']\n    access_style = ACCESS_EGRESS_STYLES.get(access_mode, {'color': '#666666', 'weight': 3})\n\n    add_line_segment(\n        [[df_plot['p_lat'], df_plot['p_lon']],\n         [access_node['lat'], access_node['lon']]],\n        color=access_style['color'],\n        weight=access_style['weight'],\n        dash_array='8, 4',\n        label=f'Access: {access_mode}',\n        popup_text=f\"Access: {access_mode}&lt;br&gt;To Network Node: {access_node['node_id']}&lt;br&gt;Distance: {df_plot.get('access_dist', 'N/A'):.2f} mi\",\n        feature_group=fg_trip_lines\n    )\n\n    # Snap connection (Access Node → First Transit Stop)\n    if len(trip_sequence) &gt; 0:\n        first_stop = trip_sequence[0]\n        add_line_segment(\n            [[access_node['lat'], access_node['lon']],\n             [first_stop['on_lat'], first_stop['on_lon']]],\n            color=SNAP_LINE_STYLE['color'],\n            weight=SNAP_LINE_STYLE['weight'],\n            dash_array=SNAP_LINE_STYLE['dash_array'],\n            label='Access Snap Connection',\n            popup_text=f\"Connection: Network Node → Surveyed Stop\",\n            feature_group=fg_snap_connections\n        )\n\n# 2. Transit segments and transfers\nfor i, segment in enumerate(trip_sequence):\n    add_stop_marker(\n        segment['on_lat'],\n        segment['on_lon'],\n        f\"Board: {segment['label']}&lt;br&gt;{segment['mode_name']}\",\n        segment['mode_num'],\n        fg_stops\n    )\n\n    base_weight = get_mode_weight(segment['mode_num'])\n    weight = int(base_weight * SURVEYED_TRIP_WEIGHT_MULTIPLIER) if segment['is_current'] else base_weight\n\n    add_line_segment(\n        [[segment['on_lat'], segment['on_lon']],\n         [segment['off_lat'], segment['off_lon']]],\n        color=get_mode_color(segment['mode_num']),\n        weight=weight,\n        dash_array=None,\n        label=f\"{segment['label']}: {segment['mode_name']}\",\n        popup_text=f\"{segment['label']}&lt;br&gt;{segment['mode_name']}\" +\n                   (\"&lt;br&gt;&lt;b&gt;★ SURVEYED TRIP ★&lt;/b&gt;\" if segment['is_current'] else \"\"),\n        feature_group=fg_trip_lines\n    )\n\n    add_stop_marker(\n        segment['off_lat'],\n        segment['off_lon'],\n        f\"Alight: {segment['label']}&lt;br&gt;{segment['mode_name']}\",\n        segment['mode_num'],\n        fg_stops\n    )\n\n    if i &lt; len(trip_sequence) - 1:\n        next_segment = trip_sequence[i + 1]\n        add_line_segment(\n            [[segment['off_lat'], segment['off_lon']],\n             [next_segment['on_lat'], next_segment['on_lon']]],\n            color=TRANSFER_WALK_STYLE['color'],\n            weight=TRANSFER_WALK_STYLE['weight'],\n            dash_array='2, 6',\n            label='Transfer Walk',\n            popup_text='Walking between stops',\n            feature_group=fg_trip_lines\n        )\n\n# 3. Egress segment (Last Transit Stop → Snapped Egress Stop → Attraction Address)\nif len(trip_sequence) &gt; 0:\n    last_stop = trip_sequence[-1]\n    egress_mode = df_plot['Eg_Mode3_Model']\n    egress_style = ACCESS_EGRESS_STYLES.get(egress_mode, {'color': '#666666', 'weight': 3})\n\n    # Find egress node\n    egress_node = next((n for n in snapped_nodes if n['type'] == 'egress'), None)\n\n    if egress_node:\n        # Snap connection (Last Transit Stop → Egress Node)\n        add_line_segment(\n            [[last_stop['off_lat'], last_stop['off_lon']],\n             [egress_node['lat'], egress_node['lon']]],\n            color=SNAP_LINE_STYLE['color'],\n            weight=SNAP_LINE_STYLE['weight'],\n            dash_array=SNAP_LINE_STYLE['dash_array'],\n            label='Egress Snap Connection',\n            popup_text=f\"Connection: Surveyed Stop → Network Node\",\n            feature_group=fg_snap_connections\n        )\n\n        # Egress walk (Egress Node → Attraction Address)\n        add_line_segment(\n            [[egress_node['lat'], egress_node['lon']],\n             [df_plot['a_lat'], df_plot['a_lon']]],\n            color=egress_style['color'],\n            weight=egress_style['weight'],\n            dash_array='8, 4',\n            label=f'Egress: {egress_mode}',\n            popup_text=f\"Egress: {egress_mode}&lt;br&gt;From Network Node: {egress_node['node_id']}&lt;br&gt;Distance: {df_plot.get('egress_dist', 'N/A'):.2f} mi\",\n            feature_group=fg_trip_lines\n        )\n\n# ============================================================================\n# ADD ADDRESS MARKERS\n# ============================================================================\nif pd.notna(df_plot['HOME_ADDRESS_LAT']) and pd.notna(df_plot['HOME_ADDRESS_LONG']):\n    folium.Marker(\n        location=[df_plot['HOME_ADDRESS_LAT'], df_plot['HOME_ADDRESS_LONG']],\n        icon=folium.Icon(color='red', icon='home', prefix='fa'),\n        popup='Home Address',\n        tooltip='Home'\n    ).add_to(fg_addresses)\n\nif pd.notna(df_plot['p_lat']) and pd.notna(df_plot['p_lon']):\n    folium.Marker(\n        location=[df_plot['p_lat'], df_plot['p_lon']],\n        icon=folium.Icon(color='darkgreen', icon='play', prefix='fa'),\n        popup=f\"Production Address&lt;br&gt;TAZ: {df_plot['p_TAZID']}&lt;br&gt;Purpose: {df_plot['Purp5_text']}\",\n        tooltip='Production (Trip Start)'\n    ).add_to(fg_addresses)\n\nif pd.notna(df_plot['a_lat']) and pd.notna(df_plot['a_lon']):\n    folium.Marker(\n        location=[df_plot['a_lat'], df_plot['a_lon']],\n        icon=folium.Icon(color='darkblue', icon='stop', prefix='fa'),\n        popup=f\"Attraction Address&lt;br&gt;TAZ: {df_plot['a_TAZID']}&lt;br&gt;Purpose: {df_plot['Purp5_text']}\",\n        tooltip='Attraction (Trip End)'\n    ).add_to(fg_addresses)\n\n# ============================================================================\n# ADD LAYERS\n# ============================================================================\nfg_taz.add_to(m)\nfg_all_nodes.add_to(m)  # Add all network nodes layer\nfg_trip_lines.add_to(m)\nfg_snap_connections.add_to(m)\nfg_stops.add_to(m)\nfg_snapped_stops.add_to(m)  # Snapped nodes on top\nfg_addresses.add_to(m)\n\nfolium.LayerControl(collapsed=False).add_to(m)\n\n# ============================================================================\n# ADD TRIP SUMMARY\n# ============================================================================\nmode_sequence_html = \"\"\nfor i, seg in enumerate(trip_sequence, 1):\n    color = get_mode_color(seg['mode_num'])\n    star = \" ★\" if seg['is_current'] else \"\"\n    mode_sequence_html += f'{i}. &lt;span style=\"color:{color}\"&gt;●&lt;/span&gt; {seg[\"mode_name\"]}{star}&lt;br&gt;'\n\ntrip_summary = f\"\"\"\n&lt;div style=\"position: fixed;\n     top: 10px; left: 60px; width: 340px;\n     background-color: white; border:2px solid grey; z-index:9999;\n     font-size:13px; padding: 10px; font-family: Arial, sans-serif;\"&gt;\n&lt;b style=\"font-size: 15px;\"&gt;Trip ID: {df_plot['ID']}&lt;/b&gt;&lt;br&gt;\n&lt;hr style=\"margin: 5px 0;\"&gt;\n&lt;b&gt;Trip Details:&lt;/b&gt;&lt;br&gt;\nDirection: {df_plot['PA_AP']} (P → A)&lt;br&gt;\nPurpose: {df_plot['Purp5_text']}&lt;br&gt;\nPeriod: {df_plot['Period']}&lt;br&gt;\nLinked Mode: {df_plot['Linked_Mode_txt']}&lt;br&gt;\nTotal Transfers: {df_plot['Total_Xfer']}&lt;br&gt;\n&lt;hr style=\"margin: 5px 0;\"&gt;\n&lt;b&gt;Access/Egress:&lt;/b&gt;&lt;br&gt;\nAccess: {df_plot['Ac_Mode3_Model']} ({df_plot.get('access_dist', 'N/A'):.2f} mi)&lt;br&gt;\nEgress: {df_plot['Eg_Mode3_Model']} ({df_plot.get('egress_dist', 'N/A'):.2f} mi)&lt;br&gt;\n&lt;hr style=\"margin: 5px 0;\"&gt;\n&lt;b&gt;Transit Sequence:&lt;/b&gt;&lt;br&gt;\n{mode_sequence_html}\n&lt;hr style=\"margin: 5px 0;\"&gt;\n&lt;small&gt;\n&lt;span style=\"color:{SNAP_LINE_STYLE['color']}\"&gt;- - -&lt;/span&gt; = Network snap connection&lt;br&gt;\n&lt;span style=\"color:{TRANSFER_WALK_STYLE['color']}\"&gt;⋯⋯&lt;/span&gt; = Transfer walk&lt;br&gt;\n&lt;span style=\"color:#898a8d\"&gt;▬ ▬&lt;/span&gt; = Access/Egress walk\n&lt;/small&gt;\n&lt;/div&gt;\n\"\"\"\nm.get_root().html.add_child(folium.Element(trip_summary))\n\nm"
  },
  {
    "objectID": "1-crt-trip-distances.html#assign-crt-class-for-each-bins",
    "href": "1-crt-trip-distances.html#assign-crt-class-for-each-bins",
    "title": "Explore Distribution of CRT Travel Distances",
    "section": "4.3 Assign CRT Class for each bins",
    "text": "4.3 Assign CRT Class for each bins\n\n\nShow the code\n# Create CRT distance classes using pd.cut\ndf_obs_linked['class_CRT'] = pd.cut(\n    df_obs_linked['dist_CRT'],\n    bins=[0, 10, 20, 30, 40, 50, 60, 70, 80, np.inf],\n    labels=['CRT_00', 'CRT_01', 'CRT_02', 'CRT_03', 'CRT_04', 'CRT_05', 'CRT_06', 'CRT_07', 'CRT_08'],\n    right=True,  # intervals are (a, b] - include right edge\n    include_lowest=False  # exclude 0 from first bin\n)\n\n# Check the distribution\ndf_obs_linked['class_CRT'].value_counts().sort_index()\n\n\nclass_CRT\nCRT_00    168\nCRT_01    309\nCRT_02    489\nCRT_03    799\nCRT_04    369\nCRT_05     63\nCRT_06     47\nCRT_07     25\nCRT_08      8\nName: count, dtype: int64"
  },
  {
    "objectID": "1-crt-trip-distances.html#static",
    "href": "1-crt-trip-distances.html#static",
    "title": "Explore Distribution of CRT Travel Distances",
    "section": "5.1 Static",
    "text": "5.1 Static\n\n\nShow the code\n# ============================================================================\n# Trip Length Frequency\n# ============================================================================\n\n# Set seaborn style\nsns.set_style(\"whitegrid\")\nsns.set_context(\"notebook\", font_scale=1.1)\n\nplt.figure(figsize=(10, 6))\n\n# Filter relevant data\ndf_valid = df_obs_linked[\n    (df_obs_linked['dist_CRT'].notna()) #& # filter out NaNs and\n    # (df_obs_linked['dist_CRT'] &gt; 0) # only non-zero distances\n]\n\n# Create histogram with KDE\nax = sns.histplot(\n    data=df_valid,\n    x='dist_CRT',\n    weights=\"trip_weight\",\n    bins=(lambda x: range(0, int(df_valid['dist_CRT'].max()) + x, x))(5),\n    kde=True,\n    color='steelblue',\n    edgecolor='white',\n    linewidth=0.5,\n    alpha=0.7\n)\n\n# Add 95th percentile line using sorted cumulative weights\ndf_sorted = df_valid.sort_values('dist_CRT')\ncumsum = df_sorted['trip_weight'].cumsum()\npct95 = df_sorted.loc[cumsum &gt;= cumsum.iloc[-1] * 0.95, 'dist_CRT'].iloc[0]\n\nplt.axvline(pct95, color='red', linestyle='--', linewidth=2, label=f'95th percentile: {pct95:.1f} mi')\nplt.legend()\n\n# Add grid for better readability\nplt.grid(axis='y', alpha=0.3, linestyle='--')\n\n# Customize\nplt.title('Distribution of CRT Travel Distances', fontsize=16, fontweight='bold', pad=20)\nplt.xlabel('Distance (miles)', fontsize=12)\nplt.ylabel('Frequency', fontsize=12)\n\nsns.despine()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "1-crt-trip-distances.html#interactive",
    "href": "1-crt-trip-distances.html#interactive",
    "title": "Explore Distribution of CRT Travel Distances",
    "section": "5.2 Interactive",
    "text": "5.2 Interactive\n\n\nShow the code\n# ============================================================================\n# Data Preparation and Library Imports\n# ============================================================================\n\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport numpy as np\nimport pandas as pd\n\n# Filter valid CRT data\ndf_crt_valid = df_obs_linked[\n    (df_obs_linked['dist_CRT'].notna()) &\n    (df_obs_linked['trip_weight'].notna())\n].copy()\n\n# Create distance bins for clustered bar chart\ndf_crt_valid['dist_bin'] = pd.cut(\n    df_crt_valid['dist_CRT'],\n    bins=[0, 10, 20, 30, 40, 50, 60, 70, 80, np.inf],\n    labels=['0-10', '10-20', '20-30', '30-40', '40-50', '50-60', '60-70', '70-80', '80+'],\n    right=True\n)\n\n# Helper function for weighted histograms\ndef create_weighted_bins(distances, weights, bin_size=5, max_dist=None):\n    \"\"\"Create weighted histogram bins\"\"\"\n    if max_dist is None:\n        max_dist = distances.max()\n    bins = np.arange(0, max_dist + bin_size, bin_size)\n    bin_centers = bins[:-1] + bin_size/2\n    hist_values, _ = np.histogram(distances, bins=bins, weights=weights)\n    return bin_centers, hist_values\n\n# Prepare data for all visualizations\nmax_dist = df_crt_valid['dist_CRT'].max()\nbin_size = 5\nperiods = sorted(df_crt_valid['PK_OK'].dropna().unique())\npurposes = sorted(df_crt_valid['Purp5_text'].dropna().unique())\ndistance_bins = ['0-10', '10-20', '20-30', '30-40', '40-50', '50-60', '60-70', '70-80', '80+']\n\n# Color schemes\ncolors_period = {'PK': '#E74C3C', 'OK': '#3498DB'}\ncolors_purpose = px.colors.qualitative.Set2[:len(purposes)]\n\n\n\n\nShow the code\n# ============================================================================\n# Interactive Histogram by Period\n# ============================================================================\n\nfig_period = go.Figure()\n\nfor period in periods:\n    df_period = df_crt_valid[df_crt_valid['PK_OK'] == period]\n    bin_centers, hist_values = create_weighted_bins(\n        df_period['dist_CRT'].values,\n        df_period['trip_weight'].values,\n        bin_size=bin_size,\n        max_dist=max_dist\n    )\n\n    fig_period.add_trace(\n        go.Bar(\n            x=bin_centers,\n            y=hist_values,\n            name=f'{period} Period',\n            marker_color=colors_period.get(period, '#95A5A6'),\n            opacity=0.85,  # Slightly less transparent for stacked bars\n            width=bin_size * 0.9,\n            hovertemplate='&lt;b&gt;%{fullData.name}&lt;/b&gt;&lt;br&gt;' +\n                          'Distance: %{x:.1f} mi&lt;br&gt;' +\n                          'Weighted Trips: %{y:.0f}&lt;br&gt;' +\n                          '&lt;extra&gt;&lt;/extra&gt;'\n        )\n    )\n\nfig_period.update_layout(\n    title={\n        'text': '&lt;b&gt;CRT Trip Length Distribution by Period&lt;/b&gt;&lt;br&gt;&lt;sub&gt;Click legend items to toggle on/off&lt;/sub&gt;',\n        'x': 0.5,\n        'xanchor': 'center',\n        'font': {'size': 16}\n    },\n    xaxis=dict(\n        title='Distance (miles)',\n        range=[0, max_dist + 5]\n    ),\n    yaxis=dict(\n        title='Weighted Trip Frequency',\n        gridcolor='lightgray'\n    ),\n    height=675,\n    width=1200,\n    template='plotly_white',\n    hovermode='closest',\n    barmode='stack',  # Changed to stack\n    showlegend=True,\n    legend=dict(\n        orientation='v',\n        yanchor='top',\n        y=1,\n        xanchor='left',\n        x=1.02,\n        bgcolor='rgba(255, 255, 255, 0.9)',\n        bordercolor='gray',\n        borderwidth=1\n    )\n)\n\nfig_period.show()\n\n\n                            \n                                            \n\n\n\n\nShow the code\n# ============================================================================\n# Interactive Histogram by Purpose\n# ============================================================================\n\nfig_purpose = go.Figure()\n\nfor i, purpose in enumerate(purposes):\n    df_purpose = df_crt_valid[df_crt_valid['Purp5_text'] == purpose]\n    bin_centers, hist_values = create_weighted_bins(\n        df_purpose['dist_CRT'].values,\n        df_purpose['trip_weight'].values,\n        bin_size=bin_size,\n        max_dist=max_dist\n    )\n\n    fig_purpose.add_trace(\n        go.Bar(\n            x=bin_centers,\n            y=hist_values,\n            name=purpose,\n            marker_color=colors_purpose[i],\n            opacity=0.85,  # Slightly less transparent for stacked bars\n            width=bin_size * 0.9,\n            hovertemplate='&lt;b&gt;%{fullData.name}&lt;/b&gt;&lt;br&gt;' +\n                          'Distance: %{x:.1f} mi&lt;br&gt;' +\n                          'Weighted Trips: %{y:.0f}&lt;br&gt;' +\n                          '&lt;extra&gt;&lt;/extra&gt;'\n        )\n    )\n\nfig_purpose.update_layout(\n    title={\n        'text': '&lt;b&gt;CRT Trip Length Distribution by Purpose&lt;/b&gt;&lt;br&gt;&lt;sub&gt;Click legend items to toggle on/off&lt;/sub&gt;',\n        'x': 0.5,\n        'xanchor': 'center',\n        'font': {'size': 16}\n    },\n    xaxis=dict(\n        title='Distance (miles)',\n        range=[0, max_dist + 5]\n    ),\n    yaxis=dict(\n        title='Weighted Trip Frequency',\n        gridcolor='lightgray'\n    ),\n    height=675,\n    width=1200,\n    template='plotly_white',\n    hovermode='closest',\n    barmode='stack',  # Changed to stack\n    showlegend=True,\n    legend=dict(\n        orientation='v',\n        yanchor='top',\n        y=1,\n        xanchor='left',\n        x=1.02,\n        bgcolor='rgba(255, 255, 255, 0.9)',\n        bordercolor='gray',\n        borderwidth=1\n    )\n)\n\nfig_purpose.show()\n\n\n                            \n                                            \n\n\n\n\nShow the code\n# ============================================================================\n# Clustered Stacked Bar Chart by Period and Purpose\n# ============================================================================\n\n# Aggregate data\nagg_data = df_crt_valid.groupby(['dist_bin', 'PK_OK', 'Purp5_text'], observed=True)['trip_weight'].sum().reset_index()\n\n# Create figure\nfig_cluster = go.Figure()\n\n# Bar width settings\npk_width = 0.35\nok_width = pk_width * 0.7\ngap = 0.15\n\n# Create traces\nfor i, purpose in enumerate(purposes):\n    for j, period in enumerate(periods):\n        data_subset = agg_data[\n            (agg_data['Purp5_text'] == purpose) &\n            (agg_data['PK_OK'] == period)\n        ]\n        data_subset = data_subset.set_index('dist_bin').reindex(distance_bins, fill_value=0).reset_index()\n\n        base_x = np.arange(len(distance_bins))\n\n        if period == 'PK':\n            x_offset = -gap/2 - pk_width/2\n            bar_width = pk_width\n        else:\n            x_offset = gap/2 + ok_width/2\n            bar_width = ok_width\n\n        x_positions = base_x + x_offset\n\n        fig_cluster.add_trace(go.Bar(\n            name=purpose,\n            x=x_positions,\n            y=data_subset['trip_weight'],\n            marker_color=colors_purpose[i],\n            opacity=0.85,\n            width=bar_width,\n            legendgroup=purpose,\n            showlegend=(j == 0),\n            offsetgroup=period,\n            hovertemplate='&lt;b&gt;%{fullData.legendgroup}&lt;/b&gt;&lt;br&gt;' +\n                          'Period: ' + period + '&lt;br&gt;' +\n                          'Distance: %{text}&lt;br&gt;' +\n                          'Weighted Trips: %{y:.0f}&lt;br&gt;' +\n                          '&lt;extra&gt;&lt;/extra&gt;',\n            text=data_subset['dist_bin']\n        ))\n\n# Update layout\nfig_cluster.update_layout(\n    title={\n        'text': '&lt;b&gt;CRT Trip Length Distribution by Distance, Period, and Purpose&lt;/b&gt;&lt;br&gt;&lt;sub&gt;Wider bars = Peak | Narrower bars = Off-Peak&lt;/sub&gt;',\n        'x': 0.5,\n        'xanchor': 'center',\n        'font': {'size': 16}\n    },\n    xaxis=dict(\n        title='Distance Range (miles)',\n        tickmode='array',\n        tickvals=np.arange(len(distance_bins)),\n        ticktext=distance_bins,\n        tickangle=0\n    ),\n    yaxis=dict(\n        title='Weighted Trip Frequency',\n        gridcolor='lightgray'\n    ),\n    barmode='stack',\n    height=900,\n    width=1600,\n    template='plotly_white',\n    hovermode='closest',\n    legend=dict(\n        title=dict(text='&lt;b&gt;Trip Purpose&lt;/b&gt;&lt;br&gt;&lt;i&gt;(Click to toggle)&lt;/i&gt;', font=dict(size=12)),\n        orientation='v',\n        yanchor='top',\n        y=1,\n        xanchor='left',\n        x=1.02\n    ),\n    plot_bgcolor='white',\n    bargap=0.3\n)\n\nfig_cluster.show()"
  },
  {
    "objectID": "1-crt-trip-distances.html#save-obs-data-with-crt-links",
    "href": "1-crt-trip-distances.html#save-obs-data-with-crt-links",
    "title": "Explore Distribution of CRT Travel Distances",
    "section": "6.1 Save OBS data with CRT Links",
    "text": "6.1 Save OBS data with CRT Links\n\n\nShow the code\n# Create output directory if it doesn't exist\noutput_dir = Path(\"_output\")\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# Export to CSV\ndf_obs_linked.to_csv(\n    output_dir / \"UTA_OBS_2024_Linked_FactorAdjusted_CRT.csv\",\n    index=False\n)\n\nprint(f\"Exported {len(df_obs_linked)} records\")\n\n\nExported 12020 records\n\n\n\n\n\n\n\n\nTipDownload the output file:\n\n\n\nUTA_OBS_2024_Linked_FactorAdjusted_CRT.csv"
  },
  {
    "objectID": "2-mode-choice-targets.html",
    "href": "2-mode-choice-targets.html",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "",
    "text": "This section prepares the Python environment with all necessary libraries and configurations. We’ll import data manipulation libraries (pandas, numpy), and visualization tools (matplotlib, seaborn).\n\n\n!conda install -c conda-forge numpy pandas matplotlib seaborn requests pathlib BigQuery\n\n\n\n\n\nShow the code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport importlib.util\nfrom pathlib import Path\nimport json\nimport sys\nimport os\nimport requests\nimport io\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\n\nTrue\n\n\n\n\n\n\n\nShow the code\n# Export mode control\nEXPORT_MODE = \"only_needed\"  # Options: \"export_all\", \"comment_unused\", \"only_needed\"\n\n# Optional suffix removal (when no vehicle disaggregation exists in entire file)\nREMOVE_ALL_SUFFIX = True\n\n\n\n\nShow the code\n# Variable-level vehicle ownership patterns\n# Format: {purpose: {period: {variable_family: [list of suffixes needed]}}}\n# Suffixes: '0veh', '1veh', '2veh', '3veh' for disaggregation, 'all' for aggregated\n\n# Load vehicle-level vehicle ownership patterns from JSON configuration file\nconfig_file = 'vehicle_pattern_config.json'\n\nwith open(config_file, 'r') as f:\n        variable_veh_patterns = json.load(f)\n\n\n\n\n\n\n\nShow the code\ndef fetch_github(\n    url: str,\n    mode: str = \"private\",\n    token_env_var: str = \"GITHUB_TOKEN\"\n) -&gt; requests.Response:\n    \"\"\"\n    Fetch content from GitHub repositories.\n\n    Args:\n        url: GitHub raw URL (e.g., https://raw.githubusercontent.com/...)\n        mode: \"public\" for public repos, \"private\" for private repos requiring authentication\n        token_env_var: Name of environment variable containing GitHub token (default: GITHUB_TOKEN)\n\n    Returns:\n        requests.Response object\n\n    Raises:\n        ValueError: If token is missing for private mode or invalid mode\n        requests.HTTPError: If request fails\n    \"\"\"\n    # Validate mode\n    if mode not in [\"public\", \"private\"]:\n        raise ValueError(f\"mode must be 'public' or 'private', got '{mode}'\")\n\n    if mode == \"public\":\n        response = requests.get(url, timeout=30)\n    else:\n        token = os.getenv(token_env_var)\n        if not token:\n            raise ValueError(\n                f\"GitHub token not found in environment variable '{token_env_var}'. \"\n                f\"Check your .env file has: {token_env_var}=your_token_here\"\n            )\n\n        headers = {\n            'Authorization': f'token {token}',\n            'Accept': 'application/vnd.github.v3.raw'\n        }\n        response = requests.get(url, headers=headers, timeout=30)\n\n    response.raise_for_status()\n    return response\n\n\n\n\nShow the code\ndef calculate_categorical_shares(\n    df,\n    category_col,\n    weight_col='trip_weight',\n    group_cols=['Veh_Cat3p', 'Purp5_text', 'PK_OK'],\n    filter_condition=None,\n    category_mapping=None,\n    prefix='share',\n    purpose_by_period=['HBW', 'HBO', 'NHB'],  # Only HBC and HBSch get daily\n    purpose_daily=['HBC', 'HBSch']  # NEW: Purposes that only get daily (no peak/off-peak)\n):\n    \"\"\"\n    Calculate weighted shares of categories within groups, ensuring they sum to 1.\n\n    Parameters:\n    -----------\n    df : DataFrame\n        Input dataframe\n    category_col : str\n        Column containing the categories to calculate shares for\n    weight_col : str\n        Column containing weights for each observation\n    group_cols : list\n        Columns to group by (e.g., vehicle ownership, purpose, peak/off-peak)\n    filter_condition : str or None\n        Optional pandas query string to filter data before calculation\n    category_mapping : dict or None\n        Optional mapping to rename/recode categories\n    prefix : str\n        Prefix for output column names\n    purpose_by_period : list\n        Purposes that should NOT get daily aggregates\n    purpose_daily : list\n        Purposes that ONLY get daily aggregates (no peak/off-peak breakdown)\n\n    Returns:\n    --------\n    DataFrame with shares in wide format\n    \"\"\"\n\n    # Apply filter and copy in one step\n    if filter_condition:\n        df_work = df.query(filter_condition).copy()\n    else:\n        df_work = df.copy()\n\n    # Apply category mapping if provided\n    if category_mapping:\n        df_work[category_col] = df_work[category_col].map(category_mapping)\n        # Remove unmapped values\n        df_work = df_work.dropna(subset=[category_col])\n\n    # Remove any NaN values in key columns\n    df_work = df_work.dropna(subset=[category_col, weight_col] + group_cols)\n\n    # Convert Veh_Cat3p to string format for consistency\n    df_work['Veh_Cat3p'] = df_work['Veh_Cat3p'].astype(str)\n    # Map 3 to 3+ for display\n    # df_work['Veh_Cat3p'] = df_work['Veh_Cat3p'].replace('3', '3+')\n\n    # Initialize results dictionary\n    results = {}\n\n    # Calculate weighted shares for each group combination (peak/off-peak)\n    # BUT skip purpose_daily\n    for veh in sorted(df_work['Veh_Cat3p'].unique()):\n        for purpose in sorted(df_work['Purp5_text'].unique()):\n            # Skip peak/off-peak for daily-only purposes\n            if purpose in purpose_daily:\n                continue\n\n            for peak in sorted(df_work['PK_OK'].unique()):\n                subset = df_work.query(\n                    f\"Veh_Cat3p == '{veh}' & Purp5_text == '{purpose}' & PK_OK == '{peak}'\"\n                )\n\n                if len(subset) &gt; 0:\n                    # Calculate weighted counts by category\n                    weighted_counts = subset.groupby(category_col)[weight_col].sum()\n                    total_weight = subset[weight_col].sum()\n\n                    for cat, weighted_count in weighted_counts.items():\n                        var_name = f\"{prefix}_{cat}_{veh}veh\"\n                        if var_name not in results:\n                            results[var_name] = {}\n                        results[var_name][(purpose, peak)] = weighted_count / total_weight\n\n    # Calculate \"_all\" aggregates for peak/off-peak (excluding daily-only purposes)\n    for purpose in sorted(df_work['Purp5_text'].unique()):\n        # Skip peak/off-peak for daily-only purposes\n        if purpose in purpose_daily:\n            continue\n\n        for peak in sorted(df_work['PK_OK'].unique()):\n            subset = df_work.query(f\"Purp5_text == '{purpose}' & PK_OK == '{peak}'\")\n\n            if len(subset) &gt; 0:\n                weighted_counts = subset.groupby(category_col)[weight_col].sum()\n                total_weight = subset[weight_col].sum()\n\n                for cat, weighted_count in weighted_counts.items():\n                    var_name = f\"{prefix}_{cat}_all\"\n                    if var_name not in results:\n                        results[var_name] = {}\n                    results[var_name][(purpose, peak)] = weighted_count / total_weight\n\n    # Calculate daily aggregates for specified purposes only\n    daily_purposes = [p for p in df_work['Purp5_text'].unique()\n                     if p not in purpose_by_period]\n\n    for purpose in daily_purposes:\n        subset = df_work.query(f\"Purp5_text == '{purpose}'\")\n\n        if len(subset) &gt; 0:\n            # For each vehicle category\n            for veh in sorted(df_work['Veh_Cat3p'].unique()):\n                veh_subset = subset.query(f\"Veh_Cat3p == '{veh}'\")\n                if len(veh_subset) &gt; 0:\n                    weighted_counts = veh_subset.groupby(category_col)[weight_col].sum()\n                    total_weight = veh_subset[weight_col].sum()\n\n                    for cat, weighted_count in weighted_counts.items():\n                        var_name = f\"{prefix}_{cat}_{veh}veh\"\n                        if var_name not in results:\n                            results[var_name] = {}\n                        results[var_name][(purpose, 'Daily')] = weighted_count / total_weight\n\n            # For \"all\" vehicle categories\n            weighted_counts = subset.groupby(category_col)[weight_col].sum()\n            total_weight = subset[weight_col].sum()\n\n            for cat, weighted_count in weighted_counts.items():\n                var_name = f\"{prefix}_{cat}_all\"\n                if var_name not in results:\n                    results[var_name] = {}\n                results[var_name][(purpose, 'Daily')] = weighted_count / total_weight\n\n    # Convert to DataFrame\n    df_result = pd.DataFrame(results).T\n    df_result.columns = pd.MultiIndex.from_tuples(df_result.columns)\n    df_result = df_result.fillna(0)\n\n    # Sort index\n    df_result = df_result.sort_index()\n\n    return df_result\n\n\n\n\nShow the code\n# Verify categorical shares sum to 1.0000\ndef verify_shares(df_shares, tolerance=0.01):\n    \"\"\"\n    Verify that shares sum to approximately 1 within each vehicle category group.\n\n    Now that we have separate rows for each vehicle ownership category (0veh, 1veh, 2veh, 3+veh, all),\n    we need to verify that shares sum to 1 within each vehicle category separately.\n    \"\"\"\n    print(\"=== Verifying Share Sums ===\\n\")\n\n    # Extract vehicle categories from index\n    # Assuming format: calib_share_{category}_{veh_category}\n    vehicle_categories = df_shares.index.str.extract(r'_(\\d\\+?veh|all)$')[0].unique()\n\n    print(f\"Vehicle categories found: {sorted(vehicle_categories)}\\n\")\n\n    all_good = True\n    issues = []\n\n    # Check each vehicle category separately\n    for veh_cat in sorted(vehicle_categories):\n        # Filter rows for this vehicle category\n        mask = df_shares.index.str.endswith(f'_{veh_cat}')\n        df_veh = df_shares[mask]\n\n        print(f\"--- Checking {veh_cat} ({len(df_veh)} categories) ---\")\n\n        # Check sums for each column within this vehicle category\n        veh_issues = []\n        for col in df_veh.columns:\n            col_sum = df_veh[col].sum()\n\n            if abs(col_sum - 1.0) &gt; tolerance:\n                all_good = False\n                veh_issues.append((col, col_sum))\n\n        if not veh_issues:\n            print(f\"  ✅ All shares sum to 1.0 for {veh_cat}\")\n        else:\n            print(f\"  ⚠️  {len(veh_issues)} columns don't sum to 1.0 for {veh_cat}:\")\n            for col, sum_val in veh_issues[:5]:  # Show first 5\n                print(f\"     {col}: {sum_val:.4f}\")\n            if len(veh_issues) &gt; 5:\n                print(f\"     ... and {len(veh_issues) - 5} more\")\n            issues.extend([(veh_cat, col, sum_val) for col, sum_val in veh_issues])\n        print()\n\n    print(f\"{'='*60}\")\n    if all_good:\n        print(\"✅ ALL VEHICLE CATEGORIES: Shares sum to 1.0 (within tolerance)\")\n    else:\n        print(f\"⚠️  WARNING: {len(issues)} total column/vehicle combinations don't sum to 1.0\")\n\n    print(f\"\\nTotal columns: {len(df_shares.columns)}\")\n    print(f\"Total vehicle categories: {len(vehicle_categories)}\")\n    print(f\"Total combinations: {len(df_shares.columns) * len(vehicle_categories)}\")\n\n    # Return sums by vehicle category\n    return {veh_cat: df_shares[df_shares.index.str.endswith(f'_{veh_cat}')].sum(axis=0)\n            for veh_cat in sorted(vehicle_categories)}\n\n\n\n\n\n\n\nShow the code\n# # Import global TDM functions from remote 'Resource' repo\n\n# Fetch and import BigQuery module\nresponse = fetch_github(\n    'https://raw.githubusercontent.com/WFRCAnalytics/Resources/refs/heads/master/2-Python/global-functions/BigQuery.py',\n    mode=\"public\"\n)\n\nBigQuery = importlib.util.module_from_spec(importlib.util.spec_from_loader('BigQuery', loader=None))\nexec(response.text, BigQuery.__dict__)\n\n# Initiatlize BigQuery Client\nclient = BigQuery.getBigQueryClient_Confidential2023UtahHTS()\n\n\npukar.bhandari\nC:/Users/Pukar.Bhandari/.private/confidential-2023-utah-hts-5fd7ddd219a7.json"
  },
  {
    "objectID": "2-mode-choice-targets.html#install-libraries",
    "href": "2-mode-choice-targets.html#install-libraries",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "",
    "text": "!conda install -c conda-forge numpy pandas matplotlib seaborn requests pathlib BigQuery"
  },
  {
    "objectID": "2-mode-choice-targets.html#import-libraries",
    "href": "2-mode-choice-targets.html#import-libraries",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "",
    "text": "Show the code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport importlib.util\nfrom pathlib import Path\nimport json\nimport sys\nimport os\nimport requests\nimport io\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\n\nTrue"
  },
  {
    "objectID": "2-mode-choice-targets.html#environment-variables",
    "href": "2-mode-choice-targets.html#environment-variables",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "",
    "text": "Show the code\n# Export mode control\nEXPORT_MODE = \"only_needed\"  # Options: \"export_all\", \"comment_unused\", \"only_needed\"\n\n# Optional suffix removal (when no vehicle disaggregation exists in entire file)\nREMOVE_ALL_SUFFIX = True\n\n\n\n\nShow the code\n# Variable-level vehicle ownership patterns\n# Format: {purpose: {period: {variable_family: [list of suffixes needed]}}}\n# Suffixes: '0veh', '1veh', '2veh', '3veh' for disaggregation, 'all' for aggregated\n\n# Load vehicle-level vehicle ownership patterns from JSON configuration file\nconfig_file = 'vehicle_pattern_config.json'\n\nwith open(config_file, 'r') as f:\n        variable_veh_patterns = json.load(f)"
  },
  {
    "objectID": "2-mode-choice-targets.html#helper-functions",
    "href": "2-mode-choice-targets.html#helper-functions",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "",
    "text": "Show the code\ndef fetch_github(\n    url: str,\n    mode: str = \"private\",\n    token_env_var: str = \"GITHUB_TOKEN\"\n) -&gt; requests.Response:\n    \"\"\"\n    Fetch content from GitHub repositories.\n\n    Args:\n        url: GitHub raw URL (e.g., https://raw.githubusercontent.com/...)\n        mode: \"public\" for public repos, \"private\" for private repos requiring authentication\n        token_env_var: Name of environment variable containing GitHub token (default: GITHUB_TOKEN)\n\n    Returns:\n        requests.Response object\n\n    Raises:\n        ValueError: If token is missing for private mode or invalid mode\n        requests.HTTPError: If request fails\n    \"\"\"\n    # Validate mode\n    if mode not in [\"public\", \"private\"]:\n        raise ValueError(f\"mode must be 'public' or 'private', got '{mode}'\")\n\n    if mode == \"public\":\n        response = requests.get(url, timeout=30)\n    else:\n        token = os.getenv(token_env_var)\n        if not token:\n            raise ValueError(\n                f\"GitHub token not found in environment variable '{token_env_var}'. \"\n                f\"Check your .env file has: {token_env_var}=your_token_here\"\n            )\n\n        headers = {\n            'Authorization': f'token {token}',\n            'Accept': 'application/vnd.github.v3.raw'\n        }\n        response = requests.get(url, headers=headers, timeout=30)\n\n    response.raise_for_status()\n    return response\n\n\n\n\nShow the code\ndef calculate_categorical_shares(\n    df,\n    category_col,\n    weight_col='trip_weight',\n    group_cols=['Veh_Cat3p', 'Purp5_text', 'PK_OK'],\n    filter_condition=None,\n    category_mapping=None,\n    prefix='share',\n    purpose_by_period=['HBW', 'HBO', 'NHB'],  # Only HBC and HBSch get daily\n    purpose_daily=['HBC', 'HBSch']  # NEW: Purposes that only get daily (no peak/off-peak)\n):\n    \"\"\"\n    Calculate weighted shares of categories within groups, ensuring they sum to 1.\n\n    Parameters:\n    -----------\n    df : DataFrame\n        Input dataframe\n    category_col : str\n        Column containing the categories to calculate shares for\n    weight_col : str\n        Column containing weights for each observation\n    group_cols : list\n        Columns to group by (e.g., vehicle ownership, purpose, peak/off-peak)\n    filter_condition : str or None\n        Optional pandas query string to filter data before calculation\n    category_mapping : dict or None\n        Optional mapping to rename/recode categories\n    prefix : str\n        Prefix for output column names\n    purpose_by_period : list\n        Purposes that should NOT get daily aggregates\n    purpose_daily : list\n        Purposes that ONLY get daily aggregates (no peak/off-peak breakdown)\n\n    Returns:\n    --------\n    DataFrame with shares in wide format\n    \"\"\"\n\n    # Apply filter and copy in one step\n    if filter_condition:\n        df_work = df.query(filter_condition).copy()\n    else:\n        df_work = df.copy()\n\n    # Apply category mapping if provided\n    if category_mapping:\n        df_work[category_col] = df_work[category_col].map(category_mapping)\n        # Remove unmapped values\n        df_work = df_work.dropna(subset=[category_col])\n\n    # Remove any NaN values in key columns\n    df_work = df_work.dropna(subset=[category_col, weight_col] + group_cols)\n\n    # Convert Veh_Cat3p to string format for consistency\n    df_work['Veh_Cat3p'] = df_work['Veh_Cat3p'].astype(str)\n    # Map 3 to 3+ for display\n    # df_work['Veh_Cat3p'] = df_work['Veh_Cat3p'].replace('3', '3+')\n\n    # Initialize results dictionary\n    results = {}\n\n    # Calculate weighted shares for each group combination (peak/off-peak)\n    # BUT skip purpose_daily\n    for veh in sorted(df_work['Veh_Cat3p'].unique()):\n        for purpose in sorted(df_work['Purp5_text'].unique()):\n            # Skip peak/off-peak for daily-only purposes\n            if purpose in purpose_daily:\n                continue\n\n            for peak in sorted(df_work['PK_OK'].unique()):\n                subset = df_work.query(\n                    f\"Veh_Cat3p == '{veh}' & Purp5_text == '{purpose}' & PK_OK == '{peak}'\"\n                )\n\n                if len(subset) &gt; 0:\n                    # Calculate weighted counts by category\n                    weighted_counts = subset.groupby(category_col)[weight_col].sum()\n                    total_weight = subset[weight_col].sum()\n\n                    for cat, weighted_count in weighted_counts.items():\n                        var_name = f\"{prefix}_{cat}_{veh}veh\"\n                        if var_name not in results:\n                            results[var_name] = {}\n                        results[var_name][(purpose, peak)] = weighted_count / total_weight\n\n    # Calculate \"_all\" aggregates for peak/off-peak (excluding daily-only purposes)\n    for purpose in sorted(df_work['Purp5_text'].unique()):\n        # Skip peak/off-peak for daily-only purposes\n        if purpose in purpose_daily:\n            continue\n\n        for peak in sorted(df_work['PK_OK'].unique()):\n            subset = df_work.query(f\"Purp5_text == '{purpose}' & PK_OK == '{peak}'\")\n\n            if len(subset) &gt; 0:\n                weighted_counts = subset.groupby(category_col)[weight_col].sum()\n                total_weight = subset[weight_col].sum()\n\n                for cat, weighted_count in weighted_counts.items():\n                    var_name = f\"{prefix}_{cat}_all\"\n                    if var_name not in results:\n                        results[var_name] = {}\n                    results[var_name][(purpose, peak)] = weighted_count / total_weight\n\n    # Calculate daily aggregates for specified purposes only\n    daily_purposes = [p for p in df_work['Purp5_text'].unique()\n                     if p not in purpose_by_period]\n\n    for purpose in daily_purposes:\n        subset = df_work.query(f\"Purp5_text == '{purpose}'\")\n\n        if len(subset) &gt; 0:\n            # For each vehicle category\n            for veh in sorted(df_work['Veh_Cat3p'].unique()):\n                veh_subset = subset.query(f\"Veh_Cat3p == '{veh}'\")\n                if len(veh_subset) &gt; 0:\n                    weighted_counts = veh_subset.groupby(category_col)[weight_col].sum()\n                    total_weight = veh_subset[weight_col].sum()\n\n                    for cat, weighted_count in weighted_counts.items():\n                        var_name = f\"{prefix}_{cat}_{veh}veh\"\n                        if var_name not in results:\n                            results[var_name] = {}\n                        results[var_name][(purpose, 'Daily')] = weighted_count / total_weight\n\n            # For \"all\" vehicle categories\n            weighted_counts = subset.groupby(category_col)[weight_col].sum()\n            total_weight = subset[weight_col].sum()\n\n            for cat, weighted_count in weighted_counts.items():\n                var_name = f\"{prefix}_{cat}_all\"\n                if var_name not in results:\n                    results[var_name] = {}\n                results[var_name][(purpose, 'Daily')] = weighted_count / total_weight\n\n    # Convert to DataFrame\n    df_result = pd.DataFrame(results).T\n    df_result.columns = pd.MultiIndex.from_tuples(df_result.columns)\n    df_result = df_result.fillna(0)\n\n    # Sort index\n    df_result = df_result.sort_index()\n\n    return df_result\n\n\n\n\nShow the code\n# Verify categorical shares sum to 1.0000\ndef verify_shares(df_shares, tolerance=0.01):\n    \"\"\"\n    Verify that shares sum to approximately 1 within each vehicle category group.\n\n    Now that we have separate rows for each vehicle ownership category (0veh, 1veh, 2veh, 3+veh, all),\n    we need to verify that shares sum to 1 within each vehicle category separately.\n    \"\"\"\n    print(\"=== Verifying Share Sums ===\\n\")\n\n    # Extract vehicle categories from index\n    # Assuming format: calib_share_{category}_{veh_category}\n    vehicle_categories = df_shares.index.str.extract(r'_(\\d\\+?veh|all)$')[0].unique()\n\n    print(f\"Vehicle categories found: {sorted(vehicle_categories)}\\n\")\n\n    all_good = True\n    issues = []\n\n    # Check each vehicle category separately\n    for veh_cat in sorted(vehicle_categories):\n        # Filter rows for this vehicle category\n        mask = df_shares.index.str.endswith(f'_{veh_cat}')\n        df_veh = df_shares[mask]\n\n        print(f\"--- Checking {veh_cat} ({len(df_veh)} categories) ---\")\n\n        # Check sums for each column within this vehicle category\n        veh_issues = []\n        for col in df_veh.columns:\n            col_sum = df_veh[col].sum()\n\n            if abs(col_sum - 1.0) &gt; tolerance:\n                all_good = False\n                veh_issues.append((col, col_sum))\n\n        if not veh_issues:\n            print(f\"  ✅ All shares sum to 1.0 for {veh_cat}\")\n        else:\n            print(f\"  ⚠️  {len(veh_issues)} columns don't sum to 1.0 for {veh_cat}:\")\n            for col, sum_val in veh_issues[:5]:  # Show first 5\n                print(f\"     {col}: {sum_val:.4f}\")\n            if len(veh_issues) &gt; 5:\n                print(f\"     ... and {len(veh_issues) - 5} more\")\n            issues.extend([(veh_cat, col, sum_val) for col, sum_val in veh_issues])\n        print()\n\n    print(f\"{'='*60}\")\n    if all_good:\n        print(\"✅ ALL VEHICLE CATEGORIES: Shares sum to 1.0 (within tolerance)\")\n    else:\n        print(f\"⚠️  WARNING: {len(issues)} total column/vehicle combinations don't sum to 1.0\")\n\n    print(f\"\\nTotal columns: {len(df_shares.columns)}\")\n    print(f\"Total vehicle categories: {len(vehicle_categories)}\")\n    print(f\"Total combinations: {len(df_shares.columns) * len(vehicle_categories)}\")\n\n    # Return sums by vehicle category\n    return {veh_cat: df_shares[df_shares.index.str.endswith(f'_{veh_cat}')].sum(axis=0)\n            for veh_cat in sorted(vehicle_categories)}"
  },
  {
    "objectID": "2-mode-choice-targets.html#setup-bigquery",
    "href": "2-mode-choice-targets.html#setup-bigquery",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "",
    "text": "Show the code\n# # Import global TDM functions from remote 'Resource' repo\n\n# Fetch and import BigQuery module\nresponse = fetch_github(\n    'https://raw.githubusercontent.com/WFRCAnalytics/Resources/refs/heads/master/2-Python/global-functions/BigQuery.py',\n    mode=\"public\"\n)\n\nBigQuery = importlib.util.module_from_spec(importlib.util.spec_from_loader('BigQuery', loader=None))\nexec(response.text, BigQuery.__dict__)\n\n# Initiatlize BigQuery Client\nclient = BigQuery.getBigQueryClient_Confidential2023UtahHTS()\n\n\npukar.bhandari\nC:/Users/Pukar.Bhandari/.private/confidential-2023-utah-hts-5fd7ddd219a7.json"
  },
  {
    "objectID": "2-mode-choice-targets.html#uta-on-board-survey",
    "href": "2-mode-choice-targets.html#uta-on-board-survey",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "2.1 UTA On-Board Survey",
    "text": "2.1 UTA On-Board Survey\n\n\nShow the code\ndf_obs_linked = pd.read_csv(\n    r\"_output/UTA_OBS_2024_Linked_FactorAdjusted_CRT.csv\"\n)\n\ndf_obs_linked\n\n\nC:\\Users\\Pukar.Bhandari\\AppData\\Local\\Temp\\ipykernel_12388\\3457507166.py:1: DtypeWarning:\n\nColumns (8,24,32,37,54,132,174,287) have mixed types. Specify dtype option on import or set low_memory=False.\n\n\n\n\n\n\n\n\n\n\nID\nDATE_COMPLETED\nDATE_TYPE\nROUTE_DIRECTION_Code\nROUTE_DIRECTION\nROUTE_DIRECTION_Other\nHOME_ADDRESS_CITY\nHOME_ADDRESS_STATE\nHOME_ADDRESS_ZIP\nHOME_ADDRESS_LAT\n...\naccess_dist\naccess_time\negress_dist\negress_time\nAdj_Factor\ntrip_weight\nuses_crt\nvalid_crt_mask\ndist_CRT\nclass_CRT\n\n\n\n\n0\n5948\n2024-02-27\nWeekday\nUTA_1_2_00\n2 200 SOUTH - TO U HOSPITAL\nNaN\nClearfield\nUT\n84015\n41.119565\n...\n0.37\n8.79\n0.68\n16.27\n1.195057\n7.988529\nFalse\nFalse\nNaN\nNaN\n\n\n1\n6067\n2024-02-27\nWeekday\nUTA_1_47_00\n47 4700 SOUTH - TO W VALLEY CTL\nNaN\nWest Valley City\nUT\n84119\n40.689590\n...\n0.56\n13.53\n0.24\n5.70\n1.407280\n4.420834\nFalse\nFalse\nNaN\nNaN\n\n\n2\n6073\n2024-02-28\nWeekday\nUTA_1_750_01\nFRONTRUNNER 750 - SOUTHBOUND\nNaN\nOgden\nUT\n84401\n41.236749\n...\n4.17\n8.22\n1.37\n3.29\n1.192511\n5.413118\nTrue\nTrue\n44.26\nCRT_04\n\n\n3\n6077\n2024-02-28\nWeekday\nUTA_1_47_00\n47 4700 SOUTH - TO W VALLEY CTL\nNaN\nWest Valley City\nUT\n84119\n40.669687\n...\n0.25\n6.00\n0.49\n11.88\n1.407280\n10.487378\nFalse\nFalse\nNaN\nNaN\n\n\n4\n6078\n2024-02-28\nWeekday\nUTA_1_220_00\n220 HIGHLAND DRIVE / 1300 EAST - TO SANDY\nNaN\nSalt Lake City\nUT\n84101\n40.765122\n...\n0.27\n6.51\n0.19\n4.55\n1.407280\n3.287619\nFalse\nFalse\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n12015\n30339\n2024-04-26\nWeekday\nUTA_1_704_01\nTRAX GREEN LINE 704 - TO AIRPORT\nNaN\nSalt Lake City\nUT\n84116\n40.772420\n...\n0.31\n7.53\n0.17\n4.07\n1.405314\n8.528545\nFalse\nFalse\nNaN\nNaN\n\n\n12016\n30340\n2024-04-26\nWeekday\nUTA_1_704_00\nTRAX GREEN LINE 704 - TO WEST VALLEY\nNaN\nSalt Lake City\nUT\n84116\n40.771361\n...\n0.46\n11.00\n1.21\n29.10\n1.405314\n16.406476\nFalse\nFalse\nNaN\nNaN\n\n\n12017\n30341\n2024-04-26\nWeekday\nUTA_1_704_00\nTRAX GREEN LINE 704 - TO WEST VALLEY\nNaN\nSalt Lake City\nUT\n84116\n40.773232\n...\n0.17\n4.07\n0.86\n20.68\n1.405314\n8.203238\nFalse\nFalse\nNaN\nNaN\n\n\n12018\n1000002\n2024-02-29\nWeekday\nUTA_1_701_00\nTRAX BLUE LINE 701 - TO DRAPER\nNaN\nSalt Lake City\nUT\n84111\n40.751607\n...\n0.33\n7.95\n0.29\n7.02\n0.893041\n4.139760\nFalse\nFalse\nNaN\nNaN\n\n\n12019\n1000004\n2024-02-29\nWeekday\nUTA_1_701_00\nTRAX BLUE LINE 701 - TO DRAPER\nNaN\nSalt Lake City\nUT\n84109\n40.722068\n...\n0.79\n18.85\n0.19\n4.44\n1.405314\n4.676107\nFalse\nFalse\nNaN\nNaN\n\n\n\n\n12020 rows × 313 columns"
  },
  {
    "objectID": "2-mode-choice-targets.html#household-travel-survey-linked-trips",
    "href": "2-mode-choice-targets.html#household-travel-survey-linked-trips",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "2.2 Household Travel Survey: Linked Trips",
    "text": "2.2 Household Travel Survey: Linked Trips\n\n\nShow the code\n# Load Linked Trips data from 2023 Household Travel Survey\ndf_hts_linked = client.query(\n    \"SELECT hh_id, PURP7_t, depart_per, model_trip_mode_WFv10, trip_weight FROM \" + 'wfrc-modeling-data.prd_tdm_hts_2023.trip_linked'\n).to_dataframe()\n\ndf_hts_linked\n\n\n\n\n\n\n\n\n\nhh_id\nPURP7_t\ndepart_per\nmodel_trip_mode_WFv10\ntrip_weight\n\n\n\n\n0\n23149927\nHBC\nAM\nbike\n0.000000\n\n\n1\n23005950\nHBOth\nAM\nauto-sov\n0.000000\n\n\n2\n23008391\nHBOth\nAM\nauto-occ3p\n0.000000\n\n\n3\n23008686\nHBOth\nAM\nauto-occ2\n115.118280\n\n\n4\n23011482\nHBOth\nAM\nauto-sov\n76.022360\n\n\n...\n...\n...\n...\n...\n...\n\n\n232446\n23451990\nNHBW\nPM\nauto-sov\n0.000000\n\n\n232447\n23458329\nNHBW\nPM\nauto-occ2\n0.000000\n\n\n232448\n23489923\nNHBW\nPM\nauto-sov\n17.343985\n\n\n232449\n23493042\nNHBW\nPM\nauto-occ2\n59.423909\n\n\n232450\n23551046\nNHBW\nPM\nauto-sov\n36.585607\n\n\n\n\n232451 rows × 5 columns"
  },
  {
    "objectID": "2-mode-choice-targets.html#household-travel-survey-households",
    "href": "2-mode-choice-targets.html#household-travel-survey-households",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "2.3 Household Travel Survey: Households",
    "text": "2.3 Household Travel Survey: Households\n\n\nShow the code\n# Load Household data from 2023 Household Travel Survey\ndf_hts_hh = client.query(\n    \"SELECT hh_id, num_vehicles_4cat FROM \" + 'wfrc-modeling-data.prd_tdm_hts_2023.hh'\n).to_dataframe()\n\ndf_hts_hh\n\n\n\n\n\n\n\n\n\nhh_id\nnum_vehicles_4cat\n\n\n\n\n0\n23472690\n3\n\n\n1\n23307416\n3\n\n\n2\n23317145\n2\n\n\n3\n23321176\n2\n\n\n4\n23345703\n2\n\n\n...\n...\n...\n\n\n11178\n23364646\n2\n\n\n11179\n23046966\n1\n\n\n11180\n23043011\n1\n\n\n11181\n23085912\n3\n\n\n11182\n23376736\n1\n\n\n\n\n11183 rows × 2 columns"
  },
  {
    "objectID": "2-mode-choice-targets.html#check-crt-categories",
    "href": "2-mode-choice-targets.html#check-crt-categories",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "4.1 Check CRT Categories",
    "text": "4.1 Check CRT Categories\n\n\nShow the code\ndf_obs_linked[df_obs_linked['Linked_Mode_txt'] == 'CRT']['class_CRT'].value_counts()\n\n\nclass_CRT\nCRT_03    799\nCRT_02    489\nCRT_04    369\nCRT_01    309\nCRT_00    168\nCRT_05     63\nCRT_06     47\nCRT_07     25\nCRT_08      8\nName: count, dtype: int64\n\n\n\n\nShow the code\n# Get CRT subclasses from class_CRT column\ncrt_categories = sorted(\n    df_obs_linked[df_obs_linked['Linked_Mode_txt'] == 'CRT']['class_CRT']\n    .dropna()\n    .unique()\n)\n\ncrt_categories\n\n\n['CRT_00',\n 'CRT_01',\n 'CRT_02',\n 'CRT_03',\n 'CRT_04',\n 'CRT_05',\n 'CRT_06',\n 'CRT_07',\n 'CRT_08']"
  },
  {
    "objectID": "2-mode-choice-targets.html#compute-crt-class-proportions-by-purpose",
    "href": "2-mode-choice-targets.html#compute-crt-class-proportions-by-purpose",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "4.2 Compute CRT Class Proportions by Purpose",
    "text": "4.2 Compute CRT Class Proportions by Purpose\n\n\nShow the code\n# Compute CRT Subclass Proportions by Purpose Only\n# This will be used to expand single CRT category into detailed CRT classes\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"COMPUTING CRT SUBCLASS PROPORTIONS BY PURPOSE\")\nprint(\"=\" * 80)\n\n# Filter to CRT trips only\ndf_crt_only = df_obs_linked[\n    (df_obs_linked['Linked_Mode_txt'] == 'CRT') &\n    (df_obs_linked['class_CRT'].notna())\n].copy()\n\n# Compute P(class_CRT_i | CRT, Purpose)\ncrt_purpose_props = {}\n\nfor purpose in sorted(df_crt_only['Purp5_text'].unique()):\n    purpose_subset = df_crt_only[df_crt_only['Purp5_text'] == purpose]\n\n    if len(purpose_subset) &gt; 0:\n        # Calculate weighted shares by class_CRT\n        weighted_counts = purpose_subset.groupby('class_CRT')['trip_weight'].sum()\n        total_weight = purpose_subset['trip_weight'].sum()\n\n        purpose_props = {}\n        for crt_class, weight in weighted_counts.items():\n            purpose_props[crt_class] = weight / total_weight\n\n        crt_purpose_props[purpose] = purpose_props\n\n        # # Display results\n        # print(f\"\\n{purpose}:\")\n        # for crt_class in sorted(purpose_props.keys()):\n        #     print(f\"  P({crt_class} | CRT, {purpose}) = {purpose_props[crt_class]:.4f}\")\n\n# # Verify proportions sum to 1.0 for each purpose\n# print(\"\\n\" + \"-\" * 60)\n# print(\"Verification - sums by purpose:\")\n# for purpose, props in crt_purpose_props.items():\n#     total = sum(props.values())\n#     status = \"✅\" if abs(total - 1.0) &lt; 0.01 else \"⚠️\"\n#     print(f\"  {purpose}: {total:.4f} {status}\")\n\n\n\n================================================================================\nCOMPUTING CRT SUBCLASS PROPORTIONS BY PURPOSE\n================================================================================"
  },
  {
    "objectID": "2-mode-choice-targets.html#purpose-5-category",
    "href": "2-mode-choice-targets.html#purpose-5-category",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "5.1 Purpose (5-category)",
    "text": "5.1 Purpose (5-category)\n\n\nShow the code\npurpose_mapping = {\n    'HBOth' : 'HBO',\n    'NHBNW' : 'NHB',\n    'HBW'   : 'HBW',\n    'NHBW'  : 'NHB',\n    'HBShp' : 'HBO',\n    'HBSch' : 'HBSch',\n    'HBC'   : 'HBC'\n}\n\ndf_hts_linked['Purp5_text'] = df_hts_linked['PURP7_t'].map(purpose_mapping)"
  },
  {
    "objectID": "2-mode-choice-targets.html#vehicle-ownership",
    "href": "2-mode-choice-targets.html#vehicle-ownership",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "5.2 Vehicle Ownership",
    "text": "5.2 Vehicle Ownership\n\n\nShow the code\n# Join household data to linked trips data\ndf_hts_linked = df_hts_linked.merge(\n    df_hts_hh.rename(columns={'num_vehicles_4cat': 'Veh_Cat3p'}),\n    on='hh_id',\n    how='left'  # Keep all trips, even if household data is missing\n)\n\n# Rename the vehicle ownership column to match the OBS data\ndf_hts_linked = df_hts_linked.rename(columns={'num_vehicles_4cat': 'Veh_Cat3p'})"
  },
  {
    "objectID": "2-mode-choice-targets.html#peak---offpeak",
    "href": "2-mode-choice-targets.html#peak---offpeak",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "5.3 Peak - Offpeak",
    "text": "5.3 Peak - Offpeak\n\n\nShow the code\ndf_hts_linked['PK_OK'] = np.where(\n    df_hts_linked[\"depart_per\"].isin([\"AM\", \"PM\"]),\n    \"PK\",\n    \"OK\"\n)"
  },
  {
    "objectID": "2-mode-choice-targets.html#motorized-vs-nonmotorized",
    "href": "2-mode-choice-targets.html#motorized-vs-nonmotorized",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "6.1 Motorized vs Nonmotorized",
    "text": "6.1 Motorized vs Nonmotorized\n\n\nShow the code\n(\n    df_hts_linked[\n        (df_hts_linked['model_trip_mode_WFv10'].isin(\n            [\"auto-sov\", \"auto-occ2\", \"auto-occ3p\", \"walk-to-transit\",\n             \"drive-to-transit\", \"walk\", \"bike\"]\n        )) &\n        (df_hts_linked['Purp5_text'] == 'HBW')\n    ]\n    .assign(motorized_category=lambda x: x['model_trip_mode_WFv10'].map({\n        'auto-sov'         : 'motor',\n        'auto-occ2'        : 'motor',\n        'auto-occ3p'       : 'motor',\n        'walk-to-transit'  : 'motor',\n        'drive-to-transit' : 'motor',\n        'walk'             : 'nonmotor',\n        'bike'             : 'nonmotor'\n    }))\n    .groupby('motorized_category')['trip_weight']\n    .sum()\n    .pipe(lambda x: (x / x.sum() * 100).round(2))\n)\n\n\nmotorized_category\nmotor       97.03\nnonmotor     2.97\nName: trip_weight, dtype: float64"
  },
  {
    "objectID": "2-mode-choice-targets.html#hts-vs-obs-transit-trips",
    "href": "2-mode-choice-targets.html#hts-vs-obs-transit-trips",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "6.2 HTS vs OBS Transit Trips",
    "text": "6.2 HTS vs OBS Transit Trips\n\nFilter for Transit Trips\n\n\nShow the code\n# Identify transit trips in HTS\nhts_transit = df_hts_linked[\n    df_hts_linked['model_trip_mode_WFv10'].isin(['walk-to-transit', 'drive-to-transit'])\n].copy()\n\n# Filter out microtransit\nobs_transit = df_obs_linked[df_obs_linked['Linked_Mode_txt'] != 'MT'].copy()\n\n\n\n\nBy access mode\n\n\nShow the code\nhts_by_access = hts_transit.groupby('model_trip_mode_WFv10')['trip_weight'].sum()\nhts_by_access\n\n\nmodel_trip_mode_WFv10\ndrive-to-transit     34844.620753\nwalk-to-transit     106748.298633\nName: trip_weight, dtype: float64\n\n\n\n\nShow the code\nif 'Ac_Mode2_Model' in obs_transit.columns:\n    obs_by_access = obs_transit.groupby('Ac_Mode2_Model')['trip_weight'].sum().sort_index()\n\nobs_by_access\n\n\nAc_Mode2_Model\nDrive    18682.851823\nWalk     77989.858335\nName: trip_weight, dtype: float64\n\n\n\n\nBy purpose\n\n\nShow the code\nhts_by_purpose = hts_transit.groupby('Purp5_text')['trip_weight'].sum().sort_index()\nhts_by_purpose\n\n\nPurp5_text\nHBC       9625.364706\nHBO      57055.318544\nHBSch     4709.043372\nHBW      41885.316622\nNHB      28317.876141\nName: trip_weight, dtype: float64\n\n\n\n\nShow the code\nobs_by_purpose = obs_transit.groupby('Purp5_text')['trip_weight'].sum().sort_index()\nobs_by_purpose\n\n\nPurp5_text\nHBC      18108.487940\nHBO      30554.732806\nHBSch     2668.993423\nHBW      37103.804529\nNHB       8236.691461\nName: trip_weight, dtype: float64\n\n\n\n\nBy vehicle ownership\n\n\nShow the code\nhts_by_veh = hts_transit.groupby('Veh_Cat3p')['trip_weight'].sum().sort_index()\nhts_by_veh\n\n\nVeh_Cat3p\n0    41442.964546\n1    39084.528933\n2    37437.608191\n3    23627.817715\nName: trip_weight, dtype: float64\n\n\n\n\nShow the code\nobs_by_veh = obs_transit.groupby('Veh_Cat3p')['trip_weight'].sum().sort_index()\nobs_by_veh\n\n\nVeh_Cat3p\n0    28972.297544\n1    27888.807780\n2    23944.107756\n3    15867.497078\nName: trip_weight, dtype: float64\n\n\n\n\nBy period\n\n\nShow the code\nhts_by_period = hts_transit.groupby('PK_OK')['trip_weight'].sum().sort_index()\nhts_by_period\n\n\nPK_OK\nOK    75439.954455\nPK    66152.964930\nName: trip_weight, dtype: float64\n\n\n\n\nShow the code\nobs_by_period = obs_transit.groupby('PK_OK')['trip_weight'].sum().sort_index()\nobs_by_period\n\n\nPK_OK\nOK    56043.250270\nPK    40629.459888\nName: trip_weight, dtype: float64\n\n\n\n\nSummary\n\n\nShow the code\nprint(\"\\n\" + \"=\" * 80)\nprint(\"3. COMPARISON SUMMARY\")\nprint(\"=\" * 80)\n\nhts_total = hts_transit['trip_weight'].sum()\nobs_total = obs_transit['trip_weight'].sum()\nratio = obs_total / hts_total if hts_total &gt; 0 else 0\n\nprint(f\"\\nHTS Daily Transit Trips:  {hts_total:&gt;15,.1f}\")\nprint(f\"OBS Daily Transit Trips:  {obs_total:&gt;15,.1f}\")\nprint(f\"{'─' * 45}\")\nprint(f\"Ratio (OBS/HTS):          {ratio:&gt;15.2f}x\")\nprint(f\"Difference:               {obs_total - hts_total:&gt;15,.1f}\")\nprint(f\"% Difference:             {((obs_total/hts_total - 1) * 100):&gt;15.1f}%\")\n\n\n\n================================================================================\n3. COMPARISON SUMMARY\n================================================================================\n\nHTS Daily Transit Trips:        141,592.9\nOBS Daily Transit Trips:         96,672.7\n─────────────────────────────────────────────\nRatio (OBS/HTS):                     0.68x\nDifference:                     -44,920.2\n% Difference:                       -31.7%\n\n\n\n\nShow the code\nprint(\"\\n\" + \"=\" * 80)\nprint(\"4. DETAILED COMPARISON BY PURPOSE & PERIOD\")\nprint(\"=\" * 80)\n\n# Create comparison dataframe\ncomparison_data = []\n\nfor purpose in sorted(df_hts_linked['Purp5_text'].dropna().unique()):\n    for period in ['PK', 'OK']:\n        hts_val = hts_transit[\n            (hts_transit['Purp5_text'] == purpose) &\n            (hts_transit['PK_OK'] == period)\n        ]['trip_weight'].sum()\n\n        obs_val = obs_transit[\n            (obs_transit['Purp5_text'] == purpose) &\n            (obs_transit['PK_OK'] == period)\n        ]['trip_weight'].sum()\n\n        comparison_data.append({\n            'Purpose': purpose,\n            'Period': period,\n            'HTS_Trips': hts_val,\n            'OBS_Trips': obs_val,\n            'Ratio_OBS_HTS': obs_val / hts_val if hts_val &gt; 0 else np.nan,\n            'Difference': obs_val - hts_val\n        })\n\ndf_comparison = pd.DataFrame(comparison_data)\ndf_comparison = df_comparison.sort_values(['Purpose', 'Period'])\n\nprint(\"\\n\")\nprint(df_comparison.to_string(index=False, float_format=lambda x: f'{x:,.2f}'))\n\n\n\n================================================================================\n4. DETAILED COMPARISON BY PURPOSE & PERIOD\n================================================================================\n\n\nPurpose Period  HTS_Trips  OBS_Trips  Ratio_OBS_HTS  Difference\n    HBC     OK   5,617.11  10,732.19           1.91    5,115.08\n    HBC     PK   4,008.25   7,376.29           1.84    3,368.04\n    HBO     OK  33,731.77  20,734.40           0.61  -12,997.37\n    HBO     PK  23,323.55   9,820.33           0.42  -13,503.22\n  HBSch     OK   1,472.00   1,278.16           0.87     -193.84\n  HBSch     PK   3,237.04   1,390.83           0.43   -1,846.21\n    HBW     OK  16,938.12  17,569.58           1.04      631.46\n    HBW     PK  24,947.20  19,534.23           0.78   -5,412.97\n    NHB     OK  17,680.95   5,728.92           0.32  -11,952.04\n    NHB     PK  10,636.92   2,507.77           0.24   -8,129.15"
  },
  {
    "objectID": "2-mode-choice-targets.html#create-hybrid-dataframes-hts-autonon-motorized-obs-transit",
    "href": "2-mode-choice-targets.html#create-hybrid-dataframes-hts-autonon-motorized-obs-transit",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "6.3 Create Hybrid Dataframes (HTS Auto/Non-motorized + OBS Transit)",
    "text": "6.3 Create Hybrid Dataframes (HTS Auto/Non-motorized + OBS Transit)\n\n\nShow the code\n# Non-motorized trips from HTS\ndf_hts_nonmotor = (\n    df_hts_linked[df_hts_linked['model_trip_mode_WFv10'].isin(['walk', 'bike'])]\n    [['Veh_Cat3p', 'Purp5_text', 'PK_OK', 'model_trip_mode_WFv10', 'trip_weight']]\n    .rename(columns={'trip_weight': 'weight', 'model_trip_mode_WFv10': 'mode'})\n)\n\ndf_hts_nonmotor\n\n\n\n\n\n\n\n\n\nVeh_Cat3p\nPurp5_text\nPK_OK\nmode\nweight\n\n\n\n\n0\n2\nHBC\nPK\nbike\n0.000000\n\n\n14\n2\nHBO\nPK\nwalk\n0.000000\n\n\n17\n2\nHBO\nPK\nwalk\n0.000000\n\n\n28\n0\nHBO\nPK\nwalk\n0.000000\n\n\n55\n3\nHBO\nPK\nwalk\n301.250493\n\n\n...\n...\n...\n...\n...\n...\n\n\n232391\n3\nNHB\nPK\nwalk\n0.000000\n\n\n232397\n2\nNHB\nPK\nwalk\n69.250015\n\n\n232412\n2\nNHB\nPK\nwalk\n0.000000\n\n\n232422\n3\nNHB\nPK\nwalk\n2.215895\n\n\n232436\n2\nNHB\nPK\nwalk\n86.649383\n\n\n\n\n23112 rows × 5 columns\n\n\n\n\n\nShow the code\n# Auto trips from HTS\ndf_hts_auto = (\n    df_hts_linked[df_hts_linked['model_trip_mode_WFv10'].isin(['auto-sov', 'auto-occ2', 'auto-occ3p'])]\n    [['Veh_Cat3p', 'Purp5_text', 'PK_OK', 'model_trip_mode_WFv10', 'trip_weight']]\n    .rename(columns={'trip_weight': 'weight', 'model_trip_mode_WFv10': 'mode'})\n)\n\ndf_hts_auto\n\n\n\n\n\n\n\n\n\nVeh_Cat3p\nPurp5_text\nPK_OK\nmode\nweight\n\n\n\n\n1\n2\nHBO\nPK\nauto-sov\n0.000000\n\n\n2\n3\nHBO\nPK\nauto-occ3p\n0.000000\n\n\n3\n3\nHBO\nPK\nauto-occ2\n115.118280\n\n\n4\n2\nHBO\nPK\nauto-sov\n76.022360\n\n\n5\n2\nHBO\nPK\nauto-occ2\n0.000000\n\n\n...\n...\n...\n...\n...\n...\n\n\n232446\n1\nNHB\nPK\nauto-sov\n0.000000\n\n\n232447\n2\nNHB\nPK\nauto-occ2\n0.000000\n\n\n232448\n1\nNHB\nPK\nauto-sov\n17.343985\n\n\n232449\n2\nNHB\nPK\nauto-occ2\n59.423909\n\n\n232450\n1\nNHB\nPK\nauto-sov\n36.585607\n\n\n\n\n204676 rows × 5 columns\n\n\n\n\n\nShow the code\n# Transit trips from OBS\ndf_obs_transit = (\n    df_obs_linked[df_obs_linked['Linked_Mode_txt'] != 'MT']\n    [['Veh_Cat3p', 'Purp5_text', 'PK_OK', 'Ac_Mode2_Model', 'trip_weight']]\n    .assign(mode=lambda x: x['Ac_Mode2_Model'].map({'Walk': 'walk-to-transit', 'Drive': 'drive-to-transit'}))\n    .rename(columns={'trip_weight': 'weight'})\n    [['Veh_Cat3p', 'Purp5_text', 'PK_OK', 'mode', 'weight']]\n)\n\ndf_obs_transit\n\n\n\n\n\n\n\n\n\nVeh_Cat3p\nPurp5_text\nPK_OK\nmode\nweight\n\n\n\n\n0\n0\nHBW\nPK\nwalk-to-transit\n7.988529\n\n\n1\n0\nHBC\nOK\nwalk-to-transit\n4.420834\n\n\n2\n2\nHBW\nOK\ndrive-to-transit\n5.413118\n\n\n3\n0\nHBW\nOK\nwalk-to-transit\n10.487378\n\n\n4\n1\nHBW\nOK\nwalk-to-transit\n3.287619\n\n\n...\n...\n...\n...\n...\n...\n\n\n12015\n1\nHBO\nOK\nwalk-to-transit\n8.528545\n\n\n12016\n0\nNHB\nOK\nwalk-to-transit\n16.406476\n\n\n12017\n1\nNHB\nOK\nwalk-to-transit\n8.203238\n\n\n12018\n3\nHBC\nOK\nwalk-to-transit\n4.139760\n\n\n12019\n1\nHBW\nOK\nwalk-to-transit\n4.676107\n\n\n\n\n11957 rows × 5 columns"
  },
  {
    "objectID": "2-mode-choice-targets.html#non-motorized-trips-walk-vs-bike",
    "href": "2-mode-choice-targets.html#non-motorized-trips-walk-vs-bike",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "7.1 Non-motorized Trips: Walk vs Bike",
    "text": "7.1 Non-motorized Trips: Walk vs Bike\n\n\nShow the code\n# 2. NON-MOTORIZED TRIPS\nprint(\"\\n\" + \"=\" * 60)\nprint(\"2. NON-MOTORIZED TRIPS\")\nprint(\"=\" * 60)\n\n# non_motorized_mapping = {\n#     'walk' : 'walk',\n#     'bike' : 'bike'\n# }\n\ndf_nonmotorized_cat = calculate_categorical_shares(\n    df_hts_linked,\n    category_col='model_trip_mode_WFv10',\n    weight_col='trip_weight',\n    group_cols=['Veh_Cat3p', 'Purp5_text', 'PK_OK'],\n    filter_condition='model_trip_mode_WFv10 in [\"walk\", \"bike\"]',\n    category_mapping=None,\n    prefix='calib_share'\n)\n\nprint(\"\\nShape:\", df_nonmotorized_cat.shape)\nprint(\"\\nFirst few rows:\")\nprint(df_nonmotorized_cat.head(10))\nprint(\"\\nVerification:\")\nverify_shares(df_nonmotorized_cat)\n\n\n\n============================================================\n2. NON-MOTORIZED TRIPS\n============================================================\n\nShape: (10, 8)\n\nFirst few rows:\n                            HBO                 HBW                 NHB  \\\n                             OK        PK        OK        PK        OK   \ncalib_share_bike_0veh  0.093370  0.081922  0.126895  0.184947  0.096749   \ncalib_share_bike_1veh  0.141585  0.107295  0.225268  0.300143  0.100676   \ncalib_share_bike_2veh  0.104966  0.139940  0.338888  0.382897  0.076727   \ncalib_share_bike_3veh  0.131393  0.033276  0.110195  0.564415  0.056874   \ncalib_share_bike_all   0.117384  0.107575  0.215958  0.377379  0.076507   \ncalib_share_walk_0veh  0.906630  0.918078  0.873105  0.815053  0.903251   \ncalib_share_walk_1veh  0.858415  0.892705  0.774732  0.699857  0.899324   \ncalib_share_walk_2veh  0.895034  0.860060  0.661112  0.617103  0.923273   \ncalib_share_walk_3veh  0.868607  0.966724  0.889805  0.435585  0.943126   \ncalib_share_walk_all   0.882616  0.892425  0.784042  0.622621  0.923493   \n\n                                      HBC     HBSch  \n                             PK     Daily     Daily  \ncalib_share_bike_0veh  0.168994  0.007745  0.023156  \ncalib_share_bike_1veh  0.110790  0.167210  0.135214  \ncalib_share_bike_2veh  0.074370  0.101549  0.107164  \ncalib_share_bike_3veh  0.076972  0.000000  0.103986  \ncalib_share_bike_all   0.088079  0.063449  0.107922  \ncalib_share_walk_0veh  0.831006  0.992255  0.976844  \ncalib_share_walk_1veh  0.889210  0.832790  0.864786  \ncalib_share_walk_2veh  0.925630  0.898451  0.892836  \ncalib_share_walk_3veh  0.923028  1.000000  0.896014  \ncalib_share_walk_all   0.911921  0.936551  0.892078  \n\nVerification:\n=== Verifying Share Sums ===\n\nVehicle categories found: ['0veh', '1veh', '2veh', '3veh', 'all']\n\n--- Checking 0veh (2 categories) ---\n  ✅ All shares sum to 1.0 for 0veh\n\n--- Checking 1veh (2 categories) ---\n  ✅ All shares sum to 1.0 for 1veh\n\n--- Checking 2veh (2 categories) ---\n  ✅ All shares sum to 1.0 for 2veh\n\n--- Checking 3veh (2 categories) ---\n  ✅ All shares sum to 1.0 for 3veh\n\n--- Checking all (2 categories) ---\n  ✅ All shares sum to 1.0 for all\n\n============================================================\n✅ ALL VEHICLE CATEGORIES: Shares sum to 1.0 (within tolerance)\n\nTotal columns: 8\nTotal vehicle categories: 5\nTotal combinations: 40\n\n\n{'0veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n '1veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n '2veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n '3veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n 'all': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64}"
  },
  {
    "objectID": "2-mode-choice-targets.html#motorized-trips-auto-vs-transit",
    "href": "2-mode-choice-targets.html#motorized-trips-auto-vs-transit",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "7.2 Motorized Trips: Auto vs Transit",
    "text": "7.2 Motorized Trips: Auto vs Transit\n\n\nShow the code\n# 3. Motorized Trips: Auto vs Transit\nprint(\"\\n\" + \"=\" * 60)\nprint(\"3. MOTORIZED TRIPS: AUTO VS TRANSIT\")\nprint(\"=\" * 60)\n\ndf_motorized_hybrid = pd.concat([\n    df_hts_auto.assign(mode_category='auto'),\n    df_obs_transit.assign(mode_category='transit')\n], ignore_index=True)\n\ndf_motorized_cat = calculate_categorical_shares(\n    df_motorized_hybrid,\n    category_col='mode_category',\n    weight_col='weight',\n    group_cols=['Veh_Cat3p', 'Purp5_text', 'PK_OK'],\n    filter_condition=None,\n    category_mapping=None,\n    prefix='calib_share'\n)\n\nprint(\"\\nShape:\", df_motorized_cat.shape)\nprint(\"\\nFirst few rows:\")\nprint(df_motorized_cat.head(10))\nprint(\"\\nVerification:\")\nverify_shares(df_motorized_cat)\n\n\n\n============================================================\n3. MOTORIZED TRIPS: AUTO VS TRANSIT\n============================================================\n\nShape: (10, 8)\n\nFirst few rows:\n                               HBO                 HBW                 NHB  \\\n                                OK        PK        OK        PK        OK   \ncalib_share_auto_0veh     0.726251  0.853462  0.729859  0.484913  0.849794   \ncalib_share_auto_1veh     0.992345  0.993989  0.961769  0.962011  0.996283   \ncalib_share_auto_2veh     0.997628  0.998085  0.988071  0.989968  0.998815   \ncalib_share_auto_3veh     0.998221  0.998945  0.993451  0.995569  0.999087   \ncalib_share_auto_all      0.995092  0.996495  0.981600  0.985147  0.997392   \ncalib_share_transit_0veh  0.273749  0.146538  0.270141  0.515087  0.150206   \ncalib_share_transit_1veh  0.007655  0.006011  0.038231  0.037989  0.003717   \ncalib_share_transit_2veh  0.002372  0.001915  0.011929  0.010032  0.001185   \ncalib_share_transit_3veh  0.001779  0.001055  0.006549  0.004431  0.000913   \ncalib_share_transit_all   0.004908  0.003505  0.018400  0.014853  0.002608   \n\n                                       HBSch       HBC  \n                                PK     Daily     Daily  \ncalib_share_auto_0veh     0.820820  0.332160  0.312142  \ncalib_share_auto_1veh     0.997339  0.982720  0.761914  \ncalib_share_auto_2veh     0.999222  0.996159  0.852769  \ncalib_share_auto_3veh     0.999349  0.998333  0.867500  \ncalib_share_auto_all      0.998169  0.995509  0.809373  \ncalib_share_transit_0veh  0.179180  0.667840  0.687858  \ncalib_share_transit_1veh  0.002661  0.017280  0.238086  \ncalib_share_transit_2veh  0.000778  0.003841  0.147231  \ncalib_share_transit_3veh  0.000651  0.001667  0.132500  \ncalib_share_transit_all   0.001831  0.004491  0.190627  \n\nVerification:\n=== Verifying Share Sums ===\n\nVehicle categories found: ['0veh', '1veh', '2veh', '3veh', 'all']\n\n--- Checking 0veh (2 categories) ---\n  ✅ All shares sum to 1.0 for 0veh\n\n--- Checking 1veh (2 categories) ---\n  ✅ All shares sum to 1.0 for 1veh\n\n--- Checking 2veh (2 categories) ---\n  ✅ All shares sum to 1.0 for 2veh\n\n--- Checking 3veh (2 categories) ---\n  ✅ All shares sum to 1.0 for 3veh\n\n--- Checking all (2 categories) ---\n  ✅ All shares sum to 1.0 for all\n\n============================================================\n✅ ALL VEHICLE CATEGORIES: Shares sum to 1.0 (within tolerance)\n\nTotal columns: 8\nTotal vehicle categories: 5\nTotal combinations: 40\n\n\n{'0veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBSch  Daily    1.0\n HBC    Daily    1.0\n dtype: float64,\n '1veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBSch  Daily    1.0\n HBC    Daily    1.0\n dtype: float64,\n '2veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBSch  Daily    1.0\n HBC    Daily    1.0\n dtype: float64,\n '3veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBSch  Daily    1.0\n HBC    Daily    1.0\n dtype: float64,\n 'all': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBSch  Daily    1.0\n HBC    Daily    1.0\n dtype: float64}\n\n\n\nAuto Trips: Drive Alone vs Shared Rides\n\n\nShow the code\n# 4. Auto Trips: Drive Alone vs Shared Rides\nprint(\"\\n\" + \"=\" * 60)\nprint(\"4. AUTO TRIPS: DRIVE ALONE VS SHARED RIDES\")\nprint(\"=\" * 60)\n\nauto_ride_mapping = {\n    'auto-sov'   : 'alone',\n    'auto-occ2'  : 'shared',\n    'auto-occ3p' : 'shared'\n}\n\ndf_auto_riders = calculate_categorical_shares(\n    df_hts_linked,\n    category_col='model_trip_mode_WFv10',\n    weight_col='trip_weight',\n    group_cols=['Veh_Cat3p', 'Purp5_text', 'PK_OK'],\n    filter_condition='model_trip_mode_WFv10 in [\"auto-sov\", \"auto-occ2\", \"auto-occ3p\"]',\n    category_mapping=auto_ride_mapping,\n    prefix='calib_share'\n)\n\nprint(\"\\nShape:\", df_auto_riders.shape)\nprint(\"\\nFirst few rows:\")\nprint(df_auto_riders.head(10))\nprint(\"\\nVerification:\")\nverify_shares(df_auto_riders)\n\n\n\n============================================================\n4. AUTO TRIPS: DRIVE ALONE VS SHARED RIDES\n============================================================\n\nShape: (10, 8)\n\nFirst few rows:\n                              HBO                 HBW                 NHB  \\\n                               OK        PK        OK        PK        OK   \ncalib_share_alone_0veh   0.231886  0.134755  0.251145  0.206815  0.214780   \ncalib_share_alone_1veh   0.450430  0.422546  0.743629  0.767954  0.464611   \ncalib_share_alone_2veh   0.343353  0.306012  0.849545  0.875657  0.393618   \ncalib_share_alone_3veh   0.409285  0.362711  0.866645  0.895452  0.471189   \ncalib_share_alone_all    0.384813  0.343298  0.833347  0.868592  0.434965   \ncalib_share_shared_0veh  0.768114  0.865245  0.748855  0.793185  0.785220   \ncalib_share_shared_1veh  0.549570  0.577454  0.256371  0.232046  0.535389   \ncalib_share_shared_2veh  0.656647  0.693988  0.150455  0.124343  0.606382   \ncalib_share_shared_3veh  0.590715  0.637289  0.133355  0.104548  0.528811   \ncalib_share_shared_all   0.615187  0.656702  0.166653  0.131408  0.565035   \n\n                                        HBC     HBSch  \n                               PK     Daily     Daily  \ncalib_share_alone_0veh   0.361057  0.365629  0.000000  \ncalib_share_alone_1veh   0.516010  0.476547  0.041826  \ncalib_share_alone_2veh   0.411229  0.825620  0.030260  \ncalib_share_alone_3veh   0.471706  0.771353  0.190596  \ncalib_share_alone_all    0.452998  0.718980  0.108671  \ncalib_share_shared_0veh  0.638943  0.634371  1.000000  \ncalib_share_shared_1veh  0.483990  0.523453  0.958174  \ncalib_share_shared_2veh  0.588771  0.174380  0.969740  \ncalib_share_shared_3veh  0.528294  0.228647  0.809404  \ncalib_share_shared_all   0.547002  0.281020  0.891329  \n\nVerification:\n=== Verifying Share Sums ===\n\nVehicle categories found: ['0veh', '1veh', '2veh', '3veh', 'all']\n\n--- Checking 0veh (2 categories) ---\n  ✅ All shares sum to 1.0 for 0veh\n\n--- Checking 1veh (2 categories) ---\n  ✅ All shares sum to 1.0 for 1veh\n\n--- Checking 2veh (2 categories) ---\n  ✅ All shares sum to 1.0 for 2veh\n\n--- Checking 3veh (2 categories) ---\n  ✅ All shares sum to 1.0 for 3veh\n\n--- Checking all (2 categories) ---\n  ✅ All shares sum to 1.0 for all\n\n============================================================\n✅ ALL VEHICLE CATEGORIES: Shares sum to 1.0 (within tolerance)\n\nTotal columns: 8\nTotal vehicle categories: 5\nTotal combinations: 40\n\n\n{'0veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n '1veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n '2veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n '3veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n 'all': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64}\n\n\n\nShared Rides: 2 People vs 3+ People\n\n\nShow the code\n# 5. Shared Ride by the Number of People\nprint(\"\\n\" + \"=\" * 60)\nprint(\"5. SHARED RIDER BY NUMBER OF PEOPLE\")\nprint(\"=\" * 60)\n\nshared_ride_mapping = {\n    'auto-occ2' : 'sr2',\n    'auto-occ3p': 'sr3'\n}\n\ndf_shared_by_person = calculate_categorical_shares(\n    df_hts_linked,\n    category_col='model_trip_mode_WFv10',\n    weight_col='trip_weight',\n    group_cols=['Veh_Cat3p', 'Purp5_text', 'PK_OK'],\n    filter_condition='model_trip_mode_WFv10 in [\"auto-occ2\", \"auto-occ3p\"]',\n    category_mapping=shared_ride_mapping,\n    prefix='calib_share'\n)\n\nprint(\"\\nShape:\", df_shared_by_person.shape)\nprint(\"\\nFirst few rows:\")\nprint(df_shared_by_person.head(10))\nprint(\"\\nVerification:\")\nverify_shares(df_shared_by_person)\n\n\n\n============================================================\n5. SHARED RIDER BY NUMBER OF PEOPLE\n============================================================\n\nShape: (10, 8)\n\nFirst few rows:\n                           HBO                 HBW                 NHB  \\\n                            OK        PK        OK        PK        OK   \ncalib_share_sr2_0veh  0.709382  0.289280  0.731697  0.857568  0.740220   \ncalib_share_sr2_1veh  0.565882  0.518618  0.726008  0.795799  0.551582   \ncalib_share_sr2_2veh  0.512200  0.461630  0.792496  0.756507  0.458916   \ncalib_share_sr2_3veh  0.574960  0.531453  0.784871  0.702538  0.517496   \ncalib_share_sr2_all   0.543836  0.493188  0.770847  0.747774  0.498240   \ncalib_share_sr3_0veh  0.290618  0.710720  0.268303  0.142432  0.259780   \ncalib_share_sr3_1veh  0.434118  0.481382  0.273992  0.204201  0.448418   \ncalib_share_sr3_2veh  0.487800  0.538370  0.207504  0.243493  0.541084   \ncalib_share_sr3_3veh  0.425040  0.468547  0.215129  0.297462  0.482504   \ncalib_share_sr3_all   0.456164  0.506812  0.229153  0.252226  0.501760   \n\n                                   HBSch       HBC  \n                            PK     Daily     Daily  \ncalib_share_sr2_0veh  0.785824  0.000000  0.950890  \ncalib_share_sr2_1veh  0.509631  0.308240  0.707841  \ncalib_share_sr2_2veh  0.458682  0.298725  0.968752  \ncalib_share_sr2_3veh  0.493538  0.437045  0.582443  \ncalib_share_sr2_all   0.480974  0.360179  0.733967  \ncalib_share_sr3_0veh  0.214176  1.000000  0.049110  \ncalib_share_sr3_1veh  0.490369  0.691760  0.292159  \ncalib_share_sr3_2veh  0.541318  0.701275  0.031248  \ncalib_share_sr3_3veh  0.506462  0.562955  0.417557  \ncalib_share_sr3_all   0.519026  0.639821  0.266033  \n\nVerification:\n=== Verifying Share Sums ===\n\nVehicle categories found: ['0veh', '1veh', '2veh', '3veh', 'all']\n\n--- Checking 0veh (2 categories) ---\n  ✅ All shares sum to 1.0 for 0veh\n\n--- Checking 1veh (2 categories) ---\n  ✅ All shares sum to 1.0 for 1veh\n\n--- Checking 2veh (2 categories) ---\n  ✅ All shares sum to 1.0 for 2veh\n\n--- Checking 3veh (2 categories) ---\n  ✅ All shares sum to 1.0 for 3veh\n\n--- Checking all (2 categories) ---\n  ✅ All shares sum to 1.0 for all\n\n============================================================\n✅ ALL VEHICLE CATEGORIES: Shares sum to 1.0 (within tolerance)\n\nTotal columns: 8\nTotal vehicle categories: 5\nTotal combinations: 40\n\n\n{'0veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBSch  Daily    1.0\n HBC    Daily    1.0\n dtype: float64,\n '1veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBSch  Daily    1.0\n HBC    Daily    1.0\n dtype: float64,\n '2veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBSch  Daily    1.0\n HBC    Daily    1.0\n dtype: float64,\n '3veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBSch  Daily    1.0\n HBC    Daily    1.0\n dtype: float64,\n 'all': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBSch  Daily    1.0\n HBC    Daily    1.0\n dtype: float64}\n\n\n\n\n\nTransit Trips: By Service and By Access Mode\n\nBy Service Type\n\n\nShow the code\n# 6. Transit by Service Type (within linked transit trips)\nprint(\"\\n\" + \"=\" * 60)\nprint(\"6. TRANSIT BY SERVICE TYPE\")\nprint(\"=\" * 60)\n\n# Base service mapping\nservice_mapping = {\n    'LCL': 'LCL',\n    'COR': 'COR',\n    'EXP': 'EXP',\n    'LRT': 'LRT',\n    'CRT': 'CRT',\n    'BRT': 'BRT'\n}\n\n# 6. Transit by Service Type\ndf_transit_by_service = calculate_categorical_shares(\n    df_obs_linked,\n    category_col='Linked_Mode_txt',\n    weight_col='trip_weight',\n    group_cols=['Veh_Cat3p', 'Purp5_text', 'PK_OK'],\n    filter_condition='Linked_Mode_txt != \"MT\"',\n    category_mapping=service_mapping,\n    prefix='calib_share'\n)\n\nprint(\"\\nShape:\", df_transit_by_service.shape)\nprint(\"\\nFirst few rows:\")\nprint(df_transit_by_service.head(10))\nprint(\"\\nVerification:\")\nverify_shares(df_transit_by_service)\n\n\n\n============================================================\n6. TRANSIT BY SERVICE TYPE\n============================================================\n\nShape: (25, 8)\n\nFirst few rows:\n                           HBO                 HBW                 NHB  \\\n                            OK        PK        OK        PK        OK   \ncalib_share_BRT_0veh  0.088861  0.100063  0.043845  0.096335  0.145118   \ncalib_share_BRT_1veh  0.082129  0.057237  0.043697  0.032109  0.151861   \ncalib_share_BRT_2veh  0.045178  0.039916  0.024628  0.051088  0.204102   \ncalib_share_BRT_3veh  0.069742  0.074473  0.041242  0.048555  0.171744   \ncalib_share_BRT_all   0.074924  0.071330  0.038673  0.055927  0.161952   \ncalib_share_CRT_0veh  0.053186  0.051682  0.066407  0.082220  0.035799   \ncalib_share_CRT_1veh  0.114547  0.126383  0.139780  0.204544  0.086570   \ncalib_share_CRT_2veh  0.165676  0.166737  0.194121  0.275120  0.080481   \ncalib_share_CRT_3veh  0.198923  0.240181  0.268523  0.352321  0.057709   \ncalib_share_CRT_all   0.113275  0.121026  0.153034  0.212772  0.060859   \n\n                                     HBC     HBSch  \n                            PK     Daily     Daily  \ncalib_share_BRT_0veh  0.128626  0.260943  0.017211  \ncalib_share_BRT_1veh  0.086466  0.270153  0.045462  \ncalib_share_BRT_2veh  0.052782  0.182805  0.053173  \ncalib_share_BRT_3veh  0.127613  0.224523  0.076313  \ncalib_share_BRT_all   0.103378  0.233626  0.050701  \ncalib_share_CRT_0veh  0.022646  0.087325  0.000000  \ncalib_share_CRT_1veh  0.219213  0.156549  0.015040  \ncalib_share_CRT_2veh  0.080497  0.218088  0.088663  \ncalib_share_CRT_3veh  0.195968  0.247268  0.159022  \ncalib_share_CRT_all   0.108450  0.183626  0.066073  \n\nVerification:\n=== Verifying Share Sums ===\n\nVehicle categories found: ['0veh', '1veh', '2veh', '3veh', 'all']\n\n--- Checking 0veh (5 categories) ---\n  ✅ All shares sum to 1.0 for 0veh\n\n--- Checking 1veh (5 categories) ---\n  ✅ All shares sum to 1.0 for 1veh\n\n--- Checking 2veh (5 categories) ---\n  ✅ All shares sum to 1.0 for 2veh\n\n--- Checking 3veh (5 categories) ---\n  ✅ All shares sum to 1.0 for 3veh\n\n--- Checking all (5 categories) ---\n  ✅ All shares sum to 1.0 for all\n\n============================================================\n✅ ALL VEHICLE CATEGORIES: Shares sum to 1.0 (within tolerance)\n\nTotal columns: 8\nTotal vehicle categories: 5\nTotal combinations: 40\n\n\n{'0veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n '1veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n '2veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n '3veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n 'all': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64}\n\n\n\n\nBy Access Mode: Walk vs Drive to Transit\n\n\nShow the code\n# 7. Transit by Access Type (Walk vs Drive) (within linked transit trips)\nprint(\"\\n\" + \"=\" * 60)\nprint(\"7. TRANSIT BY ACCESS TYPE\")\nprint(\"=\" * 60)\n\naccess_mapping = {\n    'Walk': 'walkacc',\n    'Drive': 'driveacc'\n}\n\ndf_transit_by_access = calculate_categorical_shares(\n    df_obs_linked,\n    category_col='Ac_Mode2_Model',\n    weight_col='trip_weight',\n    group_cols=['Veh_Cat3p', 'Purp5_text', 'PK_OK'],\n    filter_condition='Linked_Mode_txt != \"MT\"',\n    category_mapping=access_mapping,\n    prefix='calib_share'\n)\n\nprint(\"\\nShape:\", df_transit_by_access.shape)\nprint(\"\\nFirst few rows:\")\nprint(df_transit_by_access.head(10))\nprint(\"\\nVerification:\")\nverify_shares(df_transit_by_access)\n\n\n\n============================================================\n7. TRANSIT BY ACCESS TYPE\n============================================================\n\nShape: (10, 8)\n\nFirst few rows:\n                                HBO                 HBW                 NHB  \\\n                                 OK        PK        OK        PK        OK   \ncalib_share_driveacc_0veh  0.042145  0.041288  0.039403  0.047530  0.054343   \ncalib_share_driveacc_1veh  0.124906  0.150309  0.152275  0.222280  0.046541   \ncalib_share_driveacc_2veh  0.176413  0.216849  0.279135  0.389120  0.025997   \ncalib_share_driveacc_3veh  0.266273  0.268424  0.436417  0.433596  0.038132   \ncalib_share_driveacc_all   0.123080  0.138864  0.197175  0.252116  0.044596   \ncalib_share_walkacc_0veh   0.957855  0.958712  0.960597  0.952470  0.945657   \ncalib_share_walkacc_1veh   0.875094  0.849691  0.847725  0.777720  0.953459   \ncalib_share_walkacc_2veh   0.823587  0.783151  0.720865  0.610880  0.974003   \ncalib_share_walkacc_3veh   0.733727  0.731576  0.563583  0.566404  0.961868   \ncalib_share_walkacc_all    0.876920  0.861136  0.802825  0.747884  0.955404   \n\n                                          HBC     HBSch  \n                                 PK     Daily     Daily  \ncalib_share_driveacc_0veh  0.024196  0.036860  0.018820  \ncalib_share_driveacc_1veh  0.093195  0.232073  0.078042  \ncalib_share_driveacc_2veh  0.091746  0.366970  0.171644  \ncalib_share_driveacc_3veh  0.104019  0.535594  0.162649  \ncalib_share_driveacc_all   0.065612  0.311190  0.120943  \ncalib_share_walkacc_0veh   0.975804  0.963140  0.981180  \ncalib_share_walkacc_1veh   0.906805  0.767927  0.921958  \ncalib_share_walkacc_2veh   0.908254  0.633030  0.828356  \ncalib_share_walkacc_3veh   0.895981  0.464406  0.837351  \ncalib_share_walkacc_all    0.934388  0.688810  0.879057  \n\nVerification:\n=== Verifying Share Sums ===\n\nVehicle categories found: ['0veh', '1veh', '2veh', '3veh', 'all']\n\n--- Checking 0veh (2 categories) ---\n  ✅ All shares sum to 1.0 for 0veh\n\n--- Checking 1veh (2 categories) ---\n  ✅ All shares sum to 1.0 for 1veh\n\n--- Checking 2veh (2 categories) ---\n  ✅ All shares sum to 1.0 for 2veh\n\n--- Checking 3veh (2 categories) ---\n  ✅ All shares sum to 1.0 for 3veh\n\n--- Checking all (2 categories) ---\n  ✅ All shares sum to 1.0 for all\n\n============================================================\n✅ ALL VEHICLE CATEGORIES: Shares sum to 1.0 (within tolerance)\n\nTotal columns: 8\nTotal vehicle categories: 5\nTotal combinations: 40\n\n\n{'0veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n '1veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n '2veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n '3veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n 'all': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64}\n\n\n\nWalk to Transit\n\n\nShow the code\n# 8. Walk-to-Transit by Service Type (within walk-to-transit trips)\nprint(\"\\n\" + \"=\" * 60)\nprint(\"8. WALK-TO-TRANSIT BY SERVICE TYPE\")\nprint(\"=\" * 60)\n\n# Base walk-to-transit mapping\nwalk_service_mapping = {\n    'LCL': 'wLCL',\n    'COR': 'wCOR',\n    'EXP': 'wEXP',\n    'LRT': 'wLRT',\n    'CRT': 'wCRT',\n    'BRT': 'wBRT'\n}\n\n# 8. Walk-to-Transit by Service Type\ndf_walk_to_transit = calculate_categorical_shares(\n    df_obs_linked,\n    category_col='Linked_Mode_txt',\n    weight_col='trip_weight',\n    group_cols=['Veh_Cat3p', 'Purp5_text', 'PK_OK'],\n    filter_condition='Ac_Mode2_Model == \"Walk\" & Linked_Mode_txt != \"MT\"',\n    category_mapping=walk_service_mapping,\n    prefix='calib_share'\n)\n\n\nprint(\"\\nShape:\", df_walk_to_transit.shape)\nprint(\"\\nFirst few rows:\")\nprint(df_walk_to_transit.head(10))\nprint(\"\\nVerification:\")\nverify_shares(df_walk_to_transit)\n\n\n\n============================================================\n8. WALK-TO-TRANSIT BY SERVICE TYPE\n============================================================\n\nShape: (25, 8)\n\nFirst few rows:\n                            HBO                 HBW                 NHB  \\\n                             OK        PK        OK        PK        OK   \ncalib_share_wBRT_0veh  0.092771  0.104372  0.045643  0.101143  0.148794   \ncalib_share_wBRT_1veh  0.093851  0.067362  0.051547  0.039559  0.159274   \ncalib_share_wBRT_2veh  0.054856  0.050969  0.034165  0.062345  0.203986   \ncalib_share_wBRT_3veh  0.091713  0.095129  0.059455  0.085725  0.178553   \ncalib_share_wBRT_all   0.085061  0.082176  0.046614  0.069424  0.166518   \ncalib_share_wCRT_0veh  0.040459  0.038498  0.062529  0.069506  0.019720   \ncalib_share_wCRT_1veh  0.053546  0.054532  0.092601  0.105899  0.071298   \ncalib_share_wCRT_2veh  0.090631  0.066456  0.100877  0.105374  0.061502   \ncalib_share_wCRT_3veh  0.052965  0.088973  0.099876  0.203118  0.025995   \ncalib_share_wCRT_all   0.055574  0.053794  0.085015  0.103989  0.042503   \n\n                                      HBC     HBSch  \n                             PK     Daily     Daily  \ncalib_share_wBRT_0veh  0.131816  0.270929  0.017541  \ncalib_share_wBRT_1veh  0.095352  0.281913  0.026821  \ncalib_share_wBRT_2veh  0.058114  0.197505  0.055210  \ncalib_share_wBRT_3veh  0.142428  0.238967  0.091136  \ncalib_share_wBRT_all   0.110637  0.251209  0.046426  \ncalib_share_wCRT_0veh  0.023207  0.058962  0.000000  \ncalib_share_wCRT_1veh  0.173612  0.060955  0.002162  \ncalib_share_wCRT_2veh  0.054967  0.085353  0.021008  \ncalib_share_wCRT_3veh  0.141049  0.107714  0.011371  \ncalib_share_wCRT_all   0.082568  0.074655  0.009968  \n\nVerification:\n=== Verifying Share Sums ===\n\nVehicle categories found: ['0veh', '1veh', '2veh', '3veh', 'all']\n\n--- Checking 0veh (5 categories) ---\n  ✅ All shares sum to 1.0 for 0veh\n\n--- Checking 1veh (5 categories) ---\n  ✅ All shares sum to 1.0 for 1veh\n\n--- Checking 2veh (5 categories) ---\n  ✅ All shares sum to 1.0 for 2veh\n\n--- Checking 3veh (5 categories) ---\n  ✅ All shares sum to 1.0 for 3veh\n\n--- Checking all (5 categories) ---\n  ✅ All shares sum to 1.0 for all\n\n============================================================\n✅ ALL VEHICLE CATEGORIES: Shares sum to 1.0 (within tolerance)\n\nTotal columns: 8\nTotal vehicle categories: 5\nTotal combinations: 40\n\n\n{'0veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n '1veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n '2veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n '3veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n 'all': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64}\n\n\n\n\nDrive to Transit\n\n\nShow the code\n# 9. Drive-to-Transit by Service Type (within drive-to-transit trips)\nprint(\"\\n\" + \"=\" * 60)\nprint(\"9. DRIVE-TO-TRANSIT BY SERVICE TYPE\")\nprint(\"=\" * 60)\n\n# Base drive-to-transit mapping\ndrive_service_mapping = {\n    'LCL': 'dLCL',\n    'COR': 'dCOR',\n    'EXP': 'dEXP',\n    'LRT': 'dLRT',\n    'CRT': 'dCRT',\n    'BRT': 'dBRT',\n}\n\n# 9. Drive-to-Transit by Service Type\ndf_drive_to_transit = calculate_categorical_shares(\n    df_obs_linked,\n    category_col='Linked_Mode_txt',\n    weight_col='trip_weight',\n    group_cols=['Veh_Cat3p', 'Purp5_text', 'PK_OK'],\n    filter_condition='Ac_Mode2_Model == \"Drive\" & Linked_Mode_txt != \"MT\"',\n    category_mapping=drive_service_mapping,\n    prefix='calib_share'\n)\n\nprint(\"\\nShape:\", df_drive_to_transit.shape)\nprint(\"\\nFirst few rows:\")\nprint(df_drive_to_transit.head(10))\nprint(\"\\nVerification:\")\nverify_shares(df_drive_to_transit)\n\n\n\n============================================================\n9. DRIVE-TO-TRANSIT BY SERVICE TYPE\n============================================================\n\nShape: (24, 8)\n\nFirst few rows:\n                            HBO                 HBW                 NHB  \\\n                             OK        PK        OK        PK        OK   \ncalib_share_dBRT_0veh  0.000000  0.000000  0.000000  0.000000  0.081156   \ncalib_share_dBRT_1veh  0.000000  0.000000  0.000000  0.006040  0.000000   \ncalib_share_dBRT_2veh  0.000000  0.000000  0.000000  0.033415  0.208436   \ncalib_share_dBRT_3veh  0.009200  0.018177  0.017723  0.000000  0.000000   \ncalib_share_dBRT_all   0.002703  0.004070  0.006341  0.015889  0.064129   \ncalib_share_dCRT_0veh  0.342430  0.357806  0.160948  0.336995  0.315598   \ncalib_share_dCRT_1veh  0.541918  0.532558  0.402430  0.549689  0.399439   \ncalib_share_dCRT_2veh  0.516026  0.528904  0.434922  0.541603  0.791564   \ncalib_share_dCRT_3veh  0.601117  0.652292  0.486311  0.547224  0.857674   \ncalib_share_dCRT_all   0.524379  0.537952  0.429982  0.535468  0.454111   \n\n                            HBC       NHB     HBSch  \n                          Daily        PK     Daily  \ncalib_share_dBRT_0veh  0.000000  0.000000  0.000000  \ncalib_share_dBRT_1veh  0.231240  0.000000  0.265673  \ncalib_share_dBRT_2veh  0.157446  0.000000  0.043343  \ncalib_share_dBRT_3veh  0.211999  0.000000  0.000000  \ncalib_share_dBRT_all   0.194706  0.000000  0.081770  \ncalib_share_dCRT_0veh  0.828412  0.000000  0.000000  \ncalib_share_dCRT_1veh  0.472869  0.662928  0.167173  \ncalib_share_dCRT_2veh  0.447060  0.333233  0.415167  \ncalib_share_dCRT_3veh  0.368273  0.669020  0.919162  \ncalib_share_dCRT_all   0.424829  0.477048  0.473862  \n\nVerification:\n=== Verifying Share Sums ===\n\nVehicle categories found: ['0veh', '1veh', '2veh', '3veh', 'all']\n\n--- Checking 0veh (4 categories) ---\n  ✅ All shares sum to 1.0 for 0veh\n\n--- Checking 1veh (5 categories) ---\n  ✅ All shares sum to 1.0 for 1veh\n\n--- Checking 2veh (5 categories) ---\n  ✅ All shares sum to 1.0 for 2veh\n\n--- Checking 3veh (5 categories) ---\n  ✅ All shares sum to 1.0 for 3veh\n\n--- Checking all (5 categories) ---\n  ✅ All shares sum to 1.0 for all\n\n============================================================\n✅ ALL VEHICLE CATEGORIES: Shares sum to 1.0 (within tolerance)\n\nTotal columns: 8\nTotal vehicle categories: 5\nTotal combinations: 40\n\n\n{'0veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n HBC    Daily    1.0\n NHB    PK       1.0\n HBSch  Daily    1.0\n dtype: float64,\n '1veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n HBC    Daily    1.0\n NHB    PK       1.0\n HBSch  Daily    1.0\n dtype: float64,\n '2veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n HBC    Daily    1.0\n NHB    PK       1.0\n HBSch  Daily    1.0\n dtype: float64,\n '3veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n HBC    Daily    1.0\n NHB    PK       1.0\n HBSch  Daily    1.0\n dtype: float64,\n 'all': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n HBC    Daily    1.0\n NHB    PK       1.0\n HBSch  Daily    1.0\n dtype: float64}"
  },
  {
    "objectID": "2-mode-choice-targets.html#define-expansion-function",
    "href": "2-mode-choice-targets.html#define-expansion-function",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "8.1 Define Expansion Function",
    "text": "8.1 Define Expansion Function\n\n\nShow the code\ndef expand_crt_categories(df_shares, crt_purpose_props, crt_var_prefix='CRT'):\n    \"\"\"\n    Expand single CRT category into detailed CRT subclasses.\n    Uses the prefix directly as provided (CRT, wCRT, dCRT).\n\n    Parameters:\n    -----------\n    df_shares : DataFrame\n        DataFrame with shares including single CRT category\n    crt_purpose_props : dict\n        Dictionary mapping Purpose -&gt; {class_CRT: proportion}\n    crt_var_prefix : str\n        Prefix for CRT variable (e.g., 'CRT', 'wCRT', 'dCRT')\n\n    Returns:\n    --------\n    DataFrame with expanded CRT categories\n    \"\"\"\n    df_expanded = df_shares.copy()\n\n    # Find all CRT rows (e.g., calib_share_CRT_all, calib_share_wCRT_2veh)\n    crt_pattern = f'calib_share_{crt_var_prefix}_'\n    crt_rows = df_expanded.index[df_expanded.index.str.contains(crt_pattern)].tolist()\n\n    if len(crt_rows) == 0:\n        return df_expanded\n\n    # Dictionary to store new rows\n    new_rows_dict = {}\n\n    for crt_row in crt_rows:\n        # Extract vehicle category from variable name\n        # e.g., \"calib_share_CRT_all\" -&gt; \"all\"\n        # e.g., \"calib_share_wCRT_2veh\" -&gt; \"2veh\"\n        veh_cat = crt_row.split('_')[-1]\n\n        # Get CRT share for each purpose/period combination\n        for col in df_expanded.columns:\n            purpose, period = col\n\n            crt_share = df_expanded.loc[crt_row, col]\n\n            # Skip if CRT share is 0 or purpose not in props\n            if crt_share == 0 or purpose not in crt_purpose_props:\n                continue\n\n            # Expand into CRT subclasses\n            for crt_class, crt_prop in crt_purpose_props[purpose].items():\n                # Create variable name for this CRT subclass\n                # CRT_00 -&gt; CRT00, CRT_01 -&gt; CRT01, etc.\n                crt_suffix = crt_class.replace('_', '')  # CRT_00 -&gt; CRT00\n\n                # Build variable name: calib_share_{prefix}{suffix}_{veh_cat}\n                # Examples:\n                #   prefix='CRT'  -&gt; calib_share_CRT00_all\n                #   prefix='wCRT' -&gt; calib_share_wCRT00_all\n                #   prefix='dCRT' -&gt; calib_share_dCRT00_all\n                new_var_name = f\"calib_share_{crt_var_prefix}{crt_suffix[3:]}_{veh_cat}\"\n\n                # Calculate expanded share\n                expanded_share = crt_share * crt_prop\n\n                # Initialize row if it doesn't exist\n                if new_var_name not in new_rows_dict:\n                    new_rows_dict[new_var_name] = pd.Series(0.0, index=df_expanded.columns, name=new_var_name)\n\n                # Set the value\n                new_rows_dict[new_var_name][col] = expanded_share\n\n    # Add new rows and remove old CRT rows\n    if new_rows_dict:\n        # Remove original single CRT rows first\n        df_result = df_expanded.drop(crt_rows)\n\n        # Convert new rows dict to DataFrame and concatenate\n        df_new_rows = pd.DataFrame.from_dict(new_rows_dict, orient='index')\n        df_result = pd.concat([df_result, df_new_rows], axis=0)\n\n        # Sort index\n        return df_result.sort_index()\n\n    return df_expanded"
  },
  {
    "objectID": "2-mode-choice-targets.html#expand-transit-dataframes",
    "href": "2-mode-choice-targets.html#expand-transit-dataframes",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "8.2 Expand Transit Dataframes",
    "text": "8.2 Expand Transit Dataframes\n\nBy Service Type\n\n\nShow the code\n# Expand CRT in transit by service\nprint(\"\\nExpanding CRT in Transit by Service...\")\ndf_transit_by_service_expanded = expand_crt_categories(\n    df_transit_by_service,\n    crt_purpose_props,\n    crt_var_prefix='CRT'\n)\nprint(f\"  Original shape: {df_transit_by_service.shape}\")\nprint(f\"  Expanded shape: {df_transit_by_service_expanded.shape}\")\nprint(\"\\nVerification after expansion:\")\nverify_shares(df_transit_by_service_expanded)\n\n\n\nExpanding CRT in Transit by Service...\n  Original shape: (25, 8)\n  Expanded shape: (65, 8)\n\nVerification after expansion:\n=== Verifying Share Sums ===\n\nVehicle categories found: ['0veh', '1veh', '2veh', '3veh', 'all']\n\n--- Checking 0veh (13 categories) ---\n  ✅ All shares sum to 1.0 for 0veh\n\n--- Checking 1veh (13 categories) ---\n  ✅ All shares sum to 1.0 for 1veh\n\n--- Checking 2veh (13 categories) ---\n  ✅ All shares sum to 1.0 for 2veh\n\n--- Checking 3veh (13 categories) ---\n  ✅ All shares sum to 1.0 for 3veh\n\n--- Checking all (13 categories) ---\n  ✅ All shares sum to 1.0 for all\n\n============================================================\n✅ ALL VEHICLE CATEGORIES: Shares sum to 1.0 (within tolerance)\n\nTotal columns: 8\nTotal vehicle categories: 5\nTotal combinations: 40\n\n\n{'0veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n '1veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n '2veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n '3veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n 'all': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64}\n\n\n\n\nShow the code\n# Replace original dataframes with expanded versions\ndf_transit_by_service = df_transit_by_service_expanded\n\n\n\n\nBy Walk Access\n\n\nShow the code\n# Expand CRT in walk-to-transit\nprint(\"\\nExpanding CRT in Walk-to-Transit...\")\ndf_walk_to_transit_expanded = expand_crt_categories(\n    df_walk_to_transit,\n    crt_purpose_props,\n    crt_var_prefix='wCRT'\n)\nprint(f\"  Original shape: {df_walk_to_transit.shape}\")\nprint(f\"  Expanded shape: {df_walk_to_transit_expanded.shape}\")\nprint(\"\\nVerification after expansion:\")\nverify_shares(df_walk_to_transit_expanded)\n\n\n\nExpanding CRT in Walk-to-Transit...\n  Original shape: (25, 8)\n  Expanded shape: (65, 8)\n\nVerification after expansion:\n=== Verifying Share Sums ===\n\nVehicle categories found: ['0veh', '1veh', '2veh', '3veh', 'all']\n\n--- Checking 0veh (13 categories) ---\n  ✅ All shares sum to 1.0 for 0veh\n\n--- Checking 1veh (13 categories) ---\n  ✅ All shares sum to 1.0 for 1veh\n\n--- Checking 2veh (13 categories) ---\n  ✅ All shares sum to 1.0 for 2veh\n\n--- Checking 3veh (13 categories) ---\n  ✅ All shares sum to 1.0 for 3veh\n\n--- Checking all (13 categories) ---\n  ✅ All shares sum to 1.0 for all\n\n============================================================\n✅ ALL VEHICLE CATEGORIES: Shares sum to 1.0 (within tolerance)\n\nTotal columns: 8\nTotal vehicle categories: 5\nTotal combinations: 40\n\n\n{'0veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n '1veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n '2veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n '3veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64,\n 'all': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n        PK       1.0\n HBC    Daily    1.0\n HBSch  Daily    1.0\n dtype: float64}\n\n\n\n\nShow the code\n# Replace original dataframes with expanded versions\ndf_walk_to_transit = df_walk_to_transit_expanded\n\n\n\n\nBy Drive Access\n\n\nShow the code\n# Expand CRT in drive-to-transit\nprint(\"\\nExpanding CRT in Drive-to-Transit...\")\ndf_drive_to_transit_expanded = expand_crt_categories(\n    df_drive_to_transit,\n    crt_purpose_props,\n    crt_var_prefix='dCRT'\n)\nprint(f\"  Original shape: {df_drive_to_transit.shape}\")\nprint(f\"  Expanded shape: {df_drive_to_transit_expanded.shape}\")\nprint(\"\\nVerification after expansion:\")\nverify_shares(df_drive_to_transit_expanded)\n\n\n\nExpanding CRT in Drive-to-Transit...\n  Original shape: (24, 8)\n  Expanded shape: (64, 8)\n\nVerification after expansion:\n=== Verifying Share Sums ===\n\nVehicle categories found: ['0veh', '1veh', '2veh', '3veh', 'all']\n\n--- Checking 0veh (12 categories) ---\n  ✅ All shares sum to 1.0 for 0veh\n\n--- Checking 1veh (13 categories) ---\n  ✅ All shares sum to 1.0 for 1veh\n\n--- Checking 2veh (13 categories) ---\n  ✅ All shares sum to 1.0 for 2veh\n\n--- Checking 3veh (13 categories) ---\n  ✅ All shares sum to 1.0 for 3veh\n\n--- Checking all (13 categories) ---\n  ✅ All shares sum to 1.0 for all\n\n============================================================\n✅ ALL VEHICLE CATEGORIES: Shares sum to 1.0 (within tolerance)\n\nTotal columns: 8\nTotal vehicle categories: 5\nTotal combinations: 40\n\n\n{'0veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n HBC    Daily    1.0\n NHB    PK       1.0\n HBSch  Daily    1.0\n dtype: float64,\n '1veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n HBC    Daily    1.0\n NHB    PK       1.0\n HBSch  Daily    1.0\n dtype: float64,\n '2veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n HBC    Daily    1.0\n NHB    PK       1.0\n HBSch  Daily    1.0\n dtype: float64,\n '3veh': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n HBC    Daily    1.0\n NHB    PK       1.0\n HBSch  Daily    1.0\n dtype: float64,\n 'all': HBO    OK       1.0\n        PK       1.0\n HBW    OK       1.0\n        PK       1.0\n NHB    OK       1.0\n HBC    Daily    1.0\n NHB    PK       1.0\n HBSch  Daily    1.0\n dtype: float64}\n\n\n\n\nShow the code\n# Replace original dataframes with expanded versions\ndf_drive_to_transit = df_drive_to_transit_expanded"
  },
  {
    "objectID": "2-mode-choice-targets.html#output-folder",
    "href": "2-mode-choice-targets.html#output-folder",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "9.1 Output Folder",
    "text": "9.1 Output Folder\n\n\nShow the code\n# Create output directory if it doesn't exist\noutput_dir = Path(\"_output\")\noutput_dir.mkdir(parents=True, exist_ok=True)\n\nprint(\"=\" * 80)\nprint(\"EXPORTING CALIBRATION TARGET FILES\")\nprint(\"=\" * 80)\n\n\n================================================================================\nEXPORTING CALIBRATION TARGET FILES\n================================================================================"
  },
  {
    "objectID": "2-mode-choice-targets.html#helper-functions-1",
    "href": "2-mode-choice-targets.html#helper-functions-1",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "9.2 Helper Functions",
    "text": "9.2 Helper Functions\n\nExtract Values\n\n\nShow the code\ndef extract_values(df, purpose, period, pattern, include_veh_categories=True):\n    \"\"\"\n    Extract values from dataframe for given purpose/period matching a pattern.\n\n    Parameters:\n    -----------\n    df : DataFrame\n        DataFrame with MultiIndex columns (purpose, period)\n    purpose : str\n        Purpose to filter (e.g., 'HBW', 'HBO')\n    period : str\n        Period to filter (e.g., 'PK', 'OK', 'Daily')\n    pattern : str\n        Pattern to match in index (e.g., 'motor', 'bike', 'transit')\n    include_veh_categories : bool\n        If True, include all vehicle categories. If False, only include '_all' and strip suffix.\n\n    Returns:\n    --------\n    dict : {variable_name: value}\n    \"\"\"\n    results = {}\n\n    # Check if column exists\n    if (purpose, period) not in df.columns:\n        return results\n\n    # Filter rows matching pattern\n    mask = df.index.str.contains(pattern)\n\n    for idx in df[mask].index:\n        value = df.loc[idx, (purpose, period)]\n\n        if include_veh_categories:\n            # Include all vehicle categories as-is\n            results[idx] = value\n        else:\n            # Only include '_all' suffix and strip it\n            if idx.endswith('_all'):\n                # Remove '_all' suffix\n                var_name = idx.rsplit('_', 1)[0]\n                results[var_name] = value\n\n    return results\n\n\n\n\nExport Function\n\n\nShow the code\ndef export_calib_file(purpose, period, export_mode=\"only_needed\", remove_all_suffix=False):\n    \"\"\"\n    Export calibration target file for a given purpose and period.\n\n    Parameters:\n    -----------\n    purpose : str\n        Trip purpose (HBW, HBO, NHB, HBC, HBSch)\n    period : str\n        Time period (PK, OK, Daily)\n    export_mode : str\n        Export mode: \"export_all\", \"comment_unused\", \"only_needed\"\n    remove_all_suffix : bool\n        Remove _all suffix when no vehicle disaggregation exists in entire file\n    \"\"\"\n\n    # Determine file period (Daily becomes 'pk' for HBC/HBSch)\n    file_period = 'pk' if period == 'Daily' else period.lower()\n\n    # Determine display period for header\n    display_period = file_period.capitalize()\n\n    filename = f\"calib_target_{purpose.lower()}_{file_period}.txt\"\n    filepath = os.path.join(output_dir, filename)\n\n    print(f\"\\nCreating {filename}... (Mode: {export_mode})\")\n\n    # Get pattern configuration for this purpose/period\n    patterns = variable_veh_patterns.get(purpose, {}).get(period, {})\n\n    # Check if entire file has only 'all' suffix (for suffix removal)\n    file_has_only_all = all(\n        'all' in patterns.get(var, ['all']) and len(patterns.get(var, ['all'])) == 1\n        for var in patterns.keys()\n    )\n    apply_suffix_removal = remove_all_suffix and file_has_only_all\n\n    # Helper function to get variable name based on settings\n    def get_var_name(base_name, veh_cat):\n        var_name = f\"{base_name}_{veh_cat}\"\n        if apply_suffix_removal and veh_cat == 'all':\n            var_name = base_name\n        return var_name\n\n    # Helper function to determine if a variable should be included/commented\n    def should_include(var_family, veh_cat):\n        \"\"\"\n        Returns tuple: (include, comment)\n        - include: whether to write the line\n        - comment: whether to prefix with ';'\n        \"\"\"\n        # Mode 1: Export everything, no comments\n        if export_mode == \"export_all\":\n            # Exclude 3veh in mode 1 would be incorrect based on user request\n            # User said keep 3veh for mode 1\n            return (True, False)\n\n        # Get pattern for this variable family\n        pattern = patterns.get(var_family, ['all'])\n\n        # For modes 2 and 3, exclude 3veh (it's the base category)\n        if veh_cat == '3veh':\n            if export_mode == \"only_needed\":\n                return (False, False)  # Don't include at all\n            elif export_mode == \"comment_unused\":\n                return (True, True)  # Include but comment out\n\n        # Mode 2: Export all, comment unused\n        if export_mode == \"comment_unused\":\n            if veh_cat in pattern:\n                return (True, False)  # Include, no comment\n            else:\n                return (True, True)   # Include, but commented\n\n        # Mode 3: Only needed\n        if export_mode == \"only_needed\":\n            if veh_cat in pattern:\n                return (True, False)  # Include\n            else:\n                return (False, False) # Don't include\n\n        return (False, False)\n\n    # Helper function to extract and format values\n    def extract_and_write(f, df, var_pattern, var_families, section_comment, sub_comment=None):\n        \"\"\"\n        Extract values and write to file with proper formatting\n\n        Parameters:\n        -----------\n        f : file handle\n        df : DataFrame with values\n        var_pattern : str - pattern to match in index\n        var_families : list - list of variable family names (e.g., ['bike', 'walk'])\n        section_comment : str - main section comment\n        sub_comment : str - optional sub-section comment\n        \"\"\"\n        f.write(f\"{section_comment}\\t\\t\\n\")\n        if sub_comment:\n            f.write(f\"  {sub_comment}\\t\\t\\n\")\n\n        has_output = False\n\n        for var_family in var_families:\n            # Try all vehicle categories\n            for veh_cat in ['0veh', '1veh', '2veh', '3veh', 'all']:\n                include, comment = should_include(var_family, veh_cat)\n\n                if not include:\n                    continue\n\n                # Construct the full variable name in the dataframe\n                df_var_name = f\"calib_share_{var_family}_{veh_cat}\"\n\n                # Check if variable exists in dataframe\n                if df_var_name not in df.index:\n                    continue\n\n                # Check if column exists\n                if (purpose, period) not in df.columns:\n                    continue\n\n                value = df.loc[df_var_name, (purpose, period)]\n\n                # Get output variable name (with potential suffix removal)\n                output_var_name = get_var_name(f\"calib_share_{var_family}\", veh_cat)\n\n                # Write the line\n                comment_prefix = \";\" if comment else \" \"\n                f.write(f\"{comment_prefix} {output_var_name:&lt;30}\\t=\\t{value:.4f}\\n\")\n                has_output = True\n\n        if has_output:\n            f.write(\"\\t\\t\\n\")\n        f.write(\"\\t\\t\\n\")\n\n    with open(filepath, 'w') as f:\n        # Write header\n        f.write(f\";Calibration target values - {purpose} {display_period}\\t\\t\\n\")\n        f.write(\"\\t\\t\\n\")\n\n        # Write disclaimer - only for export_all and comment_unused modes\n        if export_mode in [\"export_all\", \"comment_unused\"]:\n            f.write(f\";Note: Not all of the variables listed below are used directly in the Mode Choice model. Some are included solely for verification and cross-checking purposes.\\t\\t\\n\")\n            f.write(\"\\t\\t\\n\")\n\n        # =====================================================================\n        # SECTION 1: Motorized vs Non-motorized (within total trips)\n        # =====================================================================\n        extract_and_write(\n            f, df_alltrips_cat, 'nonmotor_',\n            ['nonmotor'],\n            \";relative within total trips\",\n            \";nonmotorized shares \"\n        )\n\n        # =====================================================================\n        # SECTION 2: Walk vs Bike (within nonmotorized)\n        # =====================================================================\n        f.write(\";relative within nonmotorized\\t\\t\\n\")\n\n        # Walk shares\n        for veh_cat in ['0veh', '1veh', '2veh', '3veh', 'all']:\n            include, comment = should_include('walk', veh_cat)\n            if include:\n                df_var_name = f\"calib_share_walk_{veh_cat}\"\n                if df_var_name in df_nonmotorized_cat.index and (purpose, period) in df_nonmotorized_cat.columns:\n                    value = df_nonmotorized_cat.loc[df_var_name, (purpose, period)]\n                    output_var_name = get_var_name(\"calib_share_walk\", veh_cat)\n                    comment_prefix = \";\" if comment else \" \"\n                    if not comment:\n                        f.write(\"  ;walk shares \\t\\t\\n\")\n                    f.write(f\"{comment_prefix} {output_var_name:&lt;30}\\t=\\t{value:.4f}\\n\")\n                    if not comment:\n                        f.write(\"\\t\\t\\n\")\n\n        # Bike shares\n        for veh_cat in ['0veh', '1veh', '2veh', '3veh', 'all']:\n            include, comment = should_include('bike', veh_cat)\n            if include:\n                df_var_name = f\"calib_share_bike_{veh_cat}\"\n                if df_var_name in df_nonmotorized_cat.index and (purpose, period) in df_nonmotorized_cat.columns:\n                    value = df_nonmotorized_cat.loc[df_var_name, (purpose, period)]\n                    output_var_name = get_var_name(\"calib_share_bike\", veh_cat)\n                    comment_prefix = \";\" if comment else \" \"\n                    if not comment:\n                        f.write(\"  ;bike shares \\t\\t\\n\")\n                    f.write(f\"{comment_prefix} {output_var_name:&lt;30}\\t=\\t{value:.4f}\\n\")\n\n        f.write(\"\\t\\t\\n\")\n        f.write(\"\\t\\t\\n\")\n\n        # =====================================================================\n        # SECTION 3: Drive Alone vs Shared Ride (within auto)\n        # =====================================================================\n        f.write(\";relative within auto\\t\\t\\n\")\n\n        # Alone shares\n        for veh_cat in ['0veh', '1veh', '2veh', '3veh', 'all']:\n            include, comment = should_include('alone', veh_cat)\n            if include:\n                df_var_name = f\"calib_share_alone_{veh_cat}\"\n                if df_var_name in df_auto_riders.index and (purpose, period) in df_auto_riders.columns:\n                    value = df_auto_riders.loc[df_var_name, (purpose, period)]\n                    output_var_name = get_var_name(\"calib_share_alone\", veh_cat)\n                    comment_prefix = \";\" if comment else \" \"\n                    if not comment:\n                        f.write(\"  ;drive alone shares \\t\\t\\n\")\n                    f.write(f\"{comment_prefix} {output_var_name:&lt;30}\\t=\\t{value:.4f}\\n\")\n                    if not comment:\n                        f.write(\"\\t\\t\\n\")\n\n        # Shared shares\n        for veh_cat in ['0veh', '1veh', '2veh', '3veh', 'all']:\n            include, comment = should_include('shared', veh_cat)\n            if include:\n                df_var_name = f\"calib_share_shared_{veh_cat}\"\n                if df_var_name in df_auto_riders.index and (purpose, period) in df_auto_riders.columns:\n                    value = df_auto_riders.loc[df_var_name, (purpose, period)]\n                    output_var_name = get_var_name(\"calib_share_shared\", veh_cat)\n                    comment_prefix = \";\" if comment else \" \"\n                    if not comment:\n                        f.write(\"  ;shared ride shares \\t\\t\\n\")\n                    f.write(f\"{comment_prefix} {output_var_name:&lt;30}\\t=\\t{value:.4f}\\n\")\n\n        f.write(\"\\t\\t\\n\")\n        f.write(\"\\t\\t\\n\")\n\n        # =====================================================================\n        # SECTION 4: SR2 vs SR3 (within shared ride)\n        # =====================================================================\n        f.write(\";relative within shared ride\\t\\t\\n\")\n\n        # SR2 shares\n        for veh_cat in ['0veh', '1veh', '2veh', '3veh', 'all']:\n            include, comment = should_include('sr2', veh_cat)\n            if include:\n                df_var_name = f\"calib_share_sr2_{veh_cat}\"\n                if df_var_name in df_shared_by_person.index and (purpose, period) in df_shared_by_person.columns:\n                    value = df_shared_by_person.loc[df_var_name, (purpose, period)]\n                    output_var_name = get_var_name(\"calib_share_sr2\", veh_cat)\n                    comment_prefix = \";\" if comment else \" \"\n                    if not comment:\n                        f.write(\"  ;sr2 shares \\t\\t\\n\")\n                    f.write(f\"{comment_prefix} {output_var_name:&lt;30}\\t=\\t{value:.4f}\\n\")\n                    if not comment:\n                        f.write(\"\\t\\t\\n\")\n\n        # SR3 shares\n        for veh_cat in ['0veh', '1veh', '2veh', '3veh', 'all']:\n            include, comment = should_include('sr3', veh_cat)\n            if include:\n                df_var_name = f\"calib_share_sr3_{veh_cat}\"\n                if df_var_name in df_shared_by_person.index and (purpose, period) in df_shared_by_person.columns:\n                    value = df_shared_by_person.loc[df_var_name, (purpose, period)]\n                    output_var_name = get_var_name(\"calib_share_sr3\", veh_cat)\n                    comment_prefix = \";\" if comment else \" \"\n                    if not comment:\n                        f.write(\"  ;sr3 shares \\t\\t\\n\")\n                    f.write(f\"{comment_prefix} {output_var_name:&lt;30}\\t=\\t{value:.4f}\\n\")\n\n        f.write(\"\\t\\t\\n\")\n        f.write(\"\\t\\t\\n\")\n\n        # =====================================================================\n        # SECTION 5: Auto vs Transit (within motorized)\n        # =====================================================================\n        f.write(\";relative within motorized\\t\\t\\n\")\n\n        # Auto shares\n        for veh_cat in ['0veh', '1veh', '2veh', '3veh', 'all']:\n            include, comment = should_include('auto', veh_cat)\n            if include:\n                df_var_name = f\"calib_share_auto_{veh_cat}\"\n                if df_var_name in df_motorized_cat.index and (purpose, period) in df_motorized_cat.columns:\n                    value = df_motorized_cat.loc[df_var_name, (purpose, period)]\n                    output_var_name = get_var_name(\"calib_share_auto\", veh_cat)\n                    comment_prefix = \";\" if comment else \" \"\n                    if not comment:\n                        f.write(\"  ;auto shares \\t\\t\\n\")\n                    f.write(f\"{comment_prefix} {output_var_name:&lt;30}\\t=\\t{value:.4f}\\n\")\n                    if not comment:\n                        f.write(\"\\t\\t\\n\")\n\n        # Transit shares\n        for veh_cat in ['0veh', '1veh', '2veh', '3veh', 'all']:\n            include, comment = should_include('transit', veh_cat)\n            if include:\n                df_var_name = f\"calib_share_transit_{veh_cat}\"\n                if df_var_name in df_motorized_cat.index and (purpose, period) in df_motorized_cat.columns:\n                    value = df_motorized_cat.loc[df_var_name, (purpose, period)]\n                    output_var_name = get_var_name(\"calib_share_transit\", veh_cat)\n                    comment_prefix = \";\" if comment else \" \"\n                    if not comment:\n                        f.write(\"  ;transit shares \\t\\t\\n\")\n                    f.write(f\"{comment_prefix} {output_var_name:&lt;30}\\t=\\t{value:.4f}\\n\")\n                    if not comment:\n                        f.write(\" \\t\\t\\n\")\n\n        f.write(\"\\t\\t\\n\")\n\n        # =====================================================================\n        # SECTION 6: Transit by Service Type (within transit)\n        # Now includes expanded CRT subclasses (CRT00, CRT01, etc.)\n        # =====================================================================\n        f.write(\";relative within transit\\t\\t\\n\")\n\n        # Service types including expanded CRT\n        service_types = ['COR', 'BRT', 'LRT', 'EXP'] + [f'CRT{i:02d}' for i in range(9)]\n\n        for service in service_types:\n            for veh_cat in ['0veh', '1veh', '2veh', '3veh', 'all']:\n                include, comment = should_include(service, veh_cat)\n                if include:\n                    df_var_name = f\"calib_share_{service.lower()}_{veh_cat}\"\n                    # Check with uppercase service name as well\n                    if df_var_name not in df_transit_by_service.index:\n                        df_var_name = f\"calib_share_{service}_{veh_cat}\"\n\n                    if df_var_name in df_transit_by_service.index and (purpose, period) in df_transit_by_service.columns:\n                        value = df_transit_by_service.loc[df_var_name, (purpose, period)]\n                        output_var_name = get_var_name(f\"calib_share_{service}\", veh_cat)\n                        comment_prefix = \";\" if comment else \" \"\n                        f.write(f\"{comment_prefix} {output_var_name:&lt;30}\\t=\\t{value:.4f}\\n\")\n\n        f.write(\"  \\t\\t\\n\")\n        f.write(\"\\t\\t\\n\")\n\n        # =====================================================================\n        # SECTION 7: Walk vs Drive Access (within transit)\n        # =====================================================================\n        f.write(\";relative within transit\\t\\t\\n\")\n\n        # Walk access shares\n        for veh_cat in ['0veh', '1veh', '2veh', '3veh', 'all']:\n            include, comment = should_include('walkacc', veh_cat)\n            if include:\n                df_var_name = f\"calib_share_walkacc_{veh_cat}\"\n                if df_var_name in df_transit_by_access.index and (purpose, period) in df_transit_by_access.columns:\n                    value = df_transit_by_access.loc[df_var_name, (purpose, period)]\n                    output_var_name = get_var_name(\"calib_share_walkacc\", veh_cat)\n                    comment_prefix = \";\" if comment else \" \"\n                    if not comment:\n                        f.write(\"  ;walk-to-transit shares \\t\\t\\n\")\n                    f.write(f\"{comment_prefix} {output_var_name:&lt;30}\\t=\\t{value:.4f}\\n\")\n                    if not comment:\n                        f.write(\"\\t\\t\\n\")\n\n        # Drive access shares\n        for veh_cat in ['0veh', '1veh', '2veh', '3veh', 'all']:\n            include, comment = should_include('driveacc', veh_cat)\n            if include:\n                df_var_name = f\"calib_share_driveacc_{veh_cat}\"\n                if df_var_name in df_transit_by_access.index and (purpose, period) in df_transit_by_access.columns:\n                    value = df_transit_by_access.loc[df_var_name, (purpose, period)]\n                    output_var_name = get_var_name(\"calib_share_driveacc\", veh_cat)\n                    comment_prefix = \";\" if comment else \" \"\n                    if not comment:\n                        f.write(\"  ;drive-to-transit shares \\t\\t\\n\")\n                    f.write(f\"{comment_prefix} {output_var_name:&lt;30}\\t=\\t{value:.4f}\\n\")\n\n        f.write(\"\\t\\t\\n\")\n        f.write(\"\\t\\t\\n\")\n\n        # =====================================================================\n        # SECTION 8: Walk-to-Transit by Service (within walkacc)\n        # Now includes expanded CRT subclasses (wCRT00, wCRT01, etc.)\n        # =====================================================================\n        f.write(\";relative within walkacc\\t\\t\\n\")\n\n        # Walk service types including expanded CRT\n        walk_services = ['wCOR', 'wBRT', 'wLRT', 'wEXP'] + [f'wCRT{i:02d}' for i in range(9)]\n\n        for service in walk_services:\n            for veh_cat in ['0veh', '1veh', '2veh', '3veh', 'all']:\n                include, comment = should_include(service, veh_cat)\n                if include:\n                    df_var_name = f\"calib_share_{service.lower()}_{veh_cat}\"\n                    # Check with correct case\n                    if df_var_name not in df_walk_to_transit.index:\n                        df_var_name = f\"calib_share_{service}_{veh_cat}\"\n\n                    if df_var_name in df_walk_to_transit.index and (purpose, period) in df_walk_to_transit.columns:\n                        value = df_walk_to_transit.loc[df_var_name, (purpose, period)]\n                        output_var_name = get_var_name(f\"calib_share_{service}\", veh_cat)\n                        comment_prefix = \";\" if comment else \" \"\n                        f.write(f\"{comment_prefix} {output_var_name:&lt;30}\\t=\\t{value:.4f}\\n\")\n\n        f.write(\"\\t\\t\\n\")\n        f.write(\"\\t\\t\\n\")\n\n        # =====================================================================\n        # SECTION 9: Drive-to-Transit by Service (within driveacc)\n        # Now includes expanded CRT subclasses (dCRT00, dCRT01, etc.)\n        # =====================================================================\n        f.write(\";relative within driveacc\\t\\t\\n\")\n\n        # Drive service types including expanded CRT\n        drive_services = ['dCOR', 'dBRT', 'dLRT', 'dEXP'] + [f'dCRT{i:02d}' for i in range(9)]\n\n        for service in drive_services:\n            for veh_cat in ['0veh', '1veh', '2veh', '3veh', 'all']:\n                include, comment = should_include(service, veh_cat)\n                if include:\n                    df_var_name = f\"calib_share_{service.lower()}_{veh_cat}\"\n                    # Check with correct case\n                    if df_var_name not in df_drive_to_transit.index:\n                        df_var_name = f\"calib_share_{service}_{veh_cat}\"\n\n                    if df_var_name in df_drive_to_transit.index and (purpose, period) in df_drive_to_transit.columns:\n                        value = df_drive_to_transit.loc[df_var_name, (purpose, period)]\n                        output_var_name = get_var_name(f\"calib_share_{service}\", veh_cat)\n                        comment_prefix = \";\" if comment else \" \"\n                        f.write(f\"{comment_prefix} {output_var_name:&lt;30}\\t=\\t{value:.4f}\\n\")\n\n    print(f\"  ✅ Written to {filepath}\")"
  },
  {
    "objectID": "2-mode-choice-targets.html#export-all-files",
    "href": "2-mode-choice-targets.html#export-all-files",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "9.3 Export All Files",
    "text": "9.3 Export All Files\n\n\nShow the code\n# Define purposes and their configuration\nexport_config = {\n    'HBW':   {'periods': ['PK', 'OK']},\n    'HBO':   {'periods': ['PK', 'OK']},\n    'NHB':   {'periods': ['PK', 'OK']},\n    'HBC':   {'periods': ['Daily']},\n    'HBSch': {'periods': ['Daily']}\n}\n\n# Export all files using global configuration\nfor purpose, config in export_config.items():\n    for period in config['periods']:\n        export_calib_file(\n            purpose,\n            period,\n            export_mode=EXPORT_MODE,\n            remove_all_suffix=REMOVE_ALL_SUFFIX\n        )\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"✅ EXPORT COMPLETE!\")\nprint(\"=\" * 80)\nprint(f\"Export mode: {EXPORT_MODE}\")\nprint(f\"Remove _all suffix: {REMOVE_ALL_SUFFIX}\")\n\n\n\nCreating calib_target_hbw_pk.txt... (Mode: only_needed)\n  ✅ Written to _output\\calib_target_hbw_pk.txt\n\nCreating calib_target_hbw_ok.txt... (Mode: only_needed)\n  ✅ Written to _output\\calib_target_hbw_ok.txt\n\nCreating calib_target_hbo_pk.txt... (Mode: only_needed)\n  ✅ Written to _output\\calib_target_hbo_pk.txt\n\nCreating calib_target_hbo_ok.txt... (Mode: only_needed)\n  ✅ Written to _output\\calib_target_hbo_ok.txt\n\nCreating calib_target_nhb_pk.txt... (Mode: only_needed)\n  ✅ Written to _output\\calib_target_nhb_pk.txt\n\nCreating calib_target_nhb_ok.txt... (Mode: only_needed)\n  ✅ Written to _output\\calib_target_nhb_ok.txt\n\nCreating calib_target_hbc_pk.txt... (Mode: only_needed)\n  ✅ Written to _output\\calib_target_hbc_pk.txt\n\nCreating calib_target_hbsch_pk.txt... (Mode: only_needed)\n  ✅ Written to _output\\calib_target_hbsch_pk.txt\n\n================================================================================\n✅ EXPORT COMPLETE!\n================================================================================\nExport mode: only_needed\nRemove _all suffix: True"
  }
]