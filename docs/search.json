[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "",
    "text": "This section prepares the Python environment with all necessary libraries and configurations. We’ll import data manipulation libraries (pandas, numpy), and visualization tools (matplotlib, seaborn).\n\n\n!conda install -c conda-forge numpy pandas matplotlib seaborn requests pathlib BigQuery\n\n\n\n\n\nShow the code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport importlib.util\nfrom pathlib import Path\nimport sys\nimport os\nimport requests\nimport io\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\n\nTrue\n\n\n\n\n\n\n\n\n\n\nShow the code\ndef fetch_github(\n    url: str,\n    mode: str = \"private\",\n    token_env_var: str = \"GITHUB_TOKEN\"\n) -&gt; requests.Response:\n    \"\"\"\n    Fetch content from GitHub repositories.\n\n    Args:\n        url: GitHub raw URL (e.g., https://raw.githubusercontent.com/...)\n        mode: \"public\" for public repos, \"private\" for private repos requiring authentication\n        token_env_var: Name of environment variable containing GitHub token (default: GITHUB_TOKEN)\n\n    Returns:\n        requests.Response object\n\n    Raises:\n        ValueError: If token is missing for private mode or invalid mode\n        requests.HTTPError: If request fails\n    \"\"\"\n    # Validate mode\n    if mode not in [\"public\", \"private\"]:\n        raise ValueError(f\"mode must be 'public' or 'private', got '{mode}'\")\n\n    if mode == \"public\":\n        response = requests.get(url, timeout=30)\n    else:\n        token = os.getenv(token_env_var)\n        if not token:\n            raise ValueError(\n                f\"GitHub token not found in environment variable '{token_env_var}'. \"\n                f\"Check your .env file has: {token_env_var}=your_token_here\"\n            )\n\n        headers = {\n            'Authorization': f'token {token}',\n            'Accept': 'application/vnd.github.v3.raw'\n        }\n        response = requests.get(url, headers=headers, timeout=30)\n\n    response.raise_for_status()\n    return response\n\n\n\n\nShow the code\ndef calculate_categorical_shares(\n    df,\n    category_col,\n    weight_col='LINKED_WEIGHT',\n    group_cols=['Veh_Cat3p', 'Purp5_text', 'PK_OK'],\n    filter_condition=None,\n    category_mapping=None,\n    prefix='share',\n    exclude_daily_purposes=['HBW', 'HBO', 'NHB']  # Only HBC and HBSch get daily\n):\n    \"\"\"\n    Calculate weighted shares of categories within groups, ensuring they sum to 1.\n\n    Parameters:\n    -----------\n    df : DataFrame\n        Input dataframe\n    category_col : str\n        Column containing the categories to calculate shares for\n    weight_col : str\n        Column containing weights for each observation\n    group_cols : list\n        Columns to group by (e.g., vehicle ownership, purpose, peak/off-peak)\n    filter_condition : str or None\n        Optional pandas query string to filter data before calculation\n    category_mapping : dict or None\n        Optional mapping to rename/recode categories\n    prefix : str\n        Prefix for output column names\n    exclude_daily_purposes : list\n        Purposes that should NOT get daily aggregates (only peak/off-peak)\n\n    Returns:\n    --------\n    DataFrame with shares in wide format\n    \"\"\"\n\n    # Start with a copy\n    df_work = df.copy()\n\n    # Apply filter if provided\n    if filter_condition:\n        df_work = df_work.query(filter_condition)\n\n    # Apply category mapping if provided\n    if category_mapping:\n        df_work[category_col] = df_work[category_col].map(category_mapping)\n        # Remove unmapped values\n        df_work = df_work.dropna(subset=[category_col])\n\n    # Remove any NaN values in key columns\n    df_work = df_work.dropna(subset=[category_col, weight_col] + group_cols)\n\n    # Initialize results dictionary\n    results = {}\n\n    # Calculate weighted shares for each group combination\n    for veh in sorted(df_work['Veh_Cat3p'].unique()):\n        for purpose in sorted(df_work['Purp5_text'].unique()):\n            for peak in sorted(df_work['PK_OK'].unique()):\n                subset = df_work.query(\n                    f\"Veh_Cat3p == '{veh}' & Purp5_text == '{purpose}' & PK_OK == '{peak}'\"\n                )\n\n                if len(subset) &gt; 0:\n                    # Calculate weighted counts by category\n                    weighted_counts = subset.groupby(category_col)[weight_col].sum()\n                    total_weight = subset[weight_col].sum()\n\n                    for cat, weighted_count in weighted_counts.items():\n                        var_name = f\"{prefix}_{cat}_{veh}veh\"\n                        if var_name not in results:\n                            results[var_name] = {}\n                        results[var_name][(purpose, peak)] = weighted_count / total_weight\n\n    # Calculate \"_all\" aggregates (across all vehicle categories)\n    for purpose in sorted(df_work['Purp5_text'].unique()):\n        for peak in sorted(df_work['PK_OK'].unique()):\n            subset = df_work.query(f\"Purp5_text == '{purpose}' & PK_OK == '{peak}'\")\n\n            if len(subset) &gt; 0:\n                weighted_counts = subset.groupby(category_col)[weight_col].sum()\n                total_weight = subset[weight_col].sum()\n\n                for cat, weighted_count in weighted_counts.items():\n                    var_name = f\"{prefix}_{cat}_all\"\n                    if var_name not in results:\n                        results[var_name] = {}\n                    results[var_name][(purpose, peak)] = weighted_count / total_weight\n\n    # Calculate daily aggregates for HBC and HBSch only\n    daily_purposes = [p for p in df_work['Purp5_text'].unique()\n                     if p not in exclude_daily_purposes]\n\n    for purpose in daily_purposes:\n        subset = df_work.query(f\"Purp5_text == '{purpose}'\")\n\n        if len(subset) &gt; 0:\n            # For each vehicle category\n            for veh in sorted(df_work['Veh_Cat3p'].unique()):\n                veh_subset = subset.query(f\"Veh_Cat3p == '{veh}'\")\n                if len(veh_subset) &gt; 0:\n                    weighted_counts = veh_subset.groupby(category_col)[weight_col].sum()\n                    total_weight = veh_subset[weight_col].sum()\n\n                    for cat, weighted_count in weighted_counts.items():\n                        var_name = f\"{prefix}_{cat}_{veh}veh\"\n                        if var_name not in results:\n                            results[var_name] = {}\n                        results[var_name][(purpose, 'Daily')] = weighted_count / total_weight\n\n            # For \"all\" vehicle categories\n            weighted_counts = subset.groupby(category_col)[weight_col].sum()\n            total_weight = subset[weight_col].sum()\n\n            for cat, weighted_count in weighted_counts.items():\n                var_name = f\"{prefix}_{cat}_all\"\n                if var_name not in results:\n                    results[var_name] = {}\n                results[var_name][(purpose, 'Daily')] = weighted_count / total_weight\n\n    # Convert to DataFrame\n    df_result = pd.DataFrame(results).T\n    df_result.columns = pd.MultiIndex.from_tuples(df_result.columns)\n    df_result = df_result.fillna(0)\n\n    # Sort index\n    df_result = df_result.sort_index()\n\n    return df_result\n\n\n\n\nShow the code\n# Verify categorical shares sum to 1.0000\ndef verify_shares(df_shares, tolerance=0.01):\n    \"\"\"\n    Verify that shares sum to approximately 1 within each group.\n    \"\"\"\n    print(\"=== Verifying Share Sums ===\\n\")\n\n    # Check sums for each column\n    all_good = True\n    issues = []\n\n    for col in df_shares.columns:\n        col_sum = df_shares[col].sum()\n\n        if abs(col_sum - 1.0) &gt; tolerance:\n            all_good = False\n            issues.append((col, col_sum))\n\n    if all_good:\n        print(\"✅ All shares sum to 1.0 (within tolerance)\")\n    else:\n        print(f\"⚠️  Warning: {len(issues)} columns don't sum to 1.0:\\n\")\n        for col, sum_val in issues[:10]:  # Show first 10\n            print(f\"   {col}: {sum_val:.4f}\")\n        if len(issues) &gt; 10:\n            print(f\"   ... and {len(issues) - 10} more\")\n\n    print(f\"\\nTotal columns: {len(df_shares.columns)}\")\n    print(f\"Columns with correct sums: {len(df_shares.columns) - len(issues)}\")\n\n    return df_shares.sum(axis=0)\n\n\n\n\n\n\n\nShow the code\n# # Import global TDM functions from remote 'Resource' repo\n\n# Fetch and import BigQuery module\nresponse = fetch_github(\n    'https://raw.githubusercontent.com/WFRCAnalytics/Resources/refs/heads/master/2-Python/global-functions/BigQuery.py',\n    mode=\"public\"\n)\n\nBigQuery = importlib.util.module_from_spec(importlib.util.spec_from_loader('BigQuery', loader=None))\nexec(response.text, BigQuery.__dict__)\n\n# Initiatlize BigQuery Client\nclient = BigQuery.getBigQueryClient_Confidential2023UtahHTS()\n\n\npukar.bhandari\nC:/Users/Pukar.Bhandari/.private/confidential-2023-utah-hts-5fd7ddd219a7.json"
  },
  {
    "objectID": "index.html#install-libraries",
    "href": "index.html#install-libraries",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "",
    "text": "!conda install -c conda-forge numpy pandas matplotlib seaborn requests pathlib BigQuery"
  },
  {
    "objectID": "index.html#import-libraries",
    "href": "index.html#import-libraries",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "",
    "text": "Show the code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport importlib.util\nfrom pathlib import Path\nimport sys\nimport os\nimport requests\nimport io\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\n\nTrue"
  },
  {
    "objectID": "index.html#helper-functions",
    "href": "index.html#helper-functions",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "",
    "text": "Show the code\ndef fetch_github(\n    url: str,\n    mode: str = \"private\",\n    token_env_var: str = \"GITHUB_TOKEN\"\n) -&gt; requests.Response:\n    \"\"\"\n    Fetch content from GitHub repositories.\n\n    Args:\n        url: GitHub raw URL (e.g., https://raw.githubusercontent.com/...)\n        mode: \"public\" for public repos, \"private\" for private repos requiring authentication\n        token_env_var: Name of environment variable containing GitHub token (default: GITHUB_TOKEN)\n\n    Returns:\n        requests.Response object\n\n    Raises:\n        ValueError: If token is missing for private mode or invalid mode\n        requests.HTTPError: If request fails\n    \"\"\"\n    # Validate mode\n    if mode not in [\"public\", \"private\"]:\n        raise ValueError(f\"mode must be 'public' or 'private', got '{mode}'\")\n\n    if mode == \"public\":\n        response = requests.get(url, timeout=30)\n    else:\n        token = os.getenv(token_env_var)\n        if not token:\n            raise ValueError(\n                f\"GitHub token not found in environment variable '{token_env_var}'. \"\n                f\"Check your .env file has: {token_env_var}=your_token_here\"\n            )\n\n        headers = {\n            'Authorization': f'token {token}',\n            'Accept': 'application/vnd.github.v3.raw'\n        }\n        response = requests.get(url, headers=headers, timeout=30)\n\n    response.raise_for_status()\n    return response\n\n\n\n\nShow the code\ndef calculate_categorical_shares(\n    df,\n    category_col,\n    weight_col='LINKED_WEIGHT',\n    group_cols=['Veh_Cat3p', 'Purp5_text', 'PK_OK'],\n    filter_condition=None,\n    category_mapping=None,\n    prefix='share',\n    exclude_daily_purposes=['HBW', 'HBO', 'NHB']  # Only HBC and HBSch get daily\n):\n    \"\"\"\n    Calculate weighted shares of categories within groups, ensuring they sum to 1.\n\n    Parameters:\n    -----------\n    df : DataFrame\n        Input dataframe\n    category_col : str\n        Column containing the categories to calculate shares for\n    weight_col : str\n        Column containing weights for each observation\n    group_cols : list\n        Columns to group by (e.g., vehicle ownership, purpose, peak/off-peak)\n    filter_condition : str or None\n        Optional pandas query string to filter data before calculation\n    category_mapping : dict or None\n        Optional mapping to rename/recode categories\n    prefix : str\n        Prefix for output column names\n    exclude_daily_purposes : list\n        Purposes that should NOT get daily aggregates (only peak/off-peak)\n\n    Returns:\n    --------\n    DataFrame with shares in wide format\n    \"\"\"\n\n    # Start with a copy\n    df_work = df.copy()\n\n    # Apply filter if provided\n    if filter_condition:\n        df_work = df_work.query(filter_condition)\n\n    # Apply category mapping if provided\n    if category_mapping:\n        df_work[category_col] = df_work[category_col].map(category_mapping)\n        # Remove unmapped values\n        df_work = df_work.dropna(subset=[category_col])\n\n    # Remove any NaN values in key columns\n    df_work = df_work.dropna(subset=[category_col, weight_col] + group_cols)\n\n    # Initialize results dictionary\n    results = {}\n\n    # Calculate weighted shares for each group combination\n    for veh in sorted(df_work['Veh_Cat3p'].unique()):\n        for purpose in sorted(df_work['Purp5_text'].unique()):\n            for peak in sorted(df_work['PK_OK'].unique()):\n                subset = df_work.query(\n                    f\"Veh_Cat3p == '{veh}' & Purp5_text == '{purpose}' & PK_OK == '{peak}'\"\n                )\n\n                if len(subset) &gt; 0:\n                    # Calculate weighted counts by category\n                    weighted_counts = subset.groupby(category_col)[weight_col].sum()\n                    total_weight = subset[weight_col].sum()\n\n                    for cat, weighted_count in weighted_counts.items():\n                        var_name = f\"{prefix}_{cat}_{veh}veh\"\n                        if var_name not in results:\n                            results[var_name] = {}\n                        results[var_name][(purpose, peak)] = weighted_count / total_weight\n\n    # Calculate \"_all\" aggregates (across all vehicle categories)\n    for purpose in sorted(df_work['Purp5_text'].unique()):\n        for peak in sorted(df_work['PK_OK'].unique()):\n            subset = df_work.query(f\"Purp5_text == '{purpose}' & PK_OK == '{peak}'\")\n\n            if len(subset) &gt; 0:\n                weighted_counts = subset.groupby(category_col)[weight_col].sum()\n                total_weight = subset[weight_col].sum()\n\n                for cat, weighted_count in weighted_counts.items():\n                    var_name = f\"{prefix}_{cat}_all\"\n                    if var_name not in results:\n                        results[var_name] = {}\n                    results[var_name][(purpose, peak)] = weighted_count / total_weight\n\n    # Calculate daily aggregates for HBC and HBSch only\n    daily_purposes = [p for p in df_work['Purp5_text'].unique()\n                     if p not in exclude_daily_purposes]\n\n    for purpose in daily_purposes:\n        subset = df_work.query(f\"Purp5_text == '{purpose}'\")\n\n        if len(subset) &gt; 0:\n            # For each vehicle category\n            for veh in sorted(df_work['Veh_Cat3p'].unique()):\n                veh_subset = subset.query(f\"Veh_Cat3p == '{veh}'\")\n                if len(veh_subset) &gt; 0:\n                    weighted_counts = veh_subset.groupby(category_col)[weight_col].sum()\n                    total_weight = veh_subset[weight_col].sum()\n\n                    for cat, weighted_count in weighted_counts.items():\n                        var_name = f\"{prefix}_{cat}_{veh}veh\"\n                        if var_name not in results:\n                            results[var_name] = {}\n                        results[var_name][(purpose, 'Daily')] = weighted_count / total_weight\n\n            # For \"all\" vehicle categories\n            weighted_counts = subset.groupby(category_col)[weight_col].sum()\n            total_weight = subset[weight_col].sum()\n\n            for cat, weighted_count in weighted_counts.items():\n                var_name = f\"{prefix}_{cat}_all\"\n                if var_name not in results:\n                    results[var_name] = {}\n                results[var_name][(purpose, 'Daily')] = weighted_count / total_weight\n\n    # Convert to DataFrame\n    df_result = pd.DataFrame(results).T\n    df_result.columns = pd.MultiIndex.from_tuples(df_result.columns)\n    df_result = df_result.fillna(0)\n\n    # Sort index\n    df_result = df_result.sort_index()\n\n    return df_result\n\n\n\n\nShow the code\n# Verify categorical shares sum to 1.0000\ndef verify_shares(df_shares, tolerance=0.01):\n    \"\"\"\n    Verify that shares sum to approximately 1 within each group.\n    \"\"\"\n    print(\"=== Verifying Share Sums ===\\n\")\n\n    # Check sums for each column\n    all_good = True\n    issues = []\n\n    for col in df_shares.columns:\n        col_sum = df_shares[col].sum()\n\n        if abs(col_sum - 1.0) &gt; tolerance:\n            all_good = False\n            issues.append((col, col_sum))\n\n    if all_good:\n        print(\"✅ All shares sum to 1.0 (within tolerance)\")\n    else:\n        print(f\"⚠️  Warning: {len(issues)} columns don't sum to 1.0:\\n\")\n        for col, sum_val in issues[:10]:  # Show first 10\n            print(f\"   {col}: {sum_val:.4f}\")\n        if len(issues) &gt; 10:\n            print(f\"   ... and {len(issues) - 10} more\")\n\n    print(f\"\\nTotal columns: {len(df_shares.columns)}\")\n    print(f\"Columns with correct sums: {len(df_shares.columns) - len(issues)}\")\n\n    return df_shares.sum(axis=0)"
  },
  {
    "objectID": "index.html#setup-bigquery",
    "href": "index.html#setup-bigquery",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "",
    "text": "Show the code\n# # Import global TDM functions from remote 'Resource' repo\n\n# Fetch and import BigQuery module\nresponse = fetch_github(\n    'https://raw.githubusercontent.com/WFRCAnalytics/Resources/refs/heads/master/2-Python/global-functions/BigQuery.py',\n    mode=\"public\"\n)\n\nBigQuery = importlib.util.module_from_spec(importlib.util.spec_from_loader('BigQuery', loader=None))\nexec(response.text, BigQuery.__dict__)\n\n# Initiatlize BigQuery Client\nclient = BigQuery.getBigQueryClient_Confidential2023UtahHTS()\n\n\npukar.bhandari\nC:/Users/Pukar.Bhandari/.private/confidential-2023-utah-hts-5fd7ddd219a7.json"
  },
  {
    "objectID": "index.html#uta-on-board-survey",
    "href": "index.html#uta-on-board-survey",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "2.1 UTA On-Board Survey",
    "text": "2.1 UTA On-Board Survey\n\n\nShow the code\n# Read Linked UTA On-Board Survey Data directly from GitHub repo\nresponse = fetch_github(\n    \"https://raw.githubusercontent.com/WFRCAnalytics/DATA-OBS-Prep-For-TDM/refs/heads/main/_output/UTA_OBS_2024_Linked.csv\",\n    mode = \"private\"\n)\n\ndf_obs_linked = pd.read_csv(io.StringIO(response.text))\ndf_obs_linked\n\n\nC:\\Users\\Pukar.Bhandari\\AppData\\Local\\Temp\\ipykernel_19912\\4156658631.py:7: DtypeWarning: Columns (8,24,32,37,54,132,137,174) have mixed types. Specify dtype option on import or set low_memory=False.\n  df_obs_linked = pd.read_csv(io.StringIO(response.text))\n\n\n\n\n\n\n\n\n\nID\nDATE_COMPLETED\nDATE_TYPE\nROUTE_DIRECTION_Code\nROUTE_DIRECTION\nROUTE_DIRECTION_Other\nHOME_ADDRESS_CITY\nHOME_ADDRESS_STATE\nHOME_ADDRESS_ZIP\nHOME_ADDRESS_LAT\n...\np_Stop_lat\np_Stop_lon\na_Stop_lat\na_Stop_lon\np_Stop_N\na_Stop_N\naccess_dist\naccess_time\negress_dist\negress_time\n\n\n\n\n0\n5948\n2/27/2024\nWeekday\nUTA_1_2_00\n2 200 SOUTH - TO U HOSPITAL\nNaN\nClearfield\nUT\n84015\n41.119565\n...\n41.118979\n-112.025922\n40.769221\n-111.898663\n27702\n25493\n0.37\n0.148000\n0.68\n0.27200\n\n\n1\n6067\n2/27/2024\nWeekday\nUTA_1_47_00\n47 4700 SOUTH - TO W VALLEY CTL\nNaN\nWest Valley City\nUT\n84119\n40.689590\n...\n40.689384\n-111.967476\n40.660941\n-111.899443\n23195\n23685\n0.56\n0.224000\n0.24\n0.09600\n\n\n2\n6069\n3/16/2024\nSaturday\nUTA_1_750_01\nFRONTRUNNER 750 - SOUTHBOUND\nNaN\nOgden\nUT\n84401\n41.186697\n...\n41.188757\n-112.039378\n40.784356\n-111.982117\n10042\n15093\n1.36\n0.050671\n0.20\n0.08000\n\n\n3\n6073\n2/28/2024\nWeekday\nUTA_1_750_01\nFRONTRUNNER 750 - SOUTHBOUND\nNaN\nOgden\nUT\n84401\n41.236749\n...\n41.188757\n-112.039378\n40.659758\n-111.896432\n10042\n10016\n4.17\n0.136991\n1.66\n0.07316\n\n\n4\n6077\n2/28/2024\nWeekday\nUTA_1_47_00\n47 4700 SOUTH - TO W VALLEY CTL\nNaN\nWest Valley City\nUT\n84119\n40.669687\n...\n40.667711\n-111.961123\n40.681907\n-112.024041\n22922\n22421\n0.25\n0.100000\n0.49\n0.19600\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n13790\n30339\n4/26/2024\nWeekday\nUTA_1_704_01\nTRAX GREEN LINE 704 - TO AIRPORT\nNaN\nSalt Lake City\nUT\n84116\n40.772420\n...\n40.771536\n-111.945736\n40.771502\n-111.914486\n15101\n15122\n0.31\n0.124000\n0.17\n0.06800\n\n\n13791\n30340\n4/26/2024\nWeekday\nUTA_1_704_00\nTRAX GREEN LINE 704 - TO WEST VALLEY\nNaN\nSalt Lake City\nUT\n84116\n40.771361\n...\n40.771505\n-111.915361\n40.750058\n-111.896818\n25266\n15109\n0.30\n0.120000\n1.21\n0.48400\n\n\n13792\n30341\n4/26/2024\nWeekday\nUTA_1_704_00\nTRAX GREEN LINE 704 - TO WEST VALLEY\nNaN\nSalt Lake City\nUT\n84116\n40.773232\n...\n40.771505\n-111.915361\n40.725345\n-111.877499\n25266\n24968\n0.19\n0.076000\n0.86\n0.34400\n\n\n13793\n1000002\n2/29/2024\nWeekday\nUTA_1_701_00\nTRAX BLUE LINE 701 - TO DRAPER\nNaN\nSalt Lake City\nUT\n84111\n40.751607\n...\n40.755113\n-111.891095\n40.674859\n-111.943134\n15123\n23342\n0.33\n0.132000\n0.29\n0.11600\n\n\n13794\n1000004\n2/29/2024\nWeekday\nUTA_1_701_00\nTRAX BLUE LINE 701 - TO DRAPER\nNaN\nSalt Lake City\nUT\n84109\n40.722068\n...\n40.725878\n-111.830935\n40.525496\n-111.858805\n25663\n15041\n0.79\n0.316000\n0.19\n0.07600\n\n\n\n\n13795 rows × 301 columns"
  },
  {
    "objectID": "index.html#household-travel-survey",
    "href": "index.html#household-travel-survey",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "2.2 Household Travel Survey",
    "text": "2.2 Household Travel Survey\n\n\nShow the code\n# Load Linked Trips data from 2023 Household Travel Survey\ndf_hts_linked = client.query(\n    \"SELECT * FROM \" + 'wfrc-modeling-data.prd_tdm_hts_2023.trip_linked'\n).to_dataframe()\n\ndf_hts_linked\n\n\n\n\n\n\n\n\n\nunique_id\nlinked_trip_id\nhh_id\nperson_id\nday_id\nday_weight\nperson_num\nday_num\nparticipation_group\ndiary_platform\n...\ndCO_TAZID_USTMv3\npCO_TAZID_USTMv3\naCO_TAZID_USTMv3\noCO_TAZID_USTMv4\ndCO_TAZID_USTMv4\npCO_TAZID_USTMv4\naCO_TAZID_USTMv4\npSUBAREAID\naSUBAREAID\ntrip_weight\n\n\n\n\n0\n5facab93-0b23-480f-b69a-70a9d611427c\n2302328301010100\n23023283\n2302328301\n230232830101\n785.318580\n1\n1\n1\nbrowser\n...\n530200.0\n530278.0\n530200.0\n530297.0\n530323.0\n530297.0\n530323.0\n3.0\n3.0\n785.318580\n\n\n1\n27d90255-4acc-42fb-82a7-62e249f739b8\n2303463801020900\n23034638\n2303463801\n230346380102\n0.000000\n1\n2\n9\nrmove\n...\n490777.0\n490777.0\n490723.0\n490750.0\n490757.0\n490757.0\n490750.0\n1.0\n1.0\n0.000000\n\n\n2\n3808484f-788b-48e5-a5e1-dba3eec264fa\n2304417802020100\n23044178\n2304417802\n230441780202\n79.325700\n2\n2\n9\nrmove\n...\n350673.0\n350673.0\n350197.0\n350197.0\n350673.0\n350673.0\n350197.0\n1.0\n1.0\n79.325700\n\n\n3\n76bc4c44-e8f4-47ff-9c3d-36959467a428\n2305261901070200\n23052619\n2305261901\n230526190107\n508.569360\n1\n7\n9\nrmove\n...\n490774.0\n490774.0\n490723.0\n490750.0\n490754.0\n490754.0\n490750.0\n1.0\n1.0\n508.569360\n\n\n4\n4a68e37e-5f7e-4a2b-9448-08ec36f3d318\n2307491602030100\n23074916\n2307491602\n230749160203\n0.000000\n2\n3\n9\nrmove\n...\n490497.0\n490497.0\n490632.0\n490715.0\n490455.0\n490455.0\n490715.0\n1.0\n1.0\n0.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n13112\n1019ad4d-6732-45a1-a991-e5a3be830883\n2348141901030100\n23481419\n2348141901\n234814190103\n21.199747\n1\n3\n9\nrmove\n...\n350187.0\n350187.0\n350189.0\n350189.0\n350187.0\n350187.0\n350189.0\n1.0\n1.0\n21.199747\n\n\n13113\nadfacfa2-2f88-462d-b7de-17066fa37ece\n2348219501010100\n23482195\n2348219501\n234821950101\n68.644470\n1\n1\n9\nrmove\n...\n350562.0\n350562.0\n350562.0\n350562.0\n350562.0\n350562.0\n350562.0\n1.0\n1.0\n68.644470\n\n\n13114\n5c5d7f99-8bb5-49bb-8eb0-2447e9f2abfc\n2348502802010200\n23485028\n2348502802\n234850280201\n0.000000\n2\n1\n9\nrmove\n...\n351014.0\n351203.0\n351014.0\n351203.0\n351014.0\n351203.0\n351014.0\n1.0\n1.0\n0.000000\n\n\n13115\ne136d377-68a2-4086-8b89-3a030fdc1ab7\n2348561501010100\n23485615\n2348561501\n234856150101\n32.566605\n1\n1\n1\nbrowser\n...\n350253.0\n350368.0\n350253.0\n350368.0\n350253.0\n350368.0\n350253.0\n1.0\n1.0\n39.079926\n\n\n13116\ne1ba708e-8925-4357-9720-5dec0b126809\n2348838901050200\n23488389\n2348838901\n234883890105\n22.466323\n1\n5\n9\nrmove\n...\n110049.0\n110049.0\n110053.0\n110053.0\n110049.0\n110049.0\n110053.0\n1.0\n1.0\n22.466323\n\n\n\n\n232451 rows × 109 columns"
  },
  {
    "objectID": "index.html#on-board-survey",
    "href": "index.html#on-board-survey",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "4.1 On-Board Survey",
    "text": "4.1 On-Board Survey\n\n\nShow the code\ndf_obs_linked[[\n    \"Linked_Mode_txt\", # Linked Mode: MT = MicroTransit (dont need), LCL = local bus, COR = Core Bus, EXP = Express Bus, LRT = Light Rail Transit (TRAX), CRT = Commuter Rail Transit (FrontRunner), BRT = Bus Rapid Transit (OVX, UVX)\n    \"Purp5_text\", # Trip Purpose: HBW = Home-based Work, HBO = Home-based Other, HBC = Home-based College, NHB = Non-home based, HBSch = Home-based School\n    \"PK_OK\", # Peak vs OffPeak\n    \"Veh_Cat3p\", # Vehicle Ownership: 0 = 0 Vehicle Household, 1 = 1 Vehicle Household, 2 = 2 Vehicle Household, 3 = 3+ Vehicle Household\n    \"Mode_Fin\", # Current Mode: 1 = MT = MicroTransit (Dont need),  4 = LCL = local bus, 5 = COR = Core Bus, 6 = EXP = Express Bus, 7 = LRT = Light Rail Transit (TRAX), 8 = CRT = Commuter Rail Transit, 9 = BRT = Bus Rapid Transit (OVX, UVX)\n    \"Ac_Mode_Model\", # Access Mode to Transit: Walk (Walk to Transit), Drive (Drive to Transit)\n]]\n\n\n\n\n\n\n\n\n\nLinked_Mode_txt\nPurp5_text\nPK_OK\nVeh_Cat3p\nMode_Fin\nAc_Mode_Model\n\n\n\n\n0\nLCL\nHBW\nPK\n0\n4\nWalk\n\n\n1\nLCL\nHBC\nOK\n0\n4\nWalk\n\n\n2\nLRT\nHBO\nOK\n1\n8\nDrive\n\n\n3\nCRT\nHBW\nOK\n2\n8\nDrive\n\n\n4\nLCL\nHBW\nOK\n0\n4\nWalk\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n13790\nLRT\nHBO\nOK\n1\n7\nWalk\n\n\n13791\nLRT\nNHB\nOK\n0\n7\nWalk\n\n\n13792\nLCL\nNHB\nOK\n1\n7\nWalk\n\n\n13793\nLCL\nHBC\nPK\n3\n7\nWalk\n\n\n13794\nLCL\nHBW\nOK\n1\n7\nWalk\n\n\n\n\n13795 rows × 6 columns"
  },
  {
    "objectID": "index.html#non-motorized-trips-walk-vs-bike",
    "href": "index.html#non-motorized-trips-walk-vs-bike",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "5.1 Non-motorized Trips: Walk vs Bike",
    "text": "5.1 Non-motorized Trips: Walk vs Bike"
  },
  {
    "objectID": "index.html#motorized-trips-auto-vs-transit",
    "href": "index.html#motorized-trips-auto-vs-transit",
    "title": "Calculating the Mode Choice Calibration Parameters",
    "section": "5.2 Motorized Trips: Auto vs Transit",
    "text": "5.2 Motorized Trips: Auto vs Transit\n\nAuto Trips: Drive Alone vs Shared Rides\n\nShared Rides: 2 People vs 3+ People\n\n\n\nTransit Trips: By Service and By Access Mode\n\nBy Service Type\n\n\nShow the code\n# 1. Transit by Service Type (within linked transit trips)\nprint(\"\\n\" + \"=\" * 60)\nprint(\"1. TRANSIT BY SERVICE TYPE\")\nprint(\"=\" * 60)\n\nservice_mapping = {\n    'LCL': 'local',\n    'COR': 'core',\n    'EXP': 'express',\n    'LRT': 'lrt',\n    'CRT': 'crt',\n    'BRT': 'brt'\n}\n\ndf_transit_by_service = calculate_categorical_shares(\n    df_obs_linked,\n    category_col='Linked_Mode_txt',\n    weight_col='LINKED_WEIGHT',\n    group_cols=['Veh_Cat3p', 'Purp5_text', 'PK_OK'],\n    filter_condition='Linked_Mode_txt != \"MT\"',\n    category_mapping=service_mapping,\n    prefix='calib_share'\n)\n\nprint(\"\\nShape:\", df_transit_by_service.shape)\nprint(\"\\nFirst few rows:\")\nprint(df_transit_by_service.head(10))\nprint(\"\\nVerification:\")\nverify_shares(df_transit_by_service)\n\n\n\n============================================================\n1. TRANSIT BY SERVICE TYPE\n============================================================\n\n\nC:\\Users\\Pukar.Bhandari\\AppData\\Local\\Temp\\ipykernel_19912\\2056282771.py:47: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_work[category_col] = df_work[category_col].map(category_mapping)\n\n\n\nShape: (5, 12)\n\nFirst few rows:\n                              HBC                 HBO               HBSch  \\\n                               OK        PK        OK        PK        OK   \ncalib_share_brt_all      0.295180  0.235873  0.076085  0.071583  0.073582   \ncalib_share_crt_all      0.068098  0.083201  0.042798  0.039035  0.027174   \ncalib_share_express_all  0.000000  0.001163  0.000000  0.001306  0.000000   \ncalib_share_local_all    0.459143  0.448081  0.509093  0.482018  0.781475   \ncalib_share_lrt_all      0.177579  0.231682  0.372024  0.406058  0.117769   \n\n                                        HBW                 NHB            \\\n                               PK        OK        PK        OK        PK   \ncalib_share_brt_all      0.027199  0.045846  0.052426  0.158804  0.108415   \ncalib_share_crt_all      0.026417  0.074069  0.093561  0.025010  0.033162   \ncalib_share_express_all  0.000000  0.006696  0.013234  0.000000  0.000000   \ncalib_share_local_all    0.769791  0.500620  0.508396  0.445302  0.468599   \ncalib_share_lrt_all      0.176593  0.372770  0.332383  0.370883  0.389823   \n\n                              HBC     HBSch  \n                            Daily     Daily  \ncalib_share_brt_all      0.268888  0.049344  \ncalib_share_crt_all      0.074794  0.026778  \ncalib_share_express_all  0.000515  0.000000  \ncalib_share_local_all    0.454239  0.775369  \ncalib_share_lrt_all      0.201564  0.148509  \n\nVerification:\n=== Verifying Share Sums ===\n\n✅ All shares sum to 1.0 (within tolerance)\n\nTotal columns: 12\nColumns with correct sums: 12\n\n\nHBC    OK       1.0\n       PK       1.0\nHBO    OK       1.0\n       PK       1.0\nHBSch  OK       1.0\n       PK       1.0\nHBW    OK       1.0\n       PK       1.0\nNHB    OK       1.0\n       PK       1.0\nHBC    Daily    1.0\nHBSch  Daily    1.0\ndtype: float64\n\n\n\n\nBy Access Mode: Walk vs Drive to Transit\n\n\nShow the code\n# 2. Transit by Access Type (Walk vs Drive) (within linked transit trips)\nprint(\"\\n\" + \"=\" * 60)\nprint(\"2. TRANSIT BY ACCESS TYPE\")\nprint(\"=\" * 60)\n\naccess_mapping = {\n    'Walk': 'walkacc',\n    'Drive': 'driveacc'\n}\n\ndf_transit_by_access = calculate_categorical_shares(\n    df_obs_linked,\n    category_col='Ac_Mode_Model',\n    weight_col='LINKED_WEIGHT',\n    group_cols=['Veh_Cat3p', 'Purp5_text', 'PK_OK'],\n    filter_condition='Linked_Mode_txt != \"MT\"',\n    category_mapping=access_mapping,\n    prefix='calib_share'\n)\n\nprint(\"\\nShape:\", df_transit_by_access.shape)\nprint(\"\\nFirst few rows:\")\nprint(df_transit_by_access.head(10))\nprint(\"\\nVerification:\")\nverify_shares(df_transit_by_access)\n\n\n\n============================================================\n2. TRANSIT BY ACCESS TYPE\n============================================================\n\n\nC:\\Users\\Pukar.Bhandari\\AppData\\Local\\Temp\\ipykernel_19912\\2056282771.py:47: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_work[category_col] = df_work[category_col].map(category_mapping)\n\n\n\nShape: (2, 12)\n\nFirst few rows:\n                               HBC                 HBO              HBSch  \\\n                                OK        PK        OK       PK        OK   \ncalib_share_driveacc_all  0.284102  0.324053  0.120454  0.12685  0.055021   \ncalib_share_walkacc_all   0.715898  0.675947  0.879546  0.87315  0.944979   \n\n                                         HBW                 NHB            \\\n                                PK        OK        PK        OK        PK   \ncalib_share_driveacc_all  0.165856  0.188193  0.233531  0.044846  0.059741   \ncalib_share_walkacc_all   0.834144  0.811807  0.766469  0.955154  0.940259   \n\n                               HBC     HBSch  \n                             Daily     Daily  \ncalib_share_driveacc_all  0.301813  0.112939  \ncalib_share_walkacc_all   0.698187  0.887061  \n\nVerification:\n=== Verifying Share Sums ===\n\n✅ All shares sum to 1.0 (within tolerance)\n\nTotal columns: 12\nColumns with correct sums: 12\n\n\nHBC    OK       1.0\n       PK       1.0\nHBO    OK       1.0\n       PK       1.0\nHBSch  OK       1.0\n       PK       1.0\nHBW    OK       1.0\n       PK       1.0\nNHB    OK       1.0\n       PK       1.0\nHBC    Daily    1.0\nHBSch  Daily    1.0\ndtype: float64\n\n\n\nWalk to Transit\n\n\nShow the code\n# 3. Walk-to-Transit by Service Type (within walk-to-transit trips)\nprint(\"\\n\" + \"=\" * 60)\nprint(\"3. WALK-TO-TRANSIT BY SERVICE TYPE\")\nprint(\"=\" * 60)\n\nwalk_service_mapping = {\n    'LCL': 'walk-local',\n    'EXP': 'walk-express',\n    'LRT': 'walk-lrt',\n    'CRT': 'walk-crt',\n    'BRT': 'walk-brt'\n}\n\ndf_walk_to_transit = calculate_categorical_shares(\n    df_obs_linked,\n    category_col='Linked_Mode_txt',\n    weight_col='LINKED_WEIGHT',\n    group_cols=['Veh_Cat3p', 'Purp5_text', 'PK_OK'],\n    filter_condition='Ac_Mode_Model == \"Walk\" & Linked_Mode_txt != \"MT\"',\n    category_mapping=walk_service_mapping,\n    prefix='calib_share'\n)\n\n\nprint(\"\\nShape:\", df_walk_to_transit.shape)\nprint(\"\\nFirst few rows:\")\nprint(df_walk_to_transit.head(10))\nprint(\"\\nVerification:\")\nverify_shares(df_walk_to_transit)\n\n\n\n============================================================\n3. WALK-TO-TRANSIT BY SERVICE TYPE\n============================================================\n\n\nC:\\Users\\Pukar.Bhandari\\AppData\\Local\\Temp\\ipykernel_19912\\2056282771.py:47: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_work[category_col] = df_work[category_col].map(category_mapping)\n\n\n\nShape: (5, 12)\n\nFirst few rows:\n                                   HBC                 HBO            \\\n                                    OK        PK        OK        PK   \ncalib_share_walk-brt_all      0.297607  0.218401  0.084745  0.079797   \ncalib_share_walk-crt_all      0.016093  0.013708  0.012874  0.006756   \ncalib_share_walk-express_all  0.000000  0.000834  0.000000  0.001017   \ncalib_share_walk-local_all    0.527019  0.546157  0.554165  0.526032   \ncalib_share_walk-lrt_all      0.159281  0.220899  0.348216  0.386398   \n\n                                 HBSch                 HBW            \\\n                                    OK        PK        OK        PK   \ncalib_share_walk-brt_all      0.059241  0.024420  0.052287  0.058774   \ncalib_share_walk-crt_all      0.003367  0.000000  0.039720  0.041360   \ncalib_share_walk-express_all  0.000000  0.000000  0.001909  0.004684   \ncalib_share_walk-local_all    0.816277  0.868024  0.566760  0.603414   \ncalib_share_walk-lrt_all      0.121115  0.107556  0.339324  0.291767   \n\n                                   NHB                 HBC     HBSch  \n                                    OK        PK     Daily     Daily  \ncalib_share_walk-brt_all      0.161801  0.114137  0.263611  0.042130  \ncalib_share_walk-crt_all      0.011054  0.020001  0.015070  0.001712  \ncalib_share_walk-express_all  0.000000  0.000000  0.000358  0.000000  \ncalib_share_walk-local_all    0.463524  0.482216  0.535233  0.841705  \ncalib_share_walk-lrt_all      0.363621  0.383646  0.185728  0.114452  \n\nVerification:\n=== Verifying Share Sums ===\n\n✅ All shares sum to 1.0 (within tolerance)\n\nTotal columns: 12\nColumns with correct sums: 12\n\n\nHBC    OK       1.0\n       PK       1.0\nHBO    OK       1.0\n       PK       1.0\nHBSch  OK       1.0\n       PK       1.0\nHBW    OK       1.0\n       PK       1.0\nNHB    OK       1.0\n       PK       1.0\nHBC    Daily    1.0\nHBSch  Daily    1.0\ndtype: float64\n\n\n\n\nDrive to Transit\n\n\nShow the code\n# 4. Drive-to-Transit by Service Type (within drive-to-transit trips)\nprint(\"\\n\" + \"=\" * 60)\nprint(\"4. DRIVE-TO-TRANSIT BY SERVICE TYPE\")\nprint(\"=\" * 60)\n\ndrive_service_mapping = {\n    'LCL': 'drive-local',\n    'EXP': 'drive-express',\n    'LRT': 'drive-lrt',\n    'CRT': 'drive-crt',\n    'BRT': 'drive-brt'\n}\n\ndf_drive_to_transit = calculate_categorical_shares(\n    df_obs_linked,\n    category_col='Linked_Mode_txt',\n    weight_col='LINKED_WEIGHT',\n    group_cols=['Veh_Cat3p', 'Purp5_text', 'PK_OK'],\n    filter_condition='Ac_Mode_Model == \"Drive\" & Linked_Mode_txt != \"MT\"',\n    category_mapping=drive_service_mapping,\n    prefix='calib_share'\n)\n\nprint(\"\\nShape:\", df_drive_to_transit.shape)\nprint(\"\\nFirst few rows:\")\nprint(df_drive_to_transit.head(10))\nprint(\"\\nVerification:\")\nverify_shares(df_drive_to_transit)\n\n\n\n============================================================\n4. DRIVE-TO-TRANSIT BY SERVICE TYPE\n============================================================\n\n\nC:\\Users\\Pukar.Bhandari\\AppData\\Local\\Temp\\ipykernel_19912\\2056282771.py:47: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_work[category_col] = df_work[category_col].map(category_mapping)\n\n\n\nShape: (5, 12)\n\nFirst few rows:\n                                    HBC                 HBO            \\\n                                     OK        PK        OK        PK   \ncalib_share_drive-brt_all      0.289065  0.272318  0.012846  0.015046   \ncalib_share_drive-crt_all      0.199144  0.228157  0.261305  0.261220   \ncalib_share_drive-express_all  0.000000  0.001847  0.000000  0.003297   \ncalib_share_drive-local_all    0.288105  0.243502  0.179979  0.179055   \ncalib_share_drive-lrt_all      0.223685  0.254175  0.545870  0.541382   \n\n                                  HBSch                 HBW            \\\n                                     OK        PK        OK        PK   \ncalib_share_drive-brt_all      0.319894  0.041174  0.018059  0.031589   \ncalib_share_drive-crt_all      0.436062  0.159275  0.222239  0.264888   \ncalib_share_drive-express_all  0.000000  0.000000  0.027348  0.041295   \ncalib_share_drive-local_all    0.183738  0.275747  0.215311  0.196538   \ncalib_share_drive-lrt_all      0.060307  0.523803  0.517044  0.465690   \n\n                                    NHB                 HBC     HBSch  \n                                     OK        PK     Daily     Daily  \ncalib_share_drive-brt_all      0.094966  0.018370  0.281094  0.106002  \ncalib_share_drive-crt_all      0.322260  0.240296  0.212954  0.223654  \ncalib_share_drive-express_all  0.000000  0.000000  0.000879  0.000000  \ncalib_share_drive-local_all    0.057220  0.254282  0.266874  0.254347  \ncalib_share_drive-lrt_all      0.525554  0.487052  0.238198  0.415998  \n\nVerification:\n=== Verifying Share Sums ===\n\n✅ All shares sum to 1.0 (within tolerance)\n\nTotal columns: 12\nColumns with correct sums: 12\n\n\nHBC    OK       1.0\n       PK       1.0\nHBO    OK       1.0\n       PK       1.0\nHBSch  OK       1.0\n       PK       1.0\nHBW    OK       1.0\n       PK       1.0\nNHB    OK       1.0\n       PK       1.0\nHBC    Daily    1.0\nHBSch  Daily    1.0\ndtype: float64"
  }
]